{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<center>Notebook: Memory for Surprise-driven Exploration (Atari Ver.)</center>**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:**\n",
    "1. Burda, Y., Edwards, H., Storkey, A., & Klimov, O. (2018). Exploration by random network distillation. arXiv preprint arXiv:1810.12894.\n",
    "2. Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017, July). Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning (pp. 2778-2787). PMLR.\n",
    "3. Huang, S., Dossa, R. F. J., Ye, C., Braga, J., Chakraborty, D., Mehta, K., & AraÃƒÅ¡jo, J. G. (2022). Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. Journal of Machine Learning Research, 23(274), 1-18.\n",
    "4. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). Openai gym. arXiv preprint arXiv:1606.01540.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we present the implementation of method in the paper **\"Exploration by random network distillation\"**, which we refer to as RND intrinsic reward, and in the paper **\"Curiosity-driven exploration by self-supervised prediction\"**, which we refer to as ICM intrinsic reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Setting up the libraries** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these commands from the terminal to install related libraries and set up the working environment\n",
    "# pip install gym # Install the gym library with RL environments\n",
    "# pip install envpool\n",
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "import gym\n",
    "# import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import envpool\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gym.wrappers.normalize import RunningMeanStd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adventure-v5',\n",
       " 'AirRaid-v5',\n",
       " 'Alien-v5',\n",
       " 'Amidar-v5',\n",
       " 'Assault-v5',\n",
       " 'Asterix-v5',\n",
       " 'Asteroids-v5',\n",
       " 'Atlantis-v5',\n",
       " 'Atlantis2-v5',\n",
       " 'Backgammon-v5',\n",
       " 'BankHeist-v5',\n",
       " 'BasicMath-v5',\n",
       " 'BattleZone-v5',\n",
       " 'BeamRider-v5',\n",
       " 'Berzerk-v5',\n",
       " 'Blackjack-v5',\n",
       " 'Bowling-v5',\n",
       " 'Boxing-v5',\n",
       " 'Breakout-v5',\n",
       " 'Carnival-v5',\n",
       " 'Casino-v5',\n",
       " 'Centipede-v5',\n",
       " 'ChopperCommand-v5',\n",
       " 'CrazyClimber-v5',\n",
       " 'Crossbow-v5',\n",
       " 'Darkchambers-v5',\n",
       " 'Defender-v5',\n",
       " 'DemonAttack-v5',\n",
       " 'DonkeyKong-v5',\n",
       " 'DoubleDunk-v5',\n",
       " 'Earthworld-v5',\n",
       " 'ElevatorAction-v5',\n",
       " 'Enduro-v5',\n",
       " 'Entombed-v5',\n",
       " 'Et-v5',\n",
       " 'FishingDerby-v5',\n",
       " 'FlagCapture-v5',\n",
       " 'Freeway-v5',\n",
       " 'Frogger-v5',\n",
       " 'Frostbite-v5',\n",
       " 'Galaxian-v5',\n",
       " 'Gopher-v5',\n",
       " 'Gravitar-v5',\n",
       " 'Hangman-v5',\n",
       " 'HauntedHouse-v5',\n",
       " 'Hero-v5',\n",
       " 'HumanCannonball-v5',\n",
       " 'IceHockey-v5',\n",
       " 'Jamesbond-v5',\n",
       " 'JourneyEscape-v5',\n",
       " 'Kaboom-v5',\n",
       " 'Kangaroo-v5',\n",
       " 'KeystoneKapers-v5',\n",
       " 'KingKong-v5',\n",
       " 'Klax-v5',\n",
       " 'Koolaid-v5',\n",
       " 'Krull-v5',\n",
       " 'KungFuMaster-v5',\n",
       " 'LaserGates-v5',\n",
       " 'LostLuggage-v5',\n",
       " 'MarioBros-v5',\n",
       " 'MiniatureGolf-v5',\n",
       " 'MontezumaRevenge-v5',\n",
       " 'MrDo-v5',\n",
       " 'MsPacman-v5',\n",
       " 'NameThisGame-v5',\n",
       " 'Othello-v5',\n",
       " 'Pacman-v5',\n",
       " 'Phoenix-v5',\n",
       " 'Pitfall-v5',\n",
       " 'Pitfall2-v5',\n",
       " 'Pong-v5',\n",
       " 'Pooyan-v5',\n",
       " 'PrivateEye-v5',\n",
       " 'Qbert-v5',\n",
       " 'Riverraid-v5',\n",
       " 'RoadRunner-v5',\n",
       " 'Robotank-v5',\n",
       " 'Seaquest-v5',\n",
       " 'SirLancelot-v5',\n",
       " 'Skiing-v5',\n",
       " 'Solaris-v5',\n",
       " 'SpaceInvaders-v5',\n",
       " 'SpaceWar-v5',\n",
       " 'StarGunner-v5',\n",
       " 'Superman-v5',\n",
       " 'Surround-v5',\n",
       " 'Tennis-v5',\n",
       " 'Tetris-v5',\n",
       " 'TicTacToe3d-v5',\n",
       " 'TimePilot-v5',\n",
       " 'Trondead-v5',\n",
       " 'Turmoil-v5',\n",
       " 'Tutankham-v5',\n",
       " 'UpNDown-v5',\n",
       " 'Venture-v5',\n",
       " 'VideoCheckers-v5',\n",
       " 'VideoChess-v5',\n",
       " 'VideoCube-v5',\n",
       " 'VideoPinball-v5',\n",
       " 'WizardOfWor-v5',\n",
       " 'WordZapper-v5',\n",
       " 'YarsRevenge-v5',\n",
       " 'Zaxxon-v5',\n",
       " 'CarRacing-v2',\n",
       " 'BipedalWalker-v3',\n",
       " 'BipedalWalkerHardcore-v3',\n",
       " 'LunarLander-v2',\n",
       " 'LunarLanderContinuous-v2',\n",
       " 'CartPole-v0',\n",
       " 'CartPole-v1',\n",
       " 'Pendulum-v0',\n",
       " 'Pendulum-v1',\n",
       " 'MountainCar-v0',\n",
       " 'MountainCarContinuous-v0',\n",
       " 'Acrobot-v1',\n",
       " 'AcrobotSwingup-v1',\n",
       " 'AcrobotSwingupSparse-v1',\n",
       " 'BallInCupCatch-v1',\n",
       " 'CartpoleBalance-v1',\n",
       " 'CartpoleBalanceSparse-v1',\n",
       " 'CartpoleSwingup-v1',\n",
       " 'CartpoleSwingupSparse-v1',\n",
       " 'CartpoleThreePoles-v1',\n",
       " 'CartpoleTwoPoles-v1',\n",
       " 'CheetahRun-v1',\n",
       " 'FingerSpin-v1',\n",
       " 'FingerTurnEasy-v1',\n",
       " 'FingerTurnHard-v1',\n",
       " 'FishSwim-v1',\n",
       " 'FishUpright-v1',\n",
       " 'HopperHop-v1',\n",
       " 'HopperStand-v1',\n",
       " 'HumanoidRun-v1',\n",
       " 'HumanoidRunPureState-v1',\n",
       " 'HumanoidStand-v1',\n",
       " 'HumanoidWalk-v1',\n",
       " 'HumanoidCMURun-v1',\n",
       " 'HumanoidCMUStand-v1',\n",
       " 'ManipulatorBringBall-v1',\n",
       " 'ManipulatorBringPeg-v1',\n",
       " 'ManipulatorInsertBall-v1',\n",
       " 'ManipulatorInsertPeg-v1',\n",
       " 'PendulumSwingup-v1',\n",
       " 'PointMassEasy-v1',\n",
       " 'PointMassHard-v1',\n",
       " 'ReacherEasy-v1',\n",
       " 'ReacherHard-v1',\n",
       " 'SwimmerSwimmer6-v1',\n",
       " 'SwimmerSwimmer15-v1',\n",
       " 'WalkerRun-v1',\n",
       " 'WalkerStand-v1',\n",
       " 'WalkerWalk-v1',\n",
       " 'Ant-v3',\n",
       " 'Ant-v4',\n",
       " 'HalfCheetah-v3',\n",
       " 'HalfCheetah-v4',\n",
       " 'Hopper-v3',\n",
       " 'Hopper-v4',\n",
       " 'Humanoid-v3',\n",
       " 'Humanoid-v4',\n",
       " 'HumanoidStandup-v2',\n",
       " 'HumanoidStandup-v4',\n",
       " 'InvertedDoublePendulum-v2',\n",
       " 'InvertedDoublePendulum-v4',\n",
       " 'InvertedPendulum-v2',\n",
       " 'InvertedPendulum-v4',\n",
       " 'Pusher-v2',\n",
       " 'Pusher-v4',\n",
       " 'Reacher-v2',\n",
       " 'Reacher-v4',\n",
       " 'Swimmer-v3',\n",
       " 'Swimmer-v4',\n",
       " 'Walker2d-v3',\n",
       " 'Walker2d-v4',\n",
       " 'BigfishEasy-v0',\n",
       " 'BigfishHard-v0',\n",
       " 'BossfightEasy-v0',\n",
       " 'BossfightHard-v0',\n",
       " 'CaveflyerEasy-v0',\n",
       " 'CaveflyerHard-v0',\n",
       " 'CaveflyerMemory-v0',\n",
       " 'ChaserEasy-v0',\n",
       " 'ChaserHard-v0',\n",
       " 'ChaserExtreme-v0',\n",
       " 'ClimberEasy-v0',\n",
       " 'ClimberHard-v0',\n",
       " 'CoinrunEasy-v0',\n",
       " 'CoinrunHard-v0',\n",
       " 'DodgeballEasy-v0',\n",
       " 'DodgeballHard-v0',\n",
       " 'DodgeballExtreme-v0',\n",
       " 'DodgeballMemory-v0',\n",
       " 'FruitbotEasy-v0',\n",
       " 'FruitbotHard-v0',\n",
       " 'HeistEasy-v0',\n",
       " 'HeistHard-v0',\n",
       " 'HeistMemory-v0',\n",
       " 'JumperEasy-v0',\n",
       " 'JumperHard-v0',\n",
       " 'JumperMemory-v0',\n",
       " 'LeaperEasy-v0',\n",
       " 'LeaperHard-v0',\n",
       " 'LeaperExtreme-v0',\n",
       " 'MazeEasy-v0',\n",
       " 'MazeHard-v0',\n",
       " 'MazeMemory-v0',\n",
       " 'MinerEasy-v0',\n",
       " 'MinerHard-v0',\n",
       " 'MinerMemory-v0',\n",
       " 'NinjaEasy-v0',\n",
       " 'NinjaHard-v0',\n",
       " 'PlunderEasy-v0',\n",
       " 'PlunderHard-v0',\n",
       " 'StarpilotEasy-v0',\n",
       " 'StarpilotHard-v0',\n",
       " 'StarpilotExtreme-v0',\n",
       " 'Catch-v0',\n",
       " 'FrozenLake-v1',\n",
       " 'FrozenLake8x8-v1',\n",
       " 'Taxi-v3',\n",
       " 'NChain-v0',\n",
       " 'CliffWalking-v0',\n",
       " 'Blackjack-v1',\n",
       " 'D1Basic-v1',\n",
       " 'D2Navigation-v1',\n",
       " 'D3Battle-v1',\n",
       " 'D4Battle2-v1',\n",
       " 'Basic-v1',\n",
       " 'Cig-v1',\n",
       " 'DeadlyCorridor-v1',\n",
       " 'Deathmatch-v1',\n",
       " 'DefendTheCenter-v1',\n",
       " 'DefendTheLine-v1',\n",
       " 'HealthGathering-v1',\n",
       " 'HealthGatheringSupreme-v1',\n",
       " 'MultiDuel-v1',\n",
       " 'MyWayHome-v1',\n",
       " 'PredictPosition-v1',\n",
       " 'RocketBasic-v1',\n",
       " 'SimplerBasic-v1',\n",
       " 'TakeCover-v1',\n",
       " 'VizdoomCustom-v1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envpool.list_all_envs()             # envpool library supports creating a huge number of multi-process environments -> we will use this for our environnment today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Introducing the Breakout Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ðŸ‘‹ We will use the simple Breakout-v5 environment (https://gymnasium.farama.org/environments/atari/breakout/) for demonstration. A classic Atari game. ðŸŽ® \n",
    "- The goal is to move the paddle and hit the ball into a brick wall. The wall can be break through and the ball can wreak it on the other side. 1 game has 5 lives. \n",
    "- The **observation space** can be \"rgb\", \"grayscale\" or \"ram\" image (in this notebook we use the \"rgb\" variant which is the hardest), it represents the portview of the agent. And, **action space** is Discrete(4) including action to move left or right ðŸ”„, fire the ball, and action to do nothing.\n",
    "- You will get point (**reward**) depending on the color of the brick that you break: \n",
    "  - Red - 7 points         \n",
    "  - Orange - 7 points        \n",
    "  - Yellow - 4 points\n",
    "  - Green - 4 points       \n",
    "  - Aqua - 1 point           \n",
    "  - Blue - 1 point.\n",
    "- This notebook can also be used for other atari game environments as well. ðŸ˜Š All you need to do is change the env_id. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://gymnasium.farama.org/_images/breakout.gif\" width=\"300\" height=\"400\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Simple PPO** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you only want to run the surprise motivation, please run until section 3.2.2 to initialize the parameters and PPO.**\n",
    "- In this section, we will test a simple PPO algorithm on the BreakOut-v5 environment. ðŸ•¹ï¸\n",
    "- The implementation of PPO in this notebook is inspired by the implementation of PPO from CleanRL package. ðŸ“Š\n",
    "- The result of this algorithm will be presented at the end of the notebook. ðŸ“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'env_id': \"Breakout-v5\",\n",
    "          'exp_name': \"RND\",\n",
    "          'torch_deterministic': True,\n",
    "          'cuda': True,\n",
    "          'seed': 1,\n",
    "          # Training parameters\n",
    "          'num_envs': 8,                                        # number of multi-environments\n",
    "          'num_steps': 128,                                     # number of steps running in each environments per rollout\n",
    "          'num_minibatches': 4,                                 # number of minibatches\n",
    "          'total_timesteps': 10000000,                            # total training timesteps\n",
    "          'learning_rate': 1e-4,                                # learning_rate\n",
    "          'anneal_lr': True,                                    # reducing learning rate during learning\n",
    "          'num_iterations_obs_norm_init': 50,\n",
    "          'gamma': 0.999,\n",
    "          'int_gamma': 0.99,\n",
    "          'gae_lambda': 0.95,\n",
    "          'int_coef': 1.0,\n",
    "          'ext_coef': 2.0,\n",
    "          'update_epochs': 4,\n",
    "          'update_proportion': 0.25,\n",
    "          'clip_coef': 0.1,\n",
    "          'norm_adv': True,\n",
    "          'clip_vloss': True,\n",
    "          'ent_coef': 0.001,\n",
    "          'vf_coef': 0.5,\n",
    "          'max_grad_norm': 0.5,\n",
    "          'target_kl': None}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and params[\"cuda\"] else \"cpu\")\n",
    "\n",
    "# Set seed.\n",
    "random.seed(params[\"seed\"])                                                 \n",
    "np.random.seed(params[\"seed\"])\n",
    "torch.manual_seed(params[\"seed\"])\n",
    "torch.backends.cudnn.deterministic = params[\"torch_deterministic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.1 Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordEpisodeStatistics(gym.Wrapper):\n",
    "    def __init__(self, env, deque_size=100):\n",
    "        super().__init__(env)\n",
    "        self.num_envs = getattr(env, \"num_envs\", 1)\n",
    "        self.episode_returns = None\n",
    "        self.episode_lengths = None\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observations = super().reset(**kwargs)\n",
    "        self.episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n",
    "        self.episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        self.lives = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        self.returned_episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n",
    "        self.returned_episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        return observations\n",
    "\n",
    "    def step(self, action):\n",
    "        observations, rewards, dones, _, infos = super().step(action)\n",
    "        self.episode_returns += infos[\"reward\"]\n",
    "        self.episode_lengths += 1\n",
    "        self.returned_episode_returns[:] = self.episode_returns\n",
    "        self.returned_episode_lengths[:] = self.episode_lengths\n",
    "        self.episode_returns *= 1 - infos[\"terminated\"]\n",
    "        self.episode_lengths *= 1 - infos[\"terminated\"]\n",
    "        infos[\"r\"] = self.returned_episode_returns\n",
    "        infos[\"l\"] = self.returned_episode_lengths\n",
    "        return (observations, rewards, dones, infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)                # Initialize layer weights according to orthogonal method.\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)             # Set the bias of the layer.\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.2 PPO Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, envs, use_int_rews=False):\n",
    "        super().__init__()\n",
    "        self.use_int_rews = use_int_rews\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(4, 32, 8, stride=4)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(64 * 7 * 7, 256)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(256, 448)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.extra_layer = nn.Sequential(layer_init(nn.Linear(448, 448), std=0.1), nn.ReLU())\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(448, 448), std=0.01),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(448, envs.single_action_space.n), std=0.01),\n",
    "        )\n",
    "        \n",
    "        self.critic_ext = layer_init(nn.Linear(448, 1), std=0.01)\n",
    "        self.critic_int = layer_init(nn.Linear(448, 1), std=0.01)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x / 255.0)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        features = self.extra_layer(hidden)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        if self.use_int_rews:                                           # If intrinsic reward is used\n",
    "            return (action, probs.log_prob(action), probs.entropy(), self.critic_ext(features + hidden), self.critic_int(features + hidden),)\n",
    "        else:                                                           # If intrinsic reward is not used\n",
    "            return (action, probs.log_prob(action), probs.entropy(), self.critic_ext(features + hidden),)\n",
    "\n",
    "    def get_value(self, x):\n",
    "        hidden = self.network(x / 255.0)\n",
    "        features = self.extra_layer(hidden)\n",
    "        if self.use_int_rews:                                           # If intrinsic reward is used\n",
    "            return self.critic_ext(features + hidden), self.critic_int(features + hidden)\n",
    "        else:                                                           # If intrinsic reward is not used\n",
    "            return self.critic_ext(features + hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.3 Main Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action space of the environment is Discrete(4)\n",
      "The observation space of the environment is Box(0, 255, (4, 84, 84), uint8)\n"
     ]
    }
   ],
   "source": [
    "env_id = params[\"env_id\"]\n",
    "exp_name = params[\"exp_name\"]\n",
    "seed = params[\"seed\"]\n",
    "run_name = f\"{env_id}__{exp_name}__{seed}__{int(time.time())}\"\n",
    "\n",
    "envs = envpool.make(\n",
    "    params[\"env_id\"],\n",
    "    env_type=\"gym\",\n",
    "    num_envs=params[\"num_envs\"],\n",
    "    episodic_life=True,\n",
    "    reward_clip=True,\n",
    "    seed=params[\"seed\"],\n",
    "    repeat_action_probability=0.25,\n",
    ")\n",
    "\n",
    "envs.num_envs = params[\"num_envs\"]\n",
    "envs.single_action_space = envs.action_space\n",
    "print(f\"The action space of the environment is {envs.single_action_space}\")\n",
    "envs.single_observation_space = envs.observation_space\n",
    "print(f\"The observation space of the environment is {envs.single_observation_space}\")\n",
    "\n",
    "envs = RecordEpisodeStatistics(envs)\n",
    "assert isinstance(envs.action_space, gym.spaces.Discrete), \"only discrete action space is supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up agent and model\n",
    "Agent = PPOAgent(envs, use_int_rews=False).to(device)\n",
    "optimizer = optim.Adam(\n",
    "    Agent.parameters(),\n",
    "    lr=params[\"learning_rate\"],\n",
    "    eps=1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((params[\"num_steps\"], params[\"num_envs\"]) + envs.single_observation_space.shape).to(device)  \n",
    "actions = torch.zeros((params[\"num_steps\"], params[\"num_envs\"]) + envs.single_action_space.shape).to(device)   \n",
    "logprobs = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "rewards = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "dones = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "values = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "avg_returns = deque(maxlen=20)\n",
    "\n",
    "batch_size = int(params[\"num_envs\"] * params[\"num_steps\"])                \n",
    "minibatch_size = int(batch_size // params[\"num_minibatches\"])\n",
    "num_iterations = params[\"total_timesteps\"] // batch_size                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs = torch.Tensor(envs.reset()[0]).to(device)\n",
    "next_done = torch.zeros(params[\"num_envs\"]).to(device)\n",
    "results_simple_PPO = {\"global_step\":[],\n",
    "                      \"return_value\":[]}\n",
    "tracking_global_step = 0\n",
    "\n",
    "for iteration in range(1, num_iterations+1):\n",
    "    if params[\"anneal_lr\"]:\n",
    "        updated_lr = (1.0 - (iteration - 1.0) / num_iterations) * params[\"learning_rate\"]\n",
    "        optimizer.param_groups[0][\"lr\"] = updated_lr\n",
    "    \n",
    "    for step in range(0, params[\"num_steps\"]):\n",
    "        global_step += 1 * params[\"num_envs\"]\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done \n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = Agent.get_action_and_value(obs[step])\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "        for idx, d in enumerate(done):\n",
    "            # If done and no more live -> Get return\n",
    "            if d and info[\"lives\"][idx] == 0:\n",
    "                avg_returns.append(info[\"r\"][idx])\n",
    "                epi_ret = np.average(avg_returns)\n",
    "                print(\n",
    "                    f\"global_step={global_step}, episodic_return={info['r'][idx]}\"\n",
    "                )\n",
    "                if global_step - tracking_global_step > 10000:\n",
    "                    results_simple_PPO[\"global_step\"].append(global_step)\n",
    "                    results_simple_PPO[\"return_value\"].append(info['r'][idx])\n",
    "                    tracking_global_step = global_step\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value = Agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(params[\"num_steps\"])):\n",
    "            if t == params[\"num_steps\"] - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            delta = rewards[t] + params[\"gamma\"] * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + params[\"gamma\"] * params[\"gae_lambda\"] * nextnonterminal * lastgaelam\n",
    "        returns = advantages + values\n",
    "\n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(params[\"update_epochs\"]):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = Agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > params[\"clip_coef\"]).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if params[\"norm_adv\"]:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - params[\"clip_coef\"], 1 + params[\"clip_coef\"])\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if params[\"clip_vloss\"]:\n",
    "                value_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                    newvalue - b_values[mb_inds],\n",
    "                    -params[\"clip_coef\"],\n",
    "                    params[\"clip_coef\"],\n",
    "                )\n",
    "                value_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                value_loss_max = torch.max(value_loss_unclipped, value_loss_clipped)\n",
    "                value_loss = 0.5 * value_loss_max.mean()\n",
    "            else:\n",
    "                value_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - params[\"ent_coef\"] * entropy_loss + value_loss * params[\"vf_coef\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(Agent.parameters(), params[\"max_grad_norm\"])\n",
    "            optimizer.step()\n",
    "\n",
    "        if params[\"target_kl\"] is not None and approx_kl > params[\"target_kl\"]:\n",
    "            break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(Agent, \"pretrained_models/simple_ppo_for_suprised_atari.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved PPO agent\n",
    "# agent = torch.load(\"pretrained_models/simple_ppo_for_suprised_atari.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from results_simple_PPO\n",
    "ppo_global_step = results_simple_PPO[\"global_step\"]\n",
    "ppo_return_value = results_simple_PPO[\"return_value\"]\n",
    "\n",
    "\n",
    "df_ppo = pd.DataFrame({'global_step': ppo_global_step, 'return_value': ppo_return_value})\n",
    "\n",
    "\n",
    "df_ppo.to_csv('data/results_simple_ppo_atari.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Random Network Distillation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we will test the Random Network Distillation algorithm on the BreakOut-v5 environment. ðŸ•¹ï¸\n",
    "- The result of this algorithm will be presented at the end of the notebook. ðŸ“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 Key Points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prediction problem is randomly generated. This involves 2 NNs, fixed target network sets the prediction problem (find an embedding $f(O)$ for an observation) and predictor network trained on data collected (with the task to predict $\\hat{f}(O)$) from the agent, minimizing MSE Loss $\\text{MSE} = \\| \\hat{f}(x; \\theta) - f(x) \\|^{2}_2$.\n",
    "- Prediction error is expected to be higher in novel state (suprise state) that the agent is not familiar with. \n",
    "- $R=R_E+R_I$, thus, $V=V_E+V_I$.\n",
    "- Reward and Observation Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.1 RND Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNDModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        feature_output = 7 * 7 * 64\n",
    "\n",
    "        # Prediction network\n",
    "        self.predictor = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8, stride=4)),\n",
    "            nn.LeakyReLU(),\n",
    "            layer_init(nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)),\n",
    "            nn.LeakyReLU(),\n",
    "            layer_init(nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(feature_output, 512)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(512, 512)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(512, 512)),\n",
    "        )\n",
    "\n",
    "        # Target network\n",
    "        self.target = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=8, stride=4)),\n",
    "            nn.LeakyReLU(),\n",
    "            layer_init(nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)),\n",
    "            nn.LeakyReLU(),\n",
    "            layer_init(nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(feature_output, 512)),\n",
    "        )\n",
    "\n",
    "        # fixed the target network params\n",
    "        for param in self.target.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, next_obs):\n",
    "        target_feature = self.target(next_obs)\n",
    "        predict_feature = self.predictor(next_obs)\n",
    "\n",
    "        return predict_feature, target_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingSumOfReward:\n",
    "    def __init__(self, gamma):\n",
    "        self.moving_sum_of_reward = None\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, rews):\n",
    "        if self.moving_sum_of_reward is None:\n",
    "            self.moving_sum_of_reward = rews\n",
    "        else:\n",
    "            self.moving_sum_of_reward = self.moving_sum_of_reward * self.gamma + rews\n",
    "        return self.moving_sum_of_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.2 Main Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action space of the environment is Discrete(4)\n",
      "The observation space of the environment is Box(0, 255, (4, 84, 84), uint8)\n"
     ]
    }
   ],
   "source": [
    "env_id = params[\"env_id\"]\n",
    "exp_name = params[\"exp_name\"]\n",
    "seed = params[\"seed\"]\n",
    "run_name = f\"{env_id}__{exp_name}__{seed}__{int(time.time())}\"\n",
    "\n",
    "envs = envpool.make(\n",
    "    params[\"env_id\"],\n",
    "    env_type=\"gym\",\n",
    "    num_envs=params[\"num_envs\"],\n",
    "    episodic_life=True,\n",
    "    reward_clip=True,\n",
    "    seed=params[\"seed\"],\n",
    "    repeat_action_probability=0.25,\n",
    ")\n",
    "\n",
    "envs.num_envs = params[\"num_envs\"]\n",
    "envs.single_action_space = envs.action_space\n",
    "print(f\"The action space of the environment is {envs.single_action_space}\")\n",
    "envs.single_observation_space = envs.observation_space\n",
    "print(f\"The observation space of the environment is {envs.single_observation_space}\")\n",
    "\n",
    "envs = RecordEpisodeStatistics(envs)\n",
    "assert isinstance(envs.action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up agent and model\n",
    "Agent = PPOAgent(envs, use_int_rews=True).to(device)\n",
    "rnd_model = RNDModel(4, envs.single_action_space.n).to(device)\n",
    "combined_parameters = list(Agent.parameters()) + list(rnd_model.predictor.parameters())\n",
    "optimizer = optim.Adam(\n",
    "    combined_parameters,\n",
    "    lr=params[\"learning_rate\"],\n",
    "    eps=1e-5,\n",
    ")\n",
    "\n",
    "rew_runnning_mean_std = RunningMeanStd()\n",
    "obs_runnning_mean_std = RunningMeanStd(shape=(1, 1, 84, 84))            # normalizing observation\n",
    "discounted_reward = MovingSumOfReward(params[\"int_gamma\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((params[\"num_steps\"], params[\"num_envs\"]) + envs.single_observation_space.shape).to(device)  # (128, 4, 4, 84, 84)\n",
    "actions = torch.zeros((params[\"num_steps\"], params[\"num_envs\"]) + envs.single_action_space.shape).to(device)   # (128, 4) \n",
    "logprobs = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "rewards = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "surprise_rewards = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "dones = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "ext_values = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "int_values = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "avg_returns = deque(maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(params[\"num_envs\"] * params[\"num_steps\"])                      # 4 * 128\n",
    "minibatch_size = int(batch_size // params[\"num_minibatches\"])\n",
    "num_iterations = params[\"total_timesteps\"] // batch_size                        # 20000000/(4*128) -> num iterations\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "\n",
    "next_obs = torch.Tensor(envs.reset()[0]).to(device)\n",
    "next_done = torch.zeros(params[\"num_envs\"]).to(device)\n",
    "\n",
    "results_RND = {\"global_step\":[],\n",
    "                \"return_value\":[],\n",
    "                \"intrinsic_reward\":[]}\n",
    "\n",
    "tracking_global_step = 0\n",
    "\n",
    "# This will be used later when normalizing the observations\n",
    "print(\"Start to initialize observation normalization parameter.....\")\n",
    "next_ob = []\n",
    "for step in range(params[\"num_steps\"] * params[\"num_iterations_obs_norm_init\"]):\n",
    "    acs = np.random.randint(0, envs.single_action_space.n, size=(params[\"num_envs\"]))\n",
    "    s, r, d, _ = envs.step(acs)\n",
    "    next_ob += s[:, 3, :, :].reshape([-1, 1, 84, 84]).tolist()\n",
    "\n",
    "    if len(next_ob) % (params[\"num_steps\"] * params[\"num_envs\"]) == 0:\n",
    "        next_ob = np.stack(next_ob)\n",
    "        obs_runnning_mean_std.update(next_ob)\n",
    "        next_ob = []\n",
    "print(\"End to initialize...\")\n",
    "\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    if params[\"anneal_lr\"]:\n",
    "        updated_lr = (1.0 - (iteration - 1.0) / num_iterations) * params[\"learning_rate\"]\n",
    "        optimizer.param_groups[0][\"lr\"] = updated_lr\n",
    "\n",
    "    # n-step rollouts\n",
    "    for step in range(0, params[\"num_steps\"]):\n",
    "        global_step += 1 * params[\"num_envs\"]\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        with torch.no_grad():\n",
    "            value_ext, value_int = Agent.get_value(obs[step])                   # -> get the extrinsic and intrinsic value for current observation\n",
    "            ext_values[step], int_values[step] = (\n",
    "                value_ext.flatten(),\n",
    "                value_int.flatten(),\n",
    "            )\n",
    "            action, logprob, _, _, _ = Agent.get_action_and_value(obs[step])\n",
    "\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "        # Normalize obs for rnd\n",
    "        rnd_next_obs = (\n",
    "                (\n",
    "                    (next_obs[:, 3, :, :].reshape(params[\"num_envs\"], 1, 84, 84) - torch.from_numpy(obs_runnning_mean_std.mean).to(device))\n",
    "                    / torch.sqrt(torch.from_numpy(obs_runnning_mean_std.var).to(device))\n",
    "                ).clip(-5, 5)\n",
    "            ).float()\n",
    "        \n",
    "        # Get the target F(O) and predict \\hat(F)(O) value from rnd model\n",
    "        target_next_feature, predict_next_feature = rnd_model.target(rnd_next_obs), rnd_model.predictor(rnd_next_obs)\n",
    "\n",
    "        # Calculate the surprise reward based on MSE\n",
    "        surprise_rewards[step] = ((target_next_feature - predict_next_feature).pow(2).sum(1) / 2).data\n",
    "\n",
    "        for idx, d in enumerate(done):\n",
    "            # If done and no more live -> Get return\n",
    "            if d and info[\"lives\"][idx] == 0:\n",
    "                avg_returns.append(info[\"r\"][idx])\n",
    "                epi_ret = np.average(avg_returns)\n",
    "                print(\n",
    "                    f\"global_step={global_step}, episodic_return={info['r'][idx]}, surprise_reward={np.mean(surprise_rewards[step].cpu().numpy())}\"\n",
    "                )\n",
    "                if global_step - tracking_global_step > 10000:\n",
    "                    results_RND[\"global_step\"].append(global_step)\n",
    "                    results_RND[\"return_value\"].append(info['r'][idx])\n",
    "                    results_RND[\"intrinsic_reward\"].append(np.mean(surprise_rewards[step].cpu().numpy()))\n",
    "                    tracking_global_step = global_step\n",
    "\n",
    "    # Calculate the discounted reward \n",
    "    surprise_reward_per_env = np.array(\n",
    "        [discounted_reward.update(reward_per_step) for reward_per_step in surprise_rewards.cpu().data.numpy().T]\n",
    "    )\n",
    "\n",
    "    mean, std, count = (\n",
    "        np.mean(surprise_reward_per_env),\n",
    "        np.std(surprise_reward_per_env),\n",
    "        len(surprise_reward_per_env),\n",
    "    )\n",
    "    \n",
    "    rew_runnning_mean_std.update_from_moments(mean, std**2, count)\n",
    "\n",
    "    # Normalize the curiousity_rewards based on the running_mean_std\n",
    "    surprise_rewards /= np.sqrt(rew_runnning_mean_std.var)\n",
    "\n",
    "    # Calculate value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value_ext, next_value_int = Agent.get_value(next_obs)\n",
    "        next_value_ext, next_value_int = next_value_ext.reshape(1, -1), next_value_int.reshape(1, -1)   # -> get next state values external & internal\n",
    "        ext_advantages = torch.zeros_like(rewards, device=device)\n",
    "        int_advantages = torch.zeros_like(surprise_rewards, device=device)\n",
    "        ext_lastgaelam = 0\n",
    "        int_lastgaelam = 0\n",
    "        for t in reversed(range(params[\"num_steps\"])):\n",
    "            if t == params[\"num_steps\"] - 1:\n",
    "                ext_nextnonterminal = 1.0 - next_done\n",
    "                int_nextnonterminal = 1.0\n",
    "                ext_nextvalues = next_value_ext\n",
    "                int_nextvalues = next_value_int\n",
    "            else:\n",
    "                ext_nextnonterminal = 1.0 - dones[t + 1]\n",
    "                int_nextnonterminal = 1.0\n",
    "                ext_nextvalues = ext_values[t + 1]\n",
    "                int_nextvalues = int_values[t + 1]\n",
    "            ext_delta = rewards[t] + params[\"gamma\"] * ext_nextvalues * ext_nextnonterminal - ext_values[t]\n",
    "            int_delta = surprise_rewards[t] + params[\"int_gamma\"] * int_nextvalues * int_nextnonterminal - int_values[t]\n",
    "            ext_advantages[t] = ext_lastgaelam = (\n",
    "                ext_delta + params[\"gamma\"] * params[\"gae_lambda\"] * ext_nextnonterminal * ext_lastgaelam\n",
    "            )\n",
    "            int_advantages[t] = int_lastgaelam = (\n",
    "                int_delta + params[\"int_gamma\"] * params[\"gae_lambda\"] * int_nextnonterminal * int_lastgaelam\n",
    "            )\n",
    "        ext_returns = ext_advantages + ext_values\n",
    "        int_returns = int_advantages + int_values\n",
    "\n",
    "    # Collect batch data for optimization\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape(-1)\n",
    "    b_ext_advantages = ext_advantages.reshape(-1)\n",
    "    b_int_advantages = int_advantages.reshape(-1)\n",
    "    b_ext_returns = ext_returns.reshape(-1)\n",
    "    b_int_returns = int_returns.reshape(-1)\n",
    "    b_ext_values = ext_values.reshape(-1)\n",
    "\n",
    "    b_advantages = b_int_advantages * params[\"int_coef\"] + b_ext_advantages * params[\"ext_coef\"]\n",
    "\n",
    "    obs_runnning_mean_std.update(b_obs[:, 3, :, :].reshape(-1, 1, 84, 84).cpu().numpy())\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(batch_size)\n",
    "\n",
    "    rnd_next_obs = (\n",
    "        (\n",
    "            (b_obs[:, 3, :, :].reshape(-1, 1, 84, 84) - torch.from_numpy(obs_runnning_mean_std.mean).to(device))\n",
    "            / torch.sqrt(torch.from_numpy(obs_runnning_mean_std.var).to(device))\n",
    "        ).clip(-5, 5)\n",
    "    ).float()\n",
    "\n",
    "    clipfracs = []\n",
    "    for epoch in range(params[\"update_epochs\"]):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            # Forward_loss\n",
    "            predict_next_state_feature, target_next_state_feature = rnd_model(rnd_next_obs[mb_inds])\n",
    "\n",
    "            # Calculate the MSE loss in the forward prediction of the RND Model \n",
    "            forward_loss = F.mse_loss(\n",
    "                predict_next_state_feature, target_next_state_feature.detach(), reduction=\"none\"\n",
    "            ).mean(-1)\n",
    "\n",
    "            mask = torch.rand(len(forward_loss), device=device)\n",
    "            mask = (mask < params[\"update_proportion\"]).type(torch.FloatTensor).to(device)\n",
    "            forward_loss = (forward_loss * mask).sum() / torch.max(\n",
    "                mask.sum(), torch.tensor([1], device=device, dtype=torch.float32)\n",
    "            )\n",
    "            _, newlogprob, entropy, new_ext_values, new_int_values = Agent.get_action_and_value(\n",
    "                b_obs[mb_inds], b_actions.long()[mb_inds]\n",
    "            )\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > params[\"clip_coef\"]).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if params[\"norm_adv\"]:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - params[\"clip_coef\"], 1 + params[\"clip_coef\"])\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            new_ext_values, new_int_values = new_ext_values.view(-1), new_int_values.view(-1)\n",
    "            if params[\"clip_vloss\"]:\n",
    "                ext_value_loss_unclipped = (new_ext_values - b_ext_returns[mb_inds]) ** 2\n",
    "                ext_v_clipped = b_ext_values[mb_inds] + torch.clamp(\n",
    "                    new_ext_values - b_ext_values[mb_inds],\n",
    "                    -params[\"clip_coef\"],\n",
    "                params[\"clip_coef\"],\n",
    "                )\n",
    "                ext_value_loss_clipped = (ext_v_clipped - b_ext_returns[mb_inds]) ** 2\n",
    "                ext_value_loss_max = torch.max(ext_value_loss_unclipped, ext_value_loss_clipped)\n",
    "                ext_value_loss = 0.5 * ext_value_loss_max.mean()\n",
    "            else:\n",
    "                ext_value_loss = 0.5 * ((new_ext_values - b_ext_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            int_value_loss = 0.5 * ((new_int_values - b_int_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            value_loss = ext_value_loss + int_value_loss\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - params[\"ent_coef\"] * entropy_loss + value_loss * params[\"vf_coef\"] + forward_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if params[\"max_grad_norm\"]:\n",
    "                nn.utils.clip_grad_norm_(\n",
    "                    combined_parameters,\n",
    "                    params[\"max_grad_norm\"],\n",
    "                )\n",
    "            optimizer.step()\n",
    "\n",
    "        if params[\"target_kl\"] is not None:\n",
    "            if approx_kl > params[\"target_kl\"]:\n",
    "                break\n",
    "\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(Agent, \"pretrained_models/ppo_for_RND_atari.pth\")\n",
    "# torch.save(rnd_model, \"pretrained_models/rnd_atari.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the saved PPO agent\n",
    "# agent = torch.load(\"pretrained_models/ppo_for_RND_atari.pth\")\n",
    "# # Load the saved ICM model\n",
    "# icm = torch.load(\"pretrained_models/rnd_atari.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from results_RND\n",
    "rnd_global_step = results_RND[\"global_step\"]\n",
    "rnd_return_value = results_RND[\"return_value\"]\n",
    "rnd_intrinsic_reward = results_RND[\"intrinsic_reward\"]\n",
    "\n",
    "df_rnd = pd.DataFrame({'global_step': rnd_global_step, 'return_value': rnd_return_value, 'intrinsic_reward': rnd_intrinsic_reward})\n",
    "# Save DataFrames to CSV files\n",
    "df_rnd.to_csv('data/results_rnd_atari.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Curiosity-driven Exploration by Self-supervised Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we will test the Intrinsic Curiousity Motivation (ICM) algorithm on the BreakOut-v5 environment. ðŸ•¹ï¸\n",
    "- The result of this algorithm will be presented at the end of the notebook. ðŸ“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1 Key Points**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The architecture is a network with two tasks, which we refer to as inverse prediction task and forward prediction task. The network, firstly, encodes the state $s_{t}$ and state $s_{t+1}$ into feature vectors $\\phi(s_{t})$ and $\\phi(s_{t+1})$. The network used the two encoded vectors as input to predict action $a_t$ in the inverse prediction task. It then used the result feature vectors $\\phi(s_{t})$ and action $a_t$ as input to predict $\\phi(s_{t+1})$. \n",
    "- The loss for the inverse prediction task is a cross entropy loss between the action chosen by the architecture and the real action that the agent has taken. The loss for the forward prediction task is an MSE loss.\n",
    "- Prediction error is expected to be higher in novel state (suprise state) that the agent is not familiar with. \n",
    "- Reward and Observation Normalization.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./image/Curiousity-driven exploration.png\" alt=\"AutoEncoder forr Count Based Exploration\" width=\"420\" height=\"350\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2 Models**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.2.1 ICM Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICMModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, device, use_cuda=True):\n",
    "        super(ICMModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.eta = 1.\n",
    "        self.device = device\n",
    "\n",
    "        feature_output = 7 * 7 * 64\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=32,\n",
    "                kernel_size=8,\n",
    "                stride=4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=4,\n",
    "                stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_output, 512)\n",
    "        )\n",
    "\n",
    "        self.inverse_net = nn.Sequential(\n",
    "            nn.Linear(512 * 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_size)\n",
    "        )\n",
    "\n",
    "        self.residual = [nn.Sequential(\n",
    "            nn.Linear(output_size + 512, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        ).to(self.device)] * 4\n",
    "\n",
    "        self.forward_net_1 = nn.Sequential(\n",
    "            nn.Linear(output_size + 512, 512),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.forward_net_2 = nn.Sequential(\n",
    "            nn.Linear(output_size + 512, 512),\n",
    "        )\n",
    "\n",
    "        for p in self.modules():\n",
    "            if isinstance(p, nn.Conv2d):\n",
    "                init.kaiming_uniform_(p.weight)\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "            if isinstance(p, nn.Linear):\n",
    "                init.kaiming_uniform_(p.weight, a=1.0)\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        state, next_state, action = inputs\n",
    "\n",
    "        # We can the encoded value of current state and next state\n",
    "        encode_state = self.feature(state)\n",
    "        encode_next_state = self.feature(next_state)\n",
    "        # We inverse predict the value of the action\n",
    "        pred_action = torch.cat((encode_state, encode_next_state), 1)\n",
    "        pred_action = self.inverse_net(pred_action)\n",
    "\n",
    "        # Then we use the predicted action value to predict the encoded next state\n",
    "        pred_next_state_feature_orig = torch.cat((encode_state, action), 1)\n",
    "        pred_next_state_feature_orig = self.forward_net_1(pred_next_state_feature_orig)\n",
    "\n",
    "        # residual\n",
    "        for i in range(2):\n",
    "            pred_next_state_feature = self.residual[i * 2](torch.cat((pred_next_state_feature_orig, action), 1))\n",
    "            pred_next_state_feature_orig = self.residual[i * 2 + 1](\n",
    "                torch.cat((pred_next_state_feature, action), 1)) + pred_next_state_feature_orig\n",
    "\n",
    "        pred_next_state_feature = self.forward_net_2(torch.cat((pred_next_state_feature_orig, action), 1))\n",
    "\n",
    "        real_next_state_feature = encode_next_state\n",
    "        return real_next_state_feature, pred_next_state_feature, pred_action\n",
    "    \n",
    "    def compute_intrinsic_reward(self, state, next_state, action):\n",
    "        action_onehot = torch.zeros(len(action), self.output_size, device=self.device)\n",
    "        action_onehot.scatter_(1, action.view(len(action), -1), 1)\n",
    "\n",
    "        real_next_state_feature, pred_next_state_feature, pred_action = self.forward([state, next_state, action_onehot])\n",
    "        intrinsic_reward = self.eta * F.mse_loss(real_next_state_feature, pred_next_state_feature, reduction='none').mean(-1)\n",
    "        return intrinsic_reward\n",
    "    \n",
    "    def inference(self, states, next_states, actions):\n",
    "        action_onehot = torch.zeros(len(actions), self.output_size, device=self.device)\n",
    "        action_onehot.scatter_(1, actions.view(-1, 1).long(), 1)\n",
    "\n",
    "        real_next_state_feature, pred_next_state_feature, pred_action = self.forward([states, next_states, action_onehot])\n",
    "        return real_next_state_feature, pred_next_state_feature, pred_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingSumOfReward:\n",
    "    def __init__(self, gamma):\n",
    "        self.moving_sum_of_reward = None\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, rews):\n",
    "        if self.moving_sum_of_reward is None:\n",
    "            self.moving_sum_of_reward = rews\n",
    "        else:\n",
    "            self.moving_sum_of_reward = self.moving_sum_of_reward * self.gamma + rews\n",
    "        return self.moving_sum_of_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.2.2 Main Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action space of the environment is Discrete(4)\n",
      "The observation space of the environment is Box(0, 255, (4, 84, 84), uint8)\n"
     ]
    }
   ],
   "source": [
    "env_id = params[\"env_id\"]\n",
    "exp_name = params[\"exp_name\"]\n",
    "seed = params[\"seed\"]\n",
    "run_name = f\"{env_id}__{exp_name}__{seed}__{int(time.time())}\"\n",
    "\n",
    "envs = envpool.make(\n",
    "    params[\"env_id\"],\n",
    "    env_type=\"gym\",\n",
    "    num_envs=params[\"num_envs\"],\n",
    "    episodic_life=True,\n",
    "    reward_clip=True,\n",
    "    seed=params[\"seed\"],\n",
    "    repeat_action_probability=0.25,\n",
    ")\n",
    "\n",
    "envs.num_envs = params[\"num_envs\"]\n",
    "envs.single_action_space = envs.action_space\n",
    "print(f\"The action space of the environment is {envs.single_action_space}\")\n",
    "envs.single_observation_space = envs.observation_space\n",
    "print(f\"The observation space of the environment is {envs.single_observation_space}\")\n",
    "\n",
    "envs = RecordEpisodeStatistics(envs)\n",
    "assert isinstance(envs.action_space, gym.spaces.Discrete), \"only discrete action space is supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = envs.single_observation_space\n",
    "output_size = envs.single_action_space.n\n",
    "\n",
    "is_load_model = False\n",
    "is_render = False\n",
    "use_cuda = params[\"cuda\"]\n",
    "use_gae = True\n",
    "use_noisy_net = False\n",
    "lam = params[\"gae_lambda\"]\n",
    "num_step = params[\"num_steps\"]\n",
    "learning_rate = params[\"learning_rate\"]\n",
    "entropy_coef = params[\"ent_coef\"]\n",
    "gamma = params[\"gamma\"]\n",
    "clip_grad_norm = params[\"max_grad_norm\"]\n",
    "pre_obs_norm_step = params[\"num_iterations_obs_norm_init\"]\n",
    "\n",
    "# Set up agent and model\n",
    "icm = ICMModel(input_size, output_size, device).to(device)\n",
    "Agent = PPOAgent(envs, use_int_rews=True).to(device)\n",
    "\n",
    "combined_parameters = list(Agent.parameters() ) + list(icm.parameters())\n",
    "optimizer = optim.Adam(\n",
    "    combined_parameters,\n",
    "    lr=params[\"learning_rate\"],\n",
    "    eps=1e-5,\n",
    ")\n",
    "\n",
    "rew_runnning_mean_std = RunningMeanStd()\n",
    "obs_runnning_mean_std = RunningMeanStd(shape=(1, 1, 84, 84))            # normalizing observation\n",
    "discounted_reward = MovingSumOfReward(params[\"int_gamma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((params[\"num_steps\"]+1, params[\"num_envs\"]) + envs.single_observation_space.shape).to(device)  # (128, 4,, 4, 84, 84)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(params[\"num_envs\"] * params[\"num_steps\"])                      # 4 * 128\n",
    "minibatch_size = int(batch_size // params[\"num_minibatches\"])\n",
    "num_iterations = params[\"total_timesteps\"] // batch_size                        # 20000000/(4*128) -> num iterations\n",
    "global_step = 0\n",
    "tracking_global_step = 0\n",
    "start_time = time.time()\n",
    "\n",
    "next_obs = torch.Tensor(envs.reset()[0]).to(device)\n",
    "next_done = torch.zeros(params[\"num_envs\"]).to(device)\n",
    "\n",
    "results_ICM = {\"global_step\":[],\n",
    "                \"return_value\":[],\n",
    "                \"intrinsic_reward\":[]}\n",
    "\n",
    "print(\"Start to initialize observation normalization parameter.....\")\n",
    "next_ob = []\n",
    "for step in range(params[\"num_steps\"] * params[\"num_iterations_obs_norm_init\"]):\n",
    "    acs = np.random.randint(0, envs.single_action_space.n, size=(params[\"num_envs\"]))\n",
    "    s, r, d, _ = envs.step(acs)\n",
    "    next_ob += s[:, 3, :, :].reshape([-1, 1, 84, 84]).tolist()\n",
    "\n",
    "    if len(next_ob) % (params[\"num_steps\"] * params[\"num_envs\"]) == 0:\n",
    "        next_ob = np.stack(next_ob)\n",
    "        obs_runnning_mean_std.update(next_ob)\n",
    "        next_ob = []\n",
    "print(\"End to initialize...\")\n",
    "\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    actions = torch.zeros((params[\"num_steps\"], params[\"num_envs\"]) + envs.single_action_space.shape).to(device)   # (128, 4) \n",
    "    logprobs = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    rewards = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    surprise_rewards = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    dones = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    ext_values = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    int_values = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    # Calculate the new learning rate as according to the annealing rate if needed.\n",
    "    if params[\"anneal_lr\"]:\n",
    "        updated_lr = (1.0 - (iteration - 1.0) / num_iterations) * params[\"learning_rate\"]\n",
    "        optimizer.param_groups[0][\"lr\"] = updated_lr\n",
    "\n",
    "    for step in range(0, params[\"num_steps\"]):\n",
    "        global_step += 1 * params[\"num_envs\"]\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        with torch.no_grad():\n",
    "            value_ext, value_int = Agent.get_value(obs[step])                   # -> get the extrinsic and intrinsic value for current observation\n",
    "            ext_values[step], int_values[step] = (\n",
    "                value_ext.flatten(),\n",
    "                value_int.flatten(),\n",
    "            )\n",
    "            action, logprob, _, _, _ = Agent.get_action_and_value(obs[step])\n",
    "        \n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob        \n",
    "        next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "        icm_obs = (                (\n",
    "                    (obs[step][:, 3, :, :].reshape(params[\"num_envs\"], 1, 84, 84).cpu() - torch.from_numpy(obs_runnning_mean_std.mean))\n",
    "                    / torch.sqrt(torch.from_numpy(obs_runnning_mean_std.var))\n",
    "                ).clip(-5, 5)\n",
    "            ).float()\n",
    "        icm_next_obs = (\n",
    "                (\n",
    "                    (next_obs[:, 3, :, :].reshape(params[\"num_envs\"], 1, 84, 84).cpu() - torch.from_numpy(obs_runnning_mean_std.mean))\n",
    "                    / torch.sqrt(torch.from_numpy(obs_runnning_mean_std.var))\n",
    "                ).clip(-5, 5)\n",
    "            ).float()\n",
    "        \n",
    "        surprise_rewards[step] = icm.compute_intrinsic_reward(icm_obs.to(device), icm_next_obs.to(device), actions[step].long())\n",
    "\n",
    "        for idx, d in enumerate(done):\n",
    "            # If done and no more live -> Get return\n",
    "            if d and info[\"lives\"][idx] == 0:\n",
    "                print(\n",
    "                    f\"global_step={global_step}, episodic_return={info['r'][idx]}, surprise_reward={np.mean(surprise_rewards[step].data.cpu().numpy())}\"\n",
    "                )\n",
    "                if global_step - tracking_global_step > 10000:\n",
    "                    results_ICM[\"global_step\"].append(global_step)\n",
    "                    results_ICM[\"return_value\"].append(info['r'][idx])\n",
    "                    results_ICM[\"intrinsic_reward\"].append(np.mean(surprise_rewards[step].data.cpu().numpy()))\n",
    "                    tracking_global_step = global_step\n",
    "            \n",
    "    obs[-1] = next_obs\n",
    "\n",
    "    # Calculate the discounted reward \n",
    "    surprise_reward_per_env = np.array(\n",
    "        [discounted_reward.update(reward_per_step) for reward_per_step in surprise_rewards.cpu().data.numpy().T]\n",
    "    )\n",
    "\n",
    "    mean, std, count = (\n",
    "        np.mean(surprise_reward_per_env),\n",
    "        np.std(surprise_reward_per_env),\n",
    "        len(surprise_reward_per_env),\n",
    "    )\n",
    "    \n",
    "    rew_runnning_mean_std.update_from_moments(mean, std**2, count)\n",
    "\n",
    "    # Normalize the curiousity_rewards based on the running_mean_std\n",
    "    surprise_rewards /= np.sqrt(rew_runnning_mean_std.var)\n",
    "\n",
    "    # Calculate value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value_ext, next_value_int = Agent.get_value(next_obs)\n",
    "        next_value_ext, next_value_int = next_value_ext.reshape(1, -1), next_value_int.reshape(1, -1)   # -> get next state values external & internal\n",
    "        ext_advantages = torch.zeros_like(rewards, device=device)\n",
    "        int_advantages = torch.zeros_like(surprise_rewards, device=device)\n",
    "        ext_lastgaelam = 0\n",
    "        int_lastgaelam = 0\n",
    "        for t in reversed(range(params[\"num_steps\"])):\n",
    "            if t == params[\"num_steps\"] - 1:\n",
    "                ext_nextnonterminal = 1.0 - next_done\n",
    "                int_nextnonterminal = 1.0\n",
    "                ext_nextvalues = next_value_ext\n",
    "                int_nextvalues = next_value_int\n",
    "            else:\n",
    "                ext_nextnonterminal = 1.0 - dones[t + 1]\n",
    "                int_nextnonterminal = 1.0\n",
    "                ext_nextvalues = ext_values[t + 1]\n",
    "                int_nextvalues = int_values[t + 1]\n",
    "            ext_delta = rewards[t] + params[\"gamma\"] * ext_nextvalues * ext_nextnonterminal - ext_values[t]\n",
    "            int_delta = surprise_rewards[t] + params[\"int_gamma\"] * int_nextvalues * int_nextnonterminal - int_values[t]\n",
    "            ext_advantages[t] = ext_lastgaelam = (\n",
    "                ext_delta + params[\"gamma\"] * params[\"gae_lambda\"] * ext_nextnonterminal * ext_lastgaelam\n",
    "            )\n",
    "            int_advantages[t] = int_lastgaelam = (\n",
    "                int_delta + params[\"int_gamma\"] * params[\"gae_lambda\"] * int_nextnonterminal * int_lastgaelam\n",
    "            )\n",
    "        ext_returns = ext_advantages + ext_values\n",
    "        int_returns = int_advantages + int_values\n",
    "    \n",
    "    # Collect batch data for optimization\n",
    "    b_obs = obs[:-1].reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_next_obs = obs[1:].reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape(-1)\n",
    "    b_ext_advantages = ext_advantages.reshape(-1)\n",
    "    b_int_advantages = int_advantages.reshape(-1)\n",
    "    b_ext_returns = ext_returns.reshape(-1)\n",
    "    b_int_returns = int_returns.reshape(-1)\n",
    "    b_ext_values = ext_values.reshape(-1)\n",
    "\n",
    "    b_advantages = b_int_advantages * params[\"int_coef\"] + b_ext_advantages * params[\"ext_coef\"]\n",
    "\n",
    "    obs_runnning_mean_std.update(b_obs[:, 3, :, :].reshape(-1, 1, 84, 84).cpu().numpy())\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(batch_size)\n",
    "    icm_obs = (\n",
    "        (\n",
    "            (b_obs[:, 3, :, :].reshape(-1, 1, 84, 84).cpu() - torch.from_numpy(obs_runnning_mean_std.mean))\n",
    "            / torch.sqrt(torch.from_numpy(obs_runnning_mean_std.var))\n",
    "        ).clip(-5, 5)\n",
    "    ).float()\n",
    "\n",
    "    icm_next_obs = (\n",
    "        (\n",
    "            (b_next_obs[:, 3, :, :].reshape(-1, 1, 84, 84).cpu() - torch.from_numpy(obs_runnning_mean_std.mean))\n",
    "            / torch.sqrt(torch.from_numpy(obs_runnning_mean_std.var))\n",
    "        ).clip(-5, 5)\n",
    "    ).float()\n",
    "\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    forward_mse = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(params[\"update_epochs\"]):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            real_next_state_feature, pred_next_state_feature, pred_action = icm.inference(icm_obs[mb_inds].to(device), icm_next_obs[mb_inds].to(device), b_actions[mb_inds])\n",
    "            \n",
    "            # Calculate the cross entropy loss of the action.\n",
    "            inverse_loss = ce(\n",
    "                    pred_action, b_actions[mb_inds].long())\n",
    "            # Calculate the mse loss of the forward next state prediction.\n",
    "            forward_loss = forward_mse(\n",
    "                    pred_next_state_feature, real_next_state_feature.detach())\n",
    "\n",
    "            _, newlogprob, entropy, new_ext_values, new_int_values = Agent.get_action_and_value(\n",
    "                b_obs[mb_inds], b_actions.long()[mb_inds]\n",
    "            )\n",
    "            \n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if params[\"norm_adv\"]:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - params[\"clip_coef\"], 1 + params[\"clip_coef\"])\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            new_ext_values, new_int_values = new_ext_values.view(-1), new_int_values.view(-1)\n",
    "            if params[\"clip_vloss\"]:\n",
    "                ext_value_loss_unclipped = (new_ext_values - b_ext_returns[mb_inds]) ** 2\n",
    "                ext_v_clipped = b_ext_values[mb_inds] + torch.clamp(\n",
    "                    new_ext_values - b_ext_values[mb_inds],\n",
    "                    -params[\"clip_coef\"],\n",
    "                params[\"clip_coef\"],\n",
    "                )\n",
    "                ext_value_loss_clipped = (ext_v_clipped - b_ext_returns[mb_inds]) ** 2\n",
    "                ext_value_loss_max = torch.max(ext_value_loss_unclipped, ext_value_loss_clipped)\n",
    "                ext_value_loss = 0.5 * ext_value_loss_max.mean()\n",
    "            else:\n",
    "                ext_value_loss = 0.5 * ((new_ext_values - b_ext_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            int_value_loss = 0.5 * ((new_int_values - b_int_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            value_loss = ext_value_loss + int_value_loss\n",
    "            entropy_loss = entropy.mean()\n",
    "\n",
    "            loss = pg_loss - params[\"ent_coef\"] * entropy_loss + value_loss * params[\"vf_coef\"] + forward_loss + inverse_loss\n",
    "        \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if params[\"max_grad_norm\"]:\n",
    "                nn.utils.clip_grad_norm_(\n",
    "                    combined_parameters,\n",
    "                    params[\"max_grad_norm\"],\n",
    "                )\n",
    "            optimizer.step()\n",
    "\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(Agent, \"pretrained_models/ppo_for_ICM_atari.pth\")\n",
    "# torch.save(icm, \"pretrained_models/icm_atari.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the saved PPO agent\n",
    "# agent = torch.load(\"pretrained_models/ppo_for_ICM_atari.pth\")\n",
    "# # Load the saved ICM model\n",
    "# icm = torch.load(\"pretrained_models/icm_atari.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from results_RND\n",
    "icm_global_step = results_ICM[\"global_step\"]\n",
    "icm_return_value = results_ICM[\"return_value\"]\n",
    "icm_intrinsic_reward = results_ICM[\"intrinsic_reward\"]\n",
    "\n",
    "df_icm = pd.DataFrame({'global_step': icm_global_step, 'return_value': icm_return_value, 'intrinsic_reward': icm_intrinsic_reward})\n",
    "# Save DataFrames to CSV files\n",
    "df_icm.to_csv('data/results_icm_atari.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Results Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simple_PPO = pd.read_csv('data/results_simple_ppo_atari.csv')\n",
    "df_rnd = pd.read_csv('data/results_rnd_atari.csv')\n",
    "df_icm = pd.read_csv('data/results_icm_atari.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_simple_PPO, df_rnd, df_icm]\n",
    "\n",
    "for df in dfs:\n",
    "    df[\"return_value_smoothed\"] = df[\"return_value\"].ewm(alpha=1-0.9).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC6HElEQVR4nOydd3wT9f/Hn2nTvVugA8reW1AREZQloKAo4kRBcYMbB27Ur7hBfuBW3CIo7i1LkCF7KHtDW1oo3W2aJvn9cb3kLrmkaSl08H4+Hnnk7nOf+9wntZJX39PkcDgcCIIgCIIg1EECanoDgiAIgiAIVUWEjCAIgiAIdRYRMoIgCIIg1FlEyAiCIAiCUGcRISMIgiAIQp1FhIwgCIIgCHUWETKCIAiCINRZRMgIgiAIglBnESEjCIIgCEKdRYSMINQxxo0bR/Pmzat079NPP43JZKreDQkV8sknn9C+fXuCgoKIjY2t6e0IQr1ChIwgVBMmk8mv1+LFi2t6q8IpZNu2bYwbN45WrVrx7rvv8s4773idm56eziOPPEL//v2JioqS3xdB8AOT9FoShOrh008/1Z1//PHH/PHHH3zyySe68cGDB5OYmFjl51itVux2OyEhIZW+t6ysjLKyMkJDQ6v8fKFyvPXWW9xxxx3s3LmT1q1b+5y7ePFi+vfvT5s2bWjQoAErVqxg0aJFXHDBBadms4JQBzHX9AYEob4wZswY3fnKlSv5448/PMbdKSoqIjw83O/nBAUFVWl/AGazGbO5fv5vX1ZWht1uJzg4uKa3oiMzMxPAL5dSz549OXbsGPHx8Xz11VeMHj36hJ9f2d8vQahriGtJEE4hF1xwAZ07d2bt2rX069eP8PBwHn30UQC+++47Lr74YlJSUggJCaFVq1Y8++yz2Gw23RruMTL79u3DZDLxyiuv8M4779CqVStCQkI466yzWL16te5eoxgZk8nExIkT+fbbb+ncuTMhISF06tSJX3/91WP/ixcv5swzzyQ0NJRWrVrx9ttvVyruZtWqVVx00UXExcURERFB165def3113U/HyPrg6/PPH36dOdnXr9+PWazmSlTpnissX37dkwmEzNnznSO5eTkcO+995KamkpISAitW7fmxRdfxG63+/V53njjDTp16kRISAgpKSlMmDCBnJwc5/XmzZvz1FNPAdCwYUNMJhNPP/201/WioqKIj4/369lG+Pr98vbs5s2bM27cOOf5hx9+iMlk4u+//+b++++nYcOGREREcNlll5GVlVXlvQnCyaJ+/mkmCLWYY8eOMWzYMK6++mrGjBnjdDN9+OGHREZGcv/99xMZGcnChQt58sknycvL4+WXX65w3c8//5z8/Hxuu+02TCYTL730Epdffjl79uyp0IqzbNky5s+fz5133klUVBQzZsxg1KhRHDhwgISEBADWr1/P0KFDSU5OZsqUKdhsNp555hkaNmzo1+f+448/GD58OMnJydxzzz0kJSWxdetWfvzxR+655x6/1nBn9uzZlJSUcOuttxISEkJycjLnn38+c+fOdQoIlS+//JLAwECnlaOoqIjzzz+fw4cPc9ttt9G0aVOWL1/O5MmTSU9PZ/r06T6f/fTTTzNlyhQGDRrEHXfcwfbt23nzzTdZvXo1f//9N0FBQUyfPp2PP/6Yb775hjfffJPIyEi6du1apc/qL95+vyrLXXfdRVxcHE899RT79u1j+vTpTJw4kS+//LKadywIJ4hDEISTwoQJExzu/4udf/75DsDx1ltvecwvKiryGLvtttsc4eHhjpKSEufY2LFjHc2aNXOe79271wE4EhISHNnZ2c7x7777zgE4fvjhB+fYU0895bEnwBEcHOzYtWuXc2zjxo0OwPF///d/zrERI0Y4wsPDHYcPH3aO7dy502E2mz3WdKesrMzRokULR7NmzRzHjx/XXbPb7c7j888/33H++ed73O/tM0dHRzsyMzN1c99++20H4Ni8ebNuvGPHjo4BAwY4z5999llHRESEY8eOHbp5jzzyiCMwMNBx4MABr58nMzPTERwc7LjwwgsdNpvNOT5z5kwH4Pjggw+cY+rPPCsry+t6RsybN88BOBYtWuT3Pb5+vwDHU0895THerFkzx9ixY53ns2fPdgCOQYMG6f7b3HfffY7AwEBHTk5OZT6GIJx0xLUkCKeYkJAQbrzxRo/xsLAw53F+fj5Hjx6lb9++FBUVsW3btgrXveqqq4iLi3Oe9+3bF4A9e/ZUeO+gQYNo1aqV87xr165ER0c777XZbPz555+MHDmSlJQU57zWrVszbNiwCtdfv349e/fu5d577/WIFTmRdPBRo0Z5WIQuv/xyzGazznKwZcsW/vvvP6666irn2Lx58+jbty9xcXEcPXrU+Ro0aBA2m42//vrL63P//PNPSktLuffeewkIcP0zessttxAdHc1PP/1U5c90onj7/aost956q+6/Td++fbHZbOzfv/+E1xaE6kRcS4JwimncuLFhQOq///7L448/zsKFC8nLy9Ndy83NrXDdpk2b6s5VUXP8+PFK36ver96bmZlJcXGxYdZNRZk4ALt37wagc+fOFc6tDC1atPAYa9CgAQMHDmTu3Lk8++yzgOJWMpvNXH755c55O3fuZNOmTV5dY2qQrhHql3m7du1048HBwbRs2bJGv+y9/X5VlhP5fRKEU4kIGUE4xWgtLyo5OTmcf/75REdH88wzz9CqVStCQ0NZt24dDz/8sF/Bp4GBgYbjDj8qLJzIvdWJyWQyfKZ7wLOK0c8S4Oqrr+bGG29kw4YNdO/enblz5zJw4EAaNGjgnGO32xk8eDAPPfSQ4Rpt27atwieoebz9TLzh7WdbW34nBKEiRMgIQi1g8eLFHDt2jPnz59OvXz/n+N69e2twVy4aNWpEaGgou3bt8rhmNOaO6rbasmULgwYN8jovLi7O0BVWWQvHyJEjue2225zupR07djB58mSPPRUUFPjcjzeaNWsGKJlQLVu2dI6Xlpayd+/eKq15somLi9NlVIGy3/T09JrZkCBUExIjIwi1APWvX+1fu6Wlpbzxxhs1tSUdgYGBDBo0iG+//Za0tDTn+K5du/jll18qvL9Hjx60aNGC6dOne3yZaj9zq1at2LZtmy7Nd+PGjfz999+V2m9sbCxDhgxh7ty5zJkzh+DgYEaOHKmbc+WVV7JixQp+++03j/tzcnIoKyvzuv6gQYMIDg5mxowZuv2///775ObmcvHFF1dqv6eCVq1aecT9vPPOO14tMoJQVxCLjCDUAs4991zi4uIYO3Ysd999NyaTiU8++aRWmfGffvppfv/9d/r06cMdd9yBzWZj5syZdO7cmQ0bNvi8NyAggDfffJMRI0bQvXt3brzxRpKTk9m2bRv//vuvU0zcdNNNvPbaawwZMoTx48eTmZnJW2+9RadOnTzihiriqquuYsyYMbzxxhsMGTLEI8j4wQcf5Pvvv2f48OGMGzeOnj17UlhYyObNm/nqq6/Yt2+fzhWlpWHDhkyePJkpU6YwdOhQLrnkErZv384bb7zBWWedVWERRF8899xzgBIzBUqfpmXLlgHw+OOPV3ndm2++mdtvv51Ro0YxePBgNm7cyG+//eb1MwpCXUGEjCDUAhISEvjxxx954IEHePzxx4mLi2PMmDEMHDiQIUOG1PT2AKXq7C+//MKkSZN44oknSE1N5ZlnnmHr1q1+ZVUNGTKERYsWMWXKFF599VXsdjutWrXilltucc7p0KEDH3/8MU8++ST3338/HTt25JNPPuHzzz+vdM+hSy65hLCwMPLz83XZSirh4eEsWbKE559/nnnz5vHxxx8THR1N27ZtmTJlCjExMT7Xf/rpp2nYsCEzZ87kvvvuIz4+nltvvZXnn3/+hKovP/HEE7rzDz74wHl8IkLmlltuYe/evbz//vv8+uuv9O3blz/++IOBAwdWeU1BqA1IryVBEE6IkSNH8u+//7Jz586a3oogCKchEiMjCILfFBcX68537tzJzz//LE0NBUGoMcQiIwiC3yQnJzNu3DhnrZQ333wTi8XC+vXradOmTU1vTxCE0xCJkREEwW+GDh3KF198QUZGBiEhIfTu3Zvnn39eRIwgCDWGWGQEQRAEQaizSIyMIAiCIAh1FhEygiAIgiDUWep9jIzdbictLY2oqKgT6rIrCIIgCMKpw+FwkJ+fT0pKiq7LvDv1XsikpaWRmppa09sQBEEQBKEKHDx4kCZNmni9Xu+FTFRUFKD8IKKjo2t4N4IgCIIg+ENeXh6pqanO73Fv1Hsho7qToqOjRcgIgiAIQh2jorAQCfYVBEEQBKHOIkJGEARBEIQ6iwgZQRAEQRDqLPU+RsZfbDYbVqu1prchnEKCgoIIDAys6W0IgiAIJ8BpL2QcDgcZGRnk5OTU9FaEGiA2NpakpCSpMSQIglBHOe2FjCpiGjVqRHh4uHyhnSY4HA6KiorIzMwElK7OgiAIQt3jtBYyNpvNKWISEhJqejvCKSYsLAyAzMxMGjVqJG4mQRCEOshpHeyrxsSEh4fX8E6EmkL9by/xUYIgCHWT01rIqIg76fRF/tsLgiDUbUTICIIgCIJQZxEhU08xmUx8++23J/05F1xwAffee+9Jf44gCIIgGCFCpg6SlZXFHXfcQdOmTQkJCSEpKYkhQ4bw999/O+ekp6czbNiwGtyl/zRv3hyTyYTJZCIiIoIePXowb9485/Wnn37aed1sNtO8eXPuu+8+CgoKdOt89NFHnHXWWYSHhxMVFcX555/Pjz/+eKo/jiAIgnAKESFTBxk1ahTr16/no48+YseOHXz//fdccMEFHDt2zDknKSmJkJCQGtxl5XjmmWdIT09n/fr1nHXWWVx11VUsX77ceb1Tp06kp6ezb98+XnzxRd555x0eeOAB5/VJkyZx2223cdVVV7Fp0yb++ecfzjvvPC699FJmzpxZEx9JEAShXlJSVlLTW9AhQqaOkZOTw9KlS3nxxRfp378/zZo14+yzz2by5Mlccsklznla19K+ffswmUzMnTuXvn37EhYWxllnncWOHTtYvXo1Z555JpGRkQwbNoysrCznGuPGjWPkyJFMmTKFhg0bEh0dze23305paanX/VksFiZNmkTjxo2JiIigV69eLF68uMLPFRUVRVJSEm3btmXWrFmEhYXxww8/OK+bzWaSkpJo0qQJV111Fddddx3ff/89ACtXruTVV1/l5ZdfZtKkSbRu3ZoOHTrwv//9j3vvvZf777+fgwcPVvInLQiCILjz79F/Oeuzs3jxnxdreitORMhocDgcFJWW1cjL4XD4tcfIyEgiIyP59ttvsVgslfp8Tz31FI8//jjr1q3DbDZz7bXX8tBDD/H666+zdOlSdu3axZNPPqm7Z8GCBWzdupXFixfzxRdfMH/+fKZMmeL1GRMnTmTFihXMmTOHTZs2MXr0aIYOHcrOnTv93qfZbCYoKMinYAoLC3Ne/+KLL4iMjOS2227zmPfAAw9gtVr5+uuv/X6+IAiCYMz/bfg/AD7d+mkN78TFaV0Qz51iq42OT/5WI8/+75khhAdX/J/DbDbz4Ycfcsstt/DWW2/Ro0cPzj//fK6++mq6du3q895JkyYxZMgQAO655x6uueYaFixYQJ8+fQAYP348H374oe6e4OBgPvjgA8LDw+nUqRPPPPMMDz74IM8++ywBAXodfODAAWbPns2BAwdISUlxPvPXX39l9uzZPP/88xV+vtLSUl599VVyc3MZMGCA4Zy1a9fy+eefO6/v2LGDVq1aERwc7DE3JSWF6OhoduzYUeGzBUEQBN9EBUXV9BY8EItMHWTUqFGkpaXx/fffM3ToUBYvXkyPHj08RIg7WqGTmJgIQJcuXXRjasl+lW7duukKBvbu3ZuCggJDV83mzZux2Wy0bdvWaTmKjIxkyZIl7N692+feHn74YSIjIwkPD+fFF1/khRde4OKLL9atHRkZSVhYGGeffTa9e/fWxb74a9ESBEEQqk5UcO0TMmKR0RAWFMh/zwypsWdXhtDQUAYPHszgwYN54oknuPnmm3nqqacYN26c13uCgoKcx2ohOPcxu91euY1rKCgoIDAwkLVr13qU+4+MjPR574MPPsi4ceOIjIwkMTHRo1Bdu3bt+P777zGbzaSkpOisL23btmXZsmWUlpZ6WGXS0tLIy8ujbdu2Vf5cgiAIgkJksO9/y2sCETIaTCaTX+6d2kjHjh1PSt2YjRs3Ulxc7OxLtHLlSiIjI0lNTfWYe8YZZ2Cz2cjMzKRv376Vek6DBg1o3bq11+vBwcFer1999dXMmDGDt99+m7vuukt37ZVXXiEoKIhRo0ZVaj+CIAiCJ1rXks1uIzCg5nvU1c1v7dOYY8eOMXr0aG666Sa6du1KVFQUa9as4aWXXuLSSy+t9ueVlpYyfvx4Hn/8cfbt28dTTz3FxIkTPeJjQLGMXHfdddxwww28+uqrnHHGGWRlZbFgwQK6du2qcxVVJ7179+aee+7hwQcfpLS0lJEjR2K1Wvn00095/fXXmT59uqHwEgRBECqH1iJTYC0gJiSmBnejIEKmjhEZGUmvXr2YNm0au3fvxmq1kpqayi233MKjjz5a7c8bOHAgbdq0oV+/flgsFq655hqefvppr/Nnz57Nc889xwMPPMDhw4dp0KAB55xzDsOHD6/2vWmZPn06Xbt25Y033uDxxx8nMDCQHj168O233zJixIiT+mxBEITThUCTywKTV5pXK4SMyVHPoyTz8vKIiYkhNzeX6Oho3bWSkhL27t1LixYtCA0NraEd1l7GjRtHTk7OKWl1UFPI74AgCIL/fLntS55b9ZxyPPxLOiZ0PGnP8vX9rUWylgRBEARB8AsHLttHqc17ra9TiQgZQRAEQRD8QitkrHZrDe7EhcTICF6pqC6NIAiCcHqhjUYRi4wgCIIgCHUKcS0JgiAIglBn0VpkaotrSYSMIAiCIAh+obPI2MUiIwiCIAhCHUJnkbGJRUYQBEEQhDqExMgIgiAIglAvkBgZQRAEQRDqFHaH3XksMTLCCTFu3DhMJhMmk4mgoCBatGjBQw89RElJiXOOyWQiNDSU/fv36+4dOXIk48aN87pWYmIigwcP5oMPPsButyMIgiAIIK4loZoZOnQo6enp7Nmzh2nTpvH222/z1FNP6eaYTCaefPJJv9fat28fv/zyC/379+eee+5h+PDhlJWVnayPIAiCINQhJP3agMOHDzNmzBgSEhIICwujS5curFmzxnnd4XDw5JNPkpycTFhYGIMGDWLnzp01uOPaQ0hICElJSaSmpjJy5EgGDRrEH3/8oZszceJEPv30U7Zs2eLXWo0bN6ZHjx48+uijfPfdd/zyyy9S4VcQBEEA3FoUSNYSHD9+nD59+hAUFMQvv/zCf//9x6uvvkpcXJxzzksvvcSMGTN46623WLVqFREREQwZMkTnQqk2HA4oLayZ1wk2Id+yZQvLly8nODhYN96nTx+GDx/OI488Uuk1BwwYQLdu3Zg/f/4J7U0QBEGof9SWGJka7bX04osvkpqayuzZs51jLVq0cB47HA6mT5/O448/zqWXXgrAxx9/TGJiIt9++y1XX3119W7IWgTPp1Tvmv7yaBoER1Tqlh9//JHIyEjKysqwWCwEBAQwc+ZMj3lTp06la9euLF26lL59+1bqGe3bt2fTpk2VukcQBEGon2iDfcUiA3z//feceeaZjB49mkaNGnHGGWfw7rvvOq/v3buXjIwMBg0a5ByLiYmhV69erFixwnBNi8VCXl6e7lVf6d+/Pxs2bGDVqlWMHTuWG2+8kVGjRnnM69ixIzfccEOVrDIOhwOTyVQd2xUEQRDqOLqmkWKRgT179vDmm29y//338+ijj7J69WruvvtugoODGTt2LBkZGQAkJibq7ktMTHRec2fq1KlMmTKlahsKClcsIzVBUHilb4mIiKB169YAfPDBB3Tr1o3333+f8ePHe8ydMmUKbdu25dtvv63UM7Zu3aqzkgmCIAinL7Uxa6lGhYzdbufMM8/k+eefB+CMM85gy5YtvPXWW4wdO7ZKa06ePJn777/feZ6Xl0dqaqp/N5tMlXbv1BYCAgJ49NFHuf/++7n22msJCwvTXU9NTWXixIk8+uijtGrVyq81Fy5cyObNm7nvvvtOxpYFQRCEOoYu2FeyliA5OZmOHTvqxjp06MCBAwcASEpKAuDIkSO6OUeOHHFecyckJITo6Gjd63Rh9OjRBAYGMmvWLMPrkydPJi0tjT///NPjmsViISMjg8OHD7Nu3Tqef/55Lr30UoYPH84NN9xwsrcuCIIg1AU0eSkSI4OSUbN9+3bd2I4dO2jWrBmgBP4mJSWxYMEC5/W8vDxWrVpF7969T+le6wJms5mJEyfy0ksvUVhY6HE9Pj6ehx9+2DDj69dffyU5OZnmzZszdOhQFi1axIwZM/juu+8IDAw8FdsXBEEQajm1sfu1yeE4wbzfE2D16tWce+65TJkyhSuvvJJ//vmHW265hXfeeYfrrrsOUDKbXnjhBT766CNatGjBE088waZNm/jvv/8IDQ2t8Bl5eXnExMSQm5vrYZ0pKSlh7969tGjRwq+1hPqH/A4IgiD4z8z1M3l709sAnJl4JrOHzq7gjqrj6/tbS43GyJx11ll88803TJ48mWeeeYYWLVowffp0p4gBeOihhygsLOTWW28lJyeH8847j19//VW+dARBEAThFFMbY2RqVMgADB8+nOHDh3u9bjKZeOaZZ3jmmWdO4a4EQRAEQXBHl35dS7KWarxFgSAIgiAIdY/aYpERISMIgiAIgl/UxjoyImQEQRAEQfAL6X4tCIIgCEKdxY6r15JYZARBEARBqFtoCrbUljoyImQEQRAEQfALbYxMmb2sBnfiQoSMIAiCIAh+IenXgiAIgiDUWbQWGZvDhs1uq8HdKIiQqaOMGzeOkSNHOs8zMjK46667aNmyJSEhIaSmpjJixAhdn6rmzZtjMpmYM2eOx3qdOnXCZDLx4YcfnoLdC4IgCHURu8OuO68NmUsiZOoB+/bto2fPnixcuJCXX36ZzZs38+uvv9K/f38mTJigm5uamsrs2freGCtXriQjI4OIiIhTuW1BEAShjlMbAn5rvEWBcOLceeedmEwm/vnnH50Y6dSpEzfddJNu7nXXXce0adM4ePAgqampAHzwwQdcd911fPzxx6d034IgCELdQutagtoRJyNCRoPD4aC4rLhGnh1mDsNkMlX6vuzsbH799Vf+97//GVpUYmNjdeeJiYkMGTKEjz76iMcff5yioiK+/PJLlixZIkJGEARB8Ik22BfAaqt515IIGQ3FZcX0+rxXjTx71bWrCA8Kr/R9u3btwuFw0L59e7/vuemmm3jggQd47LHH+Oqrr2jVqhXdu3ev9LMFQRCE0wt3i4zEyAgnjLs69oeLL76YgoIC/vrrLz744AMP95MgCIIgGOH+nWP54wl4uQ2s/6yGdiQWGR1h5jBWXbuqxp5dFdq0aYPJZGLbtm1+32M2m7n++ut56qmnWLVqFd98802Vni0IgiCcXnjEyBQdg8JMoPJ/VFcXImQ0mEymKrl3apL4+HiGDBnCrFmzuPvuuz3iZHJycjziZEBxL73yyitcddVVxMXFnaLdCoIgCHUZd4vMwZKjtDZBSEh0De1IXEv1glmzZmGz2Tj77LP5+uuv2blzJ1u3bmXGjBn07t3b8J4OHTpw9OhRj1RsQRAEQfCGu0XmweBCzm6Wyl9FB2toR2KRqRe0bNmSdevW8b///Y8HHniA9PR0GjZsSM+ePXnzzTe93peQkHAKdykIgnCasnEOBIVDx0tqeicnjLuQAbCbTDSJblYDu1EQIVNHca/Am5yczMyZM5k5c6bXe/bt2+dzzZycnBPfmCAIguAiLx2+uU05fjIbAgJrdj8niFGCidnhIDWuTQ3sRkFcS4IgCIJwsijMch2XldTcPqoJI4tMU2sZQWHxNbAbBREygiAIgnCy0Fa+tdZMwdXqxMgi08pqhVAJ9hUEQRCE+ocl33VsLaq5fVQTjowtHmMty+xgDqmB3SiIkBEEQRCEk0VJruv4ZFpkcg5A6ckXSo68Qx5jLQk+6c/1hQgZqlYdV6gfyH97QRBOKpY81/HJsshk7YDpXeDd/idnfQ0OAzHWNLBqBV2ri9NayAQFBQFQVFT3zX1C1VD/26u/C4IgCNVKiVbInCSLzL/zlfcs/yu8Vwm7zVCMNQit2VIep3X6dWBgILGxsWRmZgIQHh5epQ7UQt3D4XBQVFREZmYmsbGxBAbW7ZRIQRBqKZZTIGRKC0/Ouu4UHMFukLUUH9301DzfC6e1kAFISkoCcIoZ4fQiNjbW+TsgCIJQ7ZyKGBmtkHE44GT9QZ6fYdhRKTiu5orhgQgZTCYTycnJNGrUCKu15tuRC6eOoKAgscQIgnBy0WUtnQIhU1YCQScpZsWSb9waMrJm/xg87YWMSmBgoHypCYIgCNWLVrycrGDf0gLNceGpFzLxLU7O8/zktA72FQRBEISTSpnFdXyyLDJagaQVNdWNxrqUFKFYYZoFxUCbC0/eM/1ALDKCIAiCcLLQtiU4WRYZrViynEQhU1rgtMiM6TCGlMgUeib2PHkxOX4iQkYQBEEQThanwiKjfYa3DKaibAiNObGmlZY87OWH5gAzg5sNrvpa1Yi4lgRBEAThZGHTCplTYJExci0d2w0vtYAvrjmx51jycZRbX0zUnlIlImQEQRAE4WShdS2drO7XWrG083dY8jLY7a6xdR+VX/vtxJ6jiZEJMNUe+SCuJUEQBEE4WZwS15JGIK16S3mPaw5dRyvHgV56IeUfgdxD0KSnf8/RZC2JRUYQBEEQ6jt/vQxHd7jOT8S1ZLfB7Ivgm9s9r5WVeo4d3+c61goZrbCacy28NwD+nuHfHrRCphZVwRchIwiCIAjVjbUEFj7nNnYCFpnDa2H/37DxC89rRi4rs9YKoxEdRceUd5sVDq9RjlfM9G8PlnzsFc865YiQEQRBEITqRhu3onKiFhnnsZucsBlYZAJDlPe8NFikEVSqkMne6xoLifJvD2KREQRBEITThDIjIXMiMTKamrruwsXIImMvU95Xv68fLzyqvB/b5RrTtlHwhTbYtxbJh9qzE0EQBEGoLxiJi+oK9tUKGVsZOAwcPt7qyWRuVd61zSxL8oznuqNNvxaLjCAIgiDUY6rbIuPwYpExcmGBq56Mu6DZ8Uv5XjRurrJiJWamIiRryZOnn34ak8mke7Vv3955vaSkhAkTJpCQkEBkZCSjRo3iyJEjNbhjQRAEQfCD6rbIaMWLViQZCSZQBIwlH1a9qR8/sKp8L27xOhVZZTI2Q1mxcdPIGqbGLTKdOnUiPT3d+Vq2bJnz2n333ccPP/zAvHnzWLJkCWlpaVx++eU1uFtBEARB8ANDi8wJBPvqrDAGosbk1nqgtBDWfew671JeU8ZmUe4pddvLth9gwTOKq8qIt/oCOLOWapNrqcYL4pnNZpKSkjzGc3Nzef/99/n8888ZMGAAALNnz6ZDhw6sXLmSc84551RvVRAEQRD8o7otMlphpBMy5c8xh4JV40YqLQBt9d2zbobN85RjS4GnqPrhHuU9oQ10N2ploLfF1KbKvjW+k507d5KSkkLLli257rrrOHDgAABr167FarUyaNAg59z27dvTtGlTVqxY4XU9i8VCXl6e7iUIgiAIpxQjIWOz6NOoK4M3i4x6bA6G5G6u8dJCfVxNai8ICleOLXnerUP56T63ITEybvTq1YsPP/yQX3/9lTfffJO9e/fSt29f8vPzycjIIDg4mNjYWN09iYmJZGRkeF1z6tSpxMTEOF+pqakn+VMIgiAIghtG1Xah6lYZXVyMVsiUB+kGBMEN30Hvicp5aaFLrJxxPZhMrnoxpQWeriWV0Bif23DUHv3ipEaFzLBhwxg9ejRdu3ZlyJAh/Pzzz+Tk5DB37twqrzl58mRyc3Odr4MHD1bjjgVBEATBD7w1iKyqkLF5cS3Zy4VMYBCExUHrci9GaaHrWaolRhUyb50HRzZ7eY5B9pImbsbR5GygdsXI1LhrSUtsbCxt27Zl165dJCUlUVpaSk5Ojm7OkSNHDGNqVEJCQoiOjta9BEEQBOGU4h7saw4rH6+qRcZLyrUqMgLKQ16DI5X30gKNkAnTXwNI32j8HKPieMXZ5QcmHOVriWvJCwUFBezevZvk5GR69uxJUFAQCxYscF7fvn07Bw4coHfv3jW4S0EQBEGoAHeLTFCo8l4tFhmN1URrkQEIjlDeta4l1SJj1AU72K09QamBkFHbGoTF1coYmRrNWpo0aRIjRoygWbNmpKWl8dRTTxEYGMg111xDTEwM48eP5/777yc+Pp7o6GjuuusuevfuLRlLgiAIQu1Ga5GJb6UIm+LjVU/BLvNSR0YbIwNuQsbNIqMWydMSlQTHNOLFYjBHbWsQnoCjXMpI1lI5hw4d4pprrqFdu3ZceeWVJCQksHLlSho2bAjAtGnTGD58OKNGjaJfv34kJSUxf/78mtyyIAiCIFSM1iIz5muXNeTH+yu3jpp5pLPIaI6dFhk311JZsUu4qEKmyxWe67vHuhiJHdUiE9EAh7qf2mOQqVmLzJw5c3xeDw0NZdasWcyaNesU7UgQBEEQqgHVatLzRohvAcfLu02nrfN/jW8nwOE1cOtiSN/kGte6lpwxMm4WGXBZUlTXUt8HoP1weH+wq9dSi/Ph6A7XPUYWmSJPi4y4lgRBEAShPuOMTwnzvOZweFpCjNjwqfK+9QfY+ZtrvMzAIqMG+5pDlCq/DhsUZnnuoWE7eHC30pIgPx2O/Aur33VdN4qRKSy3yIQn4HAoAkiEjCAIgiDUZ9TsnxCDzFmbVSlg5y+qZcV5v0EdGTXY12RS3EuWXE+LjEpgEEQkKC/3AGBtV2yV/eWtgyIa4CjNKX9M7REytSdaRxAEQRDqC04hE+l5zVuNGS3ankfucSvFOa5ju1v6NbjcS5ZyUWJkFVJp0EZ/fuRfJShZJS8d9v6lHLe8wBkjE1CL5EPt2YkgCIIg1BdU8RES5XnNW8dq3RxNmrbFrdVO3iHXsbtFBvRxMuBpkdFiMsGdK2HU+9CwgyKM9ixxXT++T3mPbgwt+jljZGqRZ0mEjCAIgiBUO6pFRs0iukaT3LLqTbDbPe/Roq03416kLlcjZOxu6dfgKZ58WWQAGnVQMpoSWinn/36jCJicgzB7qDIW2wxAgn0FQRAE4bTAPUam3TAIiVHcPUtfhegUpSO1w6FYQbQWFdDXm3GPkck1sshovs6jkvXzKxIyKqoA+u9b5XXGGNe1gEDl3WmQqT1CRiwygiAIglDdOIWMxjpi0QTSbiy30HxzO7zazpUZpKK1yBRkKu/x5RaT3EOu+jJ2t/RrgJjG+rV8uZa0uLuksvd6THFaZCTYVxAEQRDqMb6CfQEOrYbvJsKmOUrBuY2f669rhUxhuZBp1EF5Ly2Akhzl2ChGJjpFv5a/Fhl3IXNghet4yPMA2B2KS0wq+wqCIAhCfcZXsK/K+k9cx6qFRUUnZMpdSxENXTE3amaRkUUmuol+Lb8tMm6iq1y0cNWnkNxVGcJtn7UAETKCIAiCUJ3YylwxLkZ1ZAzxIWS0okgVJaXl66tCRhsjo7XImAI942+84W6RUdEIITX9WmJkBEEQBKE+4nDo6764Wzl83afFqLlkSLTLTaRed28aCfoYmaAw/6oI+9qrgUVHYmQEQRAEob7x62R4pY2rd1FgiP8VfG2lSrqzGlujtciohES5rCaqkHFvUQD6rCWjdbzh1SLjirGR9GtBEARBqE9k7YAjW6C0EFa+oYy9P1h59xboa0TmVpjeWSk8d/9/xj2PQiI9XUtqKrbWtWQOcR07bP7vwQ+LjBrsK0JGEARBEOoDs87yfs1XoK87O/9Q3vMOK+6igizj9bSupbx02DxPOQ/wMw7GF6ExxuPBmhgZSb8WBEEQhNME91YEty2F3hON52otKtl7oOCI5xx315K2I3aAm13isnfAFACdLvN/vyndjceNgn1rkZARi4wgCIIgnAzy0/XnyV2V14qZnnO1jRo/ugQSO3rOCYnWu5a0AcHuwcHdroIW/SCigf/7DQqDs2+Ff97xHHejNrmWxCIjCIIgCFXBPdOouijIgN0LPce16dfWQsg96Lqm7YitEp3sf+q1ytAXYORbmgETmEOdZ5J+LQiCIAj1BVup7+tmPyvq+ktwpCtepbRI30xSrfR7ogQEQmRD13lQuC59W2JkBEEQBKG+YCnQn0e79Tga+73xfec/XLXnRTbSWGSKoazEda0k1/ieqhCsCVJ2cyvVxqwlETKCIAjC6UtJXrkoKK1czRXQF74DCItzHXe7FlLPNr6v/6PQ9arKPQsUN5HWtaQNJk45o/LreUObNh5s3N6gNllkJNhXEARBOD0pyIQ3zoHYplCUrbhqHtim1GFxOCquiOsuZKKS4dguxVLSqr/veztdDpu+hIhGrqaQ/qB1LalCJrFL1a08hs/QFMYL0adk18aCeGKREQRBEE5PtnytdJ5OWw85+6E4G7L3wuav4H/JsOtP3/eXFurPE1rDhH/gig+g8xW+7207BG5ZCBNW6ccjGunPm56rvKuNIJ0WGY2QOf+hyhXfqwita8nNIlMb069FyAiCIAinJ2kbPMccNvh6PJQVwx9P+76/wM2S0nogxDWDzqMgoIKvV5MJGveE8Hj9eLuhEJPqOh/1Hpx5E1w/XznXCZnyGBlNVlG1oBVFJv3nqI0WGXEtCYIgCKcnRkXncg+7jiMSfN+/5SvlPTwBhr4IbQaf+J7ModDkTFdqdUxjGD7Ndd3ItaRtSVAdaNdzEzJqsG9tQiwygiAIwumHwwF7//Icz97jOrZX0Kfo2G7l/cL/QdfR1bOvwGDwZe0I0lT2PVkWGS0mY5kQ4GW8Jqg9OxEEQRCEU8Xa2cYNFY/vcx27u47cUYVEbKrveZXBbvMdZKzttaTWsfG3w3ZVcNuLFMQTBEEQhBPlh3vhx/tPbI0f7zMez9nvOvaVTZRzQMl0AsMS/pXikv9zHdss0PIC5djI6qFzLVXNIpNVlMXc7XOx2qwVT/YWI1OLgn0lRkYQBEGoO+RnKNYUgEFPee/Y7Au7DcV9Y9BiQGuRKT6u1Jdxt3hkboM3ernOg4xrrfhNjxvg+7uU47JS6D5GESdGdWicrqVCl+urkjEyDy99mNUZq/nv2H88fe7TFcx2s8jUwmBfscgIgiAItZ+/XoFFU/UVbN27S/tL3mGcImbU+/prmf/pz41K/291q9hbnTEqNouS8dT1Sohr7nldtf6U5Lrq2FTy+aszVgPw9c6vK57crI/uVHUt1SIdIxYZQRAEoZZTWgQLn1WOtV2h3Ts++0vRMeU9KkUpYueL4hylNYCWkGj9+YlaZLRUJM60xepUTkaw74R/YOfvSjdsDaqQCahFdhARMoIgCELtRtucMS/NdVzZlgIqqpAJT4CG7RQhoO1bpKX4uOdYQKD+/ERjZLRUJGSMRFOg/8G+ZfYy3bndYTfOQGrYTnm5URtjZGqPpBIEQRAEI7RCRptJVGWLTHmQbng8RDSAu9bCA9td1XM7XwHJ3ZRjI9eStus0VI+Q6XS58t7nbt/zggysL5WwyGQVZenO80uVz5JryWV79vYK76+NMTJikREEQRBqN1prSbVYZFQhU17wLqZcwNy9HnAowbMfXaKMFed43u8uZAKDqraPcuwOOwGj3odhL3q6sYyIbabPrgr0/6s8uyRbd55ryaXQWsjI70ZSXFbMnIvn0KlBJ6/3O2NkahFikREEQRBqN2Uai0x+uuv4hF1Lbu0BzMGuDKCwWOXdH4vMCXC85DgD5w3k6ZXP+CdiAG74rsrPyy3N1Z3vzd3LGxveoLhM+VkeyD+gu748bTlfbPvCKWBqo2tJLDKCIAhC7UZrkSnUuEZONNg33EcLgrA45d0oRsaSV7XnGrDs8DKOFh/l651f8/g5j2MO8ONrOb5FlZ+X57b3iQsn6s4Lra5GmDuP7+S2P24DoFvDbnRMcAVa16Zg39qzE0EQBEEwwqYJgNUJmSpaZFQrS2is9znRjZX3nAOe16rRIpMQ6hJTu3J2Vdu63sgr9S3CtEJm89HNzuPdOUo7BrXXklhkBEEQBMFftJk8hUddx1W1yKhCJDTa+5yEVsr7MQNxUY1Cxo6rCePhgsO0j29fbWsbkWvJ9XldK2SKND/fR5c9isVmwVZehE+EjCAIgiD4iy4lWRNsWmWLTLlVIiTK+5yE1sq7oZApv7/DCOj3UNX2UI5N05jSUtUCf5WgMhYZNW5GZcqKKc7jpPCk6t3YCSCuJUEQBKF24+0LvvQELTLuhe20xJXHoRQdA0uBa3zXn5C+UTnuPRGSu1ZtD+WUOVx1XSy2ky9k1HTr6GDjz64KmQN5B5ixfobhnOSIZMKrswjgCSJCRhAEQajdePuCzztUNTePP0ImJApM5YXvtAGyn47SzzlBtBYZdwvIycBqVxpFNo5sbHhdFTIv/POC1zVaxFQ92PhkIEJGEARBqN14s8is/RCmNgG73fi6N9Q4EV9CxGRyxdCorihtGnhF9/uJzaFxLWkEW5G1iPsX389Pe37SzT9ecpyH/3qYf0Ir1yhSRa3s2yjcONW7wKpYn7QuJncahDWo0rNPFrVGyLzwwguYTCbuvfde51hJSQkTJkwgISGByMhIRo0axZEjR2puk4IgCMKpp6LYkcoE/Toc/gX7gkuoqPPd42WqWciU2Fxp5p9v+5w/9v/BI0sfYUXaCtYdWcfqjNVMXTWVn/f+zPjkxBN6njcxogb4al1H8aH6ejuRQZFVevbJokpCZunSpYwZM4bevXtz+PBhAD755BOWLVtWpU2sXr2at99+m65d9b7G++67jx9++IF58+axZMkS0tLSuPzyy6v0DEEQBKGO4q0Pkoq/Qb97lsCh1VCeQlyhEAmJUd5VC457TZng6nUtlWg+p7YC761/3MrYX8dy02838dv+36rlee3j23Nzl5s9rh/KP0SZvYxws0vI3H3G3cy/ZL7zPCLIoHFlDVJpIfP1118zZMgQwsLCWL9+PRaLopRzc3N5/vnnK72BgoICrrvuOt59913i4uKc47m5ubz//vu89tprDBgwgJ49ezJ79myWL1/OypUrK/0cQRAEoY5iK/V93YcbxMnxffDxJfD+YOXcFFhx12p311KJW+pyJVoDeMOba8lbwTm1jktVUYOLzQFm7ulxD72TezuvRQRFkFmcyZojawjV9G+y2q06K0xUNQi46qTSQua5557jrbfe4t133yUoyNVfok+fPqxbt67SG5gwYQIXX3wxgwYN0o2vXbsWq9WqG2/fvj1NmzZlxYoVXtezWCzk5eXpXoIgCEIdpiLXkpq9dHyfcSVegCy3hoghUUocjC9UobOgPO3YXchUA9pu1FqLTECAH1/Pjc+Em36v1PNUi0xgeSDzFW2vACAqKIquDRSvSGZRJlab1XlPUVkRkcEuIRMUcGK9paqbSsvJ7du3069fP4/xmJgYcnJyKrXWnDlzWLduHatXr/a4lpGRQXBwMLGxsbrxxMREMjIyvK45depUpkyZ4vW6IAiCUMc4utP3dWsR5B6G17tBUAQ8luY5R/PFDPjOWFLZ9YfyfnyfElujFTKdFQHwzIpnMGHiid5PVLyeAe4xMg6HgyNFR/h+1/cV33zLgio/T22FcGHzC3mVV2kT14a3Nr4FKAHFRWWuuKN2ce10rqbaRqUtMklJSeza5VkgaNmyZbRs2dLvdQ4ePMg999zDZ599Rmio/y3IK2Ly5Mnk5uY6XwcPHqy2tQVBEIRTjN0G/37je05pIexfrhx7czO5W2r8CdTtdbvr2FrkEjLdx8Co9zhafJR5O+Yxd8dcZ32WyqKNkTlafJTLv7+cwV8N5ljJMZ/3hQZW7XvT3SIDiphpEdOCuFAlvEMrZC5ocgHnppxLYECg52K1hEoLmVtuuYV77rmHVatWYTKZSEtL47PPPmPSpEnccccdfq+zdu1aMjMz6dGjB2azGbPZzJIlS5gxYwZms5nExERKS0s9rDxHjhwhKcl7RcGQkBCio6N1L0EQBKGOUpILFdVXsRbpA4I14sCJtkcT+Cdk+j/mOl76qkvIRCSAyaRLUS6tKI7HC1qLzKr0VX73WwoKrJp7R42RMRImcSGKkMmx5FBcHkB9RdsrPNoR1LZg30q7lh555BHsdjsDBw6kqKiIfv36ERISwqRJk7jrrrv8XmfgwIFs3rxZN3bjjTfSvn17Hn74YVJTUwkKCmLBggWMGqUUINq+fTsHDhygd+/eRksKgiAI9Q3VkhIcBYkd4eAqaNQJMv91zbEU6IvmlRZAaIx+nSI3C0dFqdfqnJBopSDe0lehZf/ycWVtrRWmqkJGGyNTGUoqyuTygmqRMZs8v/5Vi0x2SbbTIqNNw36g5wOsyljFxS0vrtKzTxaVFjImk4nHHnuMBx98kF27dlFQUEDHjh2JjKxcXnlUVBSdO3fWjUVERJCQkOAcHz9+PPfffz/x8fFER0dz11130bt3b84555zKblsQBEGoKUoLYdHz0PFSSD27cvcW5yjvYXEw+kNY9zH0HAevtnPNmX8zDNLERpYWegqZfLfYSn9rwGgFirpG+dp5moq/2howlUFrkakMVrsVm91WaZeP+jyj+2JDYgHFtaRWGda6sMZ1Hse4zuOqtN+TSZVzx4KDg+nYsWN17sWDadOmERAQwKhRo7BYLAwZMoQ33njjpD5TEARBqGb+fh1WzFReT1cy80e1yITFQnQKXPCIl3muuiu63kgquW7xkv4KGa3lo6C8IGtoLKBvwFjVPkk2IzeYn1hsFsIDKheEq1qAtDEyKmpadWFZofPzBAcGV3l/p4pKC5n+/fv7bN+9cOHCKm9m8eLFuvPQ0FBmzZrFrFmzqrymIAiCUMNUlHXkC6eQifM9r/Co69io/1LuIf25P1lLHnspF0uqReYEhIzD4cBis+iaRlaWXEsub216i76N+3JW0ll+3ePLIhNmDgMUt5Wafl0vhUz37t1151arlQ0bNrBlyxbGjh1bXfsSBEEQ6gvmqvUFAvwXMumbXMezh8ETma5zmxXy0/XzqyJkVIyETEW1btx4ftXzfLXjK85JMQ6ViAiK8NnvCGDKyin8ffhvZm+Zzeaxm33OVfEVI6MWwSspK6HUrrjU6qWQmTZtmuH4008/TUGBgTlPEARBOD3J3gs7/wAqKDznC29CJqYp5B5wnR/RfJG7W0e+v8vVlkDlRPoklQuZglLXd15lYmTSC9KZs30OAMsOe7b2+WrEV7SLb8fkpZP5cc+PADzX5zke//tx3by/D/9d6a37ssio8TDFZcXO4OXggNovZKqtaeSYMWP44IMPqms5QRAEoTZxaA28Nxi2/ez/PR8MhV8ehI2fu8Ycjso915uQGfsdnDne+31qATybFTZ+4Xndn6wlgMY9De5VhIxWvFTGtZRR5L2oK+AsPqdNc06KSHK2CVBdQFXBV4yMapEpsBbgQPnvVBcsMtUmZFasWFGthe0EQRCEWsR7A+HQP/DXy/7fU2DwhV2ZTtXgXcjEt4Thr3l3EW38Ag6sUlKxjfDXInPlJ55jqpApq5qQKargZxAWpAgVrZAJNYfy+xW/8+cVf5IYXrXO1+BZ2Vf3XAOBVBeETKVdS+7dpx0OB+np6axZs4YnnqhaiWZBEAShFqPNrDm6w797DnvpvVeSB8F+FFT7513lWWqArbcYGXOoUufFne/vgrB4uH2p8X3+CpmYxtDyAtizWP9M3CwylYiRKa6gwJ9qkdE2bkwMTyQqOIqo4ChCAqsec+SMkTEQMtrnqdQF11KlhUxMjD43PyAggHbt2vHMM89w4YUXVtvGBEEQhFqCtgaLyQ9DfkEWvDfI+JolD0iueI2fJ+nPw2KN57lbDEJiwFKe4l2c7apD405lgn2z9+jPyzN3teKlMjEy2j5GRqiCorDUFezbKLyRx3UtVpvVr2q/zsq+Bq4ld9ESaAqs1a0JVCotZGbPnn0y9iEIgiDUVvIOu44teUrBOW9WlWO74f96eF/LKDXaHaPaKt4sMu5uGncXz87fjO+rjJCxaiwoYfHOw2Kba7wylX0rci0FlIvF45bjHmNgLGSKyoqICYzxGHfHV7Cve2mVuuBWgmqMkREEQRDqKe41WPYsgY9HwoGV+nGHA9Z95HstbzErWozEjjcho11vxAx9ATuABc8Y31eZrKVLNYVYr/zYeXiyLDIqV7e7GoABqQN042GBnrEsFbmrVHylX7sTFFC1fk6nGr8sMnFxcT6L4GnJzs6ueJIgCIJQd3DvHD3nGuV9zyJXpd5Nc+HnBz1bA7hT6rs2ClA5IaO1hPQcCz/cXfH6ACGVaKvT9kJ4KsfpUlLRBvsWW/VCIqsoC5PJRIOwBh7LVWSRUenSsAt/XvEnCWEJuvEQg7o8/q7pyyLjTl2xyPglZKZPn36StyEIgiDUWkr8aCsw/5byuTnKe88bYa1BKEJFQqbMAhs+048FR0FEw4r34A+tBihusaDKlfZ3FzGgt8LkW/UNJAfMU6wo665f52HZMLLIRAVF6dZQSYzwzFDS9j/ytaYRvtKv3QnwJx6qFuCXkJGKvYIgCKcxFQkZo9owjXtCt2tg7g36NOyKYmSW/x8snqofa3k++BHIWiFDX4Rzbj/xdcrRWmS0xfGyS7J142pXaRUj68n4LuNZeGAhF7W8qMLnGsbI+GGROZh3EKtdqa9jlLXkjt29iGAt5YTkVklJCXl5ebqXIAiCUM8wSm/W8tVNnmMhkdC0FzR1K8FfkUVmy3zPsYbtPMcqS5sLq1XEgL52jNaaorWOGFlKjMbiQuP47OLPuK7DdRU+tyoWGavdykXfuESSN4vMxO4Tncf1VsgUFhYyceJEGjVqREREBHFxcbqXIAiCUM/wZZEpyIR/DcRHeSdlotxSrSsSMg3beo75irtJaKO8d7xUeR/yvPG82Ka+n1tJdufsJqs4y3meX+oSMlrrTIFBcLN7PA1gGEvjjarEyOS5iVFvMTK3dbvNeazG09R2Ki1kHnroIRYuXMibb75JSEgI7733HlOmTCElJYWPP/644gUEQRCEuoUqZAxcGl5dRWowbZRbjEdFWUtGsSu+hMz182HgkzDideW89wR4aC/c9Lt+rcgk38/1E6vdym1/3MbI70bqxrWCRStqjCwlRrEwHRM6+r0Howyliiwy2gaX4F/Wks0oDb4WUmkh88MPP/DGG28watQozGYzffv25fHHH+f555/ns88+q3gBQRAEofbz7zfw62SlpktJ+ZdgTBPPed4sLMHlQiaikX68IiFjJIx8CZnYptD3AX1WU3i84ta66VfXWHSK7+f6ycq0lSxPW+4x7k3IqB2s/z78N1f9eBWLDy52Xu+T0sc5rzIWmcwiV2fvYc2HAX5YZEr9s8jURSotZLKzs2nZsiUA0dHRznTr8847j7/++qt6dycIgiDUDPPGwco3YPvPLotMw/ae89Qv0PiWkNzdNa5aZBr3QNf9uiLXkipkElq7xipK6faGNtMpyrtF5oHFD3Dzbzf7FROiZv2o9EruBShWFkd50LPW4qIKma92fMV/x/7jroV3OUXP7d1uZ0L3Cbw16C3/Pk85DcNcnyu83OpUkUUm16J3D/rKWjoz8UwA7ux+Z6X2VVNUWsi0bNmSvXv3AtC+fXvmzp0LKJaa2NjYat2cIAiCUAPYNV/oRdkuIdPdIBBVtUQER+gtNmqMTKMOcNdaGPaScm7xYZHJS1Nq0wC0ON81XlUhE66xcngpgFdqK+X3/b+zKmMV+3L3VbikuyXj3JRzMZvMFJcVk16YDri5lsqFnrZKb1phGgAxITHc3u12+jTuQ2W4rettjGw9kg+HfuiXkDmYf5Clh/Q9p3wJmal9pzJ7yGyu73h9pfZVU1RayNx4441s3LgRgEceeYRZs2YRGhrKfffdx4MPPljtGxQEQRBOMcWawqbmUJeQSewIF70C7Ye7rqsWlOBIaKdJHdYKh4RWEJnoubY72s7aSV00a1VRyJiDoctoSD0HUozbJqgWE/AvS8e9y3V8aDxt4pSA401HNwF6N5O6vlGjx6jgSlQX1hAbGsuzfZ6lZ2JPZ4NJowBilYvmX8Sc7XN0Y76K3CZFJHFm0plV2ltN4HevpUmTJnHzzTdz3333OccGDRrEtm3bWLt2La1bt6Zr164nZZOCIAjCKUTbJNKSB2pwaWgMnH2LYpl5vjwbqfCo8h4cAd2vVSrtBkcqIkJLRLl1pOiY9+dqv4y1bixvDSP9YdR7Pi8XWF2iw12kGOEeixIRFEHnBp3Zmr2VHdk76J3cm315+5zXVSFjVFyuqkJGi7+upfqM30Lmu+++Y9q0afTq1Yubb76Zq666ioiICJo1a0azZs1O5h4FQRCEU4m2gF3OAdex2mgxKAwl7sWhpF+DImRMJjjzRuM1VTePKnyM0MaxRDSEMV8rrqgI/wNhK4tWmFQkBj7f+jm/7/9dNxZhjqBJlOJSO1RwiPPmnKe7XlimCBmjDCAjK01lcVpkvPRaqiuZRyeC366lnTt3smjRItq2bcs999xDUlISN910E8uXe0ZvC4IgCHWYIo37R20YGRwFanyIyeRKbS5UhUwFvYvCy/sFleSAzWo8R7XIRKVAg9bQehB0GlnZ3VcKrWtJe6xlVfoqzv38XKb+M5W1R9bqrkUER5ASqWRE7cje4bl+eXBzqd3/7tiVwWmR8ZK1ZCRw4kPjDWbWXSoVI9OvXz8+/PBDMjIyeP3119m5cyfnnXceHTp04JVXXuHIkSMna5+CIAjCqUJbAE8VMqHR+jnBEcp7QZb+3Bvh8Tizl9ybUKqogcBn3+z3Vk8UrXjxJgZu/v1mw9ovoFhkUiIUIbM7d7fH9bk75vLZ1s+wuom3D4Z8UNUt61AtMt6sSTtzdnqMvXvhu9Xy7NpClVoUREREcNNNN7F06VJ27NjB5ZdfztSpU2natHorJwqCIAg1gLYKbO5B5d09cyjYzSJTURPGgEBXrZfCLOM5arZPNcSO+Ivq+oGqxZlEBkfSPKa5zwaLL/zzAkeKXH/ox4bEclbSWZV+lhFOIeNFhN3wyw2682kXTKNtnEH15DrMCfVaKiwsZOnSpSxZsoTjx48768sIgiAIdZgSjZDJV1KKPdKXg8otMIdWK+8VuZYAYlOV9+P7ja+rNWYqsu5UI1oB4M215IvwoHCig6PplNDJ5zytkAkODPYxs/LPB2MR5jBo5hkdHO0xVtepkpBZtmwZN910E8nJydx99920bduWpUuXsnXr1urenyAIgnCqMWoS6W5x0cz5MzyMq9J/YU/OHt/rxpf/sTvnGrCWeF5XXUshfoiiakLnWqqkRaZ9fHsig5S9npN8TgWzXYSZwyr1HH/WMrLIGAkzoz5NdR2/hUx6ejovvPAC7du3p1+/fmzbto3XXnuN9PR0PvjgA/r0qVxBH0EQBKEGsdu9W0ZK/BAyNlfw6n2JDfnPksXr6173/czQWNfxP+94XncW16sZIVNoUHXYyKoB8OagN/ly+JdOl1LvlN5+P7N5dPPKbdIHviwyOZYcj7FmUfUvy9hvIZOamsq0adMYPnw4//77L8uXL+fmm28mMvLU/cIJgiDUWrb9BK93g4P/1Ow+MrfBnsUVz1vyIrzeFTZ/5XnN0CLj1jCyTKm5ok3utVNBQbnkbq7j3QsMnlseI+OlCu/JQCtktNV3AXJKcvh659eG9zWP1sfFdG/Y3a/nNY5szBPnPFH5jXpBm37tXtDvsWWP6c5/vvxnYrVisp7gt5CZO3cuhw8f5pVXXqFDhw4nc0+CIAh1jznXwvF98Omomt3HG73g40sha7vveUteUN5/vF8RJQ6H4u6x271YZNzcIWWKa+iw2VWOrMLGh2dc76oKfMzADVUDMTJaIXO0WF/j5s4FdzJlxRTD+5Ii9L2bggKDeO9C38X3AN4a9BaJEYkVzvOXcI2lrKRM765bl7lOd54alVptz61N+F0Q7/LLLz+Z+xAEQagfGFkzThVaN0jWNmjYznhezkHXsbUIXmkL8S3g6C5of5HxZ3CP6yj/0swPcJW6L6ios3WgGYZPh20/KtlQ1hKXpcdmdbUvUGvOnAK0QuZYsb7q8Oajmw3vmTN8DuYAz6/PXsm9CDOHeS1OB9Ub6AsQGhiKCRMOHBSVFemEzenCCWUtCYIgCKeQklz47ErYOMf4urZqro90YF18it2qFKlLW6+kP2/60otFJtRzDCgOcD3n132/8v3u7318AJQqvSHRgEOxYKnkp4PDDoHBENHI9xrViDZI1t0iY0RyRLLPDCWjNgdDmg9xHgcFBFVyh74xmUyGRfHc3UyP9nq0Wp9bmxAhIwjCaY23YM5aydqPYOdv8M1txtff0pTHL/PRN8ggqFWHP1lLoz8EoMit+aB7XIYHJpPSfgD0DSRVK1F0Ywg4dV9N2joyxy3Hsdq9VB0up6LrU8+bCsCkMyfx0dCP+GTYJ7rg3uq2yIBxUTytpWnFNSu4pv011f7c2oLfriVBEIT6htVuZczPY0gMT2TGgBkntlhAkGLdAFj/KZwx5sQ36IFGdJUW6mNJbGX6Hkm+XFzlVoMyvHwJWAyq2JrdLDKdLoNfHqbYXgVXmlpcz6iCcEyTyq93Arh3v352xbMMbT6UJ/42Dsgts5f5XO+ilhfRp3EfYjQdu7Uuquq2yEB5nEyxYpGx2q0sOOAKpA4KCCLyFGaB1QRikREE4bRlx/Ed/HfsPxYdXOQRKFlptJk23004sbX8ecb7F+qv5R3Wn2tFgjs2K8vCQjmnWRO+izQKrDWwUhnFXhQc0bmWVNzL8XtgJGRKcpT3amwQebT4KG9vfJvMokyvc9zrr3yz6xtu+/M2MouN76lIyAA6EeN+fjIsMmrvpLTCNL7Y+gUPLnmQB5c8CFRsQaoPVFrIFBYW8sQTT3DuuefSunVrWrZsqXsJgiDUFUo1tVD8iY/wiXvKsK3iL7xKY9UEkR7Zor+W41YTpiSP/479x/BvhvPn/j/118os3JnYEEtAAI839BJYGxgM2oBWoxiZ5G5O11JyRLJz2FtfIidhscp7cY5rTA0UDqq+jKVJSyYxc8NMHlj8gNc5BVbluaGBxjFAKqp7aHyX8ZXeR0ywS8gEmgIrfX9FtI9vD8DWY1tZcmhJta9f26m0a+nmm29myZIlXH/99SQnJ2Ny848KgiDUFY6XuOqGHC0+SpOoE3BruLteSgtcX9jVRalb0bOi7PJmjMCR//TX1rzPw/lr2J+3n/sW38fmsS73hqOsFEdF/3aHRCuZRJZyq4mRRWbU+xQvegiKdnF20tn8sf8PisqKKCgt8N1h2cgi40y99j/rxmqzUlRW5GEBUVE7VW/I2oDNrlS8CQzQCwnVtTSt/zTu+PMOr896Y+Ab5Jbm0jGho9/7U2kX78oeOxnfme3ilPV35+52Vho+nai0kPnll1/46aefpJKvIAh1Hm3l06xiL40M/UVj3QGUOBOtkMneC4fWQOdREBBAkbWIbdnb6N6ou8+GgzrcS87PuRbG/qgUwPv1Yf21klzyDSq7Arxdll7xs0KjFeGkChl3oQbQoA3FrQfCpl2EmcOIDI6kqKyoYouMWpRNdScd3QVLX1WOvaQP2x12dh7fSWZRJh0SOtAgrAEPL32YxQcX882l39AsWl+x1t2lct3P11FSVsJXl3xFUVkRP+7+kQtSL3CmSndv2J2kiCQyCjMwIikyidSAqtVhSYpI4ouLvzhpqdGqkPv78N8e16qz+F5tpdJCJi4ujvh4H0pbEAShjqC1yGQVnaCQcc8Scq+p8n89lPRihx26XcWEBRNYc2QNj5z9CNd1uM6/Z7hbZA6sgLWzYdH/nEOOLldi2jwXgAAvCVmzHMeML2gJcWsu6OVLWM2UCQsKIyooikwyK64n426R+fFe1zWDYngf//sxL6952Xl+ZuKZPNX7Kf7Y/wcAiw8uZmynsbp7pizXF7L799i/AOzO2c3n2z5n/s75vLXxLQDMAWYigiK8WjPGdx5/wkG6nRt0PqH7fRFqJDKBp3o/xRVtrzhpz60tVDpG5tlnn+XJJ5+kqKjy7c4FQRBqE1ohc6zEjy93X7gHC7tn/qh1PfYrfzWvObIGgK92GLQI8IYmMLUMWBQeRs7h1c5mi/8FB9G/aANfJ7cGjP+BV10sFRIare955F7ZtxzVohFuDndmx1QoZNSAXjVA+egOzXM8BZNWxIDys9NmArlbUaw2K9/t/s7w0UeLj/LXob8AV0uCuJA4TCaTVyEzsvVIrx+lNuCtCWWH+NOjCn+lLTKvvvoqu3fvJjExkebNmxMUpFep69at83KnIAhC7UKbmWLUPbhSuFtktEJGKx5OJBVWs8ePY6KYFh9Hx9w1fFnuRpncsAHH7CU8HQqjADOe8RizNswyXnvkm/DDPS4XWUi0vlJwWCwOh4P1metpG9fWKVpUIaO6lsCPYN+krsr74fVKS4SCI65rfsbIaN2Ce3P36q6590zScrjgMMkRybrg7rjQOACvaconI9OoOjGyyLSPb0+nBt4L99UnKi1kRo4ceRK2IQiCcPL5J/0fNmRt4OYuNxNgCuBwgStl2VdZeZ84HPDdRM/4Fa1VIl9jMQg8AReF6loyh/FdecPe/0yuWJAjZn0gq5FF5t3N7xqv3fICpQ2BKmRC3QJoQ2P4cc+PPLrsUTrEd2DuCMV9VWx1CZmoICVzq0KLTGIn5VmWXNj2g/6an1lL2nYC7l2es0uy8cazK5+lf2p/3VhciCJk1P27U9uFjJFF5rLWl9XATmqGSgmZsrIyTCYTN910E02anNqiRYIgCCfK+N+V1NmGYQ25rM1lpBWkOa9V2SKTvQc2fOo6b3wmHF6jt8hoa7y4F6qzVkJAqXsMCjXsM13oVtMlQFMPxuFweM+YSWgD0SlgDgHVsBQSrXeXhcbww25FdGzN3uocdsbIVMYiExgEcc2UflBzb9BfC/BMT44IitAVrgM4mO/qF6WtaAuQXexdyAAea6kdoeuqRSYs0FPIeHM31UcqFSNjNpt5+eWXKSurnvoIb775Jl27diU6Opro6Gh69+7NL7/84rxeUlLChAkTSEhIIDIyklGjRnHkyBEfKwqCIFTM6ozVlJSV6NwL7l+GfpGXBsv/Tz+mlt/XphYf2+061tZOAUz5fmQQqahrnn0bNj+yeAM1rqESW4n34mjjf1fezSGusVC3YN/gSIIMrEnOGJmgcP8tMgBRScbjBsX0jAJtdWLKTYRmWxQh463bc65FXywwNiQW8N69OyQwxHC8tmAkWrwFANdHKh3sO2DAAJYsqZ6CO02aNOGFF15g7dq1rFmzhgEDBnDppZfy779KdPl9993HDz/8wLx581iyZAlpaWnShVsQhCqh7al0pOiIhzuiUq6lwqPw1yvwwRAla0hLXHkacIGmMuyxnZoHHdfH01Sm15MqetoNwxadUvF8TePAgtICPprr6W4IDgh21aLRChn3rCWTyVBQGMXIqEXmfOLuulJxT2PHWMi4W2TUysyF1kKeW/kc4F3IZBTpg4PV9GVtUb+Knl+bMBItFRX4q09UOkZm2LBhPPLII2zevJmePXsSEaH3Z15yySV+rzVixAjd+f/+9z/efPNNVq5cSZMmTXj//ff5/PPPGTBgAACzZ8+mQ4cOrFy5knPOOaeyWxcE4TRGK1QsNovHX/GVci290RsKvZS9j0xU3nVCRmORKcnRd6n2F5vVtWZ0Cja32jOl18+HZffqxko05fTzi7P5snA3mPX/7JfaS52ZTM+FOShtEM/kY8eJdLfIoP9CV11V6s8t3BzuzPrJL63AtaQsYDze5kKPoYqERH5pPr2/6M0NHW8guyTb6TryJkzcLTKqJSk50nO+OcDsf52fGsLIYnQ6WWQqLWTuvPNOAF577TWPayaTCZvNz9Q+N2w2G/PmzaOwsJDevXuzdu1arFYrgwYNcs5p3749TZs2ZcWKFV6FjMViwWJx/bWTl1eFhmaCINQ7tF9eVrvVw2rgl2uprBTWfeRdxIDLZaJt4KiN2bDkQ6GmZo3Dz38z8zMAh9KcMrwBdo21ZUxyIv9r2Fo33QYUOVxumpzc/TSzlpFRLmTibTayA5V4lLSCNHIsOXwVZIWgSHoXlzA8rgWwWLemVlAUlxUTHhSus8hEBVfCteQwiPJ5aK/LOqTBW2xPSkQKaYVKnFOZvYwPtnygWJjKiQuNI8wcVqG1TbUkJYR6tmuo7dYYMP75SIyMD+x2u9dXVUTM5s2biYyMJCQkhNtvv51vvvmGjh07kpGRQXBwMLGxsbr5iYmJZGQYV14EmDp1KjExMc5XamrVKjEKglC/yCt1/VFTUFpQNSGz8Qv4eZLvOZGNlPejGneStpCdJZ8ybbqx3eafeymvPDA5KhkCAnBoAnk3hoZ4lNcvDDBRrBFJOfmHSNLEN/554DDnNOwOwG/7f9P9PDLMZkg9Gy6YrGQX9Z7osZ3NRzczZcUUZ/0drUXGL9dSq/6eYwYiBvSNGs9OOpt+TfoREhhi2LG81O5yTUUERfDBkA8q3IoqZFrFtqJv476MaOnyFjgq4/qrRaii8nSgxu1l7dq1Y8OGDaxatYo77riDsWPH8t9//1V8oxcmT55Mbm6u83Xw4MGKbxIEod6jtcgUWAuc7gc10NOvGJk9iz3H3N0OcS3KH3gQ9ixRYmIOr3Fdt+RjyXdlS5kcDs8qwOVsz97O6ozV5c9epLw3UhoEundh1saMAOQHBGDRiJ3cggxKy/9yf/DYcYKADuU9eo6XHNd1//48oRFbc/dAgzbwyH4YolQO1v6Mbv79Zl0xv7AgTdaSgWvpYP5BHln6CHty9ygDPcbBiBmQqljXC00m3t30LgfyDnjcq/2sIYEhzBwwk5XXrtT1MDIiMijSr7YAqmspwBTAG4Pe4Pm+zzuv2fy1mNUyEsMTa3oLp4xKu5aeeeYZn9effPLJSq0XHBxM69aKSbRnz56sXr2a119/nauuuorS0lJycnJ0VpkjR46QlOQl2h0ICQkhJKR2R5gLgnDq0aYE55fmO90fDcIakGPJ8S9GJkD/T+aGkGCCGrTjVdsRWlqtPH7sOCS0gtRecHAVpK2Hhc/q17AWUazpXG03oTR/dOueXVJWwhU/KOXlF4xeQKPyisC0Hw4ocT6+SHeLhckpyqK4XMiEl7t1woKVOJjismKdSMlylHLlj1cqjSY1AcC+xF6YOYzo8vWMLDJX/nAlBdYC0gvS+WjYRxBohp5jwV4GB1fyXmw0762fwXub32PVdat092qzrewOOyaTCbOp4q+viKAIws0VCxlvaddQdy0yvj5TfaPSQuabb77RnVutVvbu3YvZbKZVq1aVFjLu2O12LBYLPXv2JCgoiAULFjBq1CgAtm/fzoEDB+jdu/cJPUMQhNOPUk02jNVudVZ/TYpIYlfOLqx2K8Vlxb5jCzSp0scCArg+JQnIBUJZHRbKw3kWgkCpJXNwlRIbc2i1xzLFh9dAeSiHxWSCnP2ubKdyVqW7vsz35u6lUWF5AbgYpYaX1SBNGaBxZGMOFxzmoLuQKcmmJEARMqF2B9z4K2H52wFFNBmJFPfaM77cb6GBoV5bFDgcDqe4WZe5jv+t/B+Te01Wgmh7joPibDYf/wdytlFUVuTxXK2QqYyFJCIowimufOGtEF5ln1eTzBsxj9E/jK7pbdQIlXYtrV+/XvfasmUL6enpDBw4kPvuu69Sa02ePJm//vqLffv2sXnzZiZPnszixYu57rrriImJYfz48dx///0sWrSItWvXcuONN9K7d2/JWBIEodK4WzDSCxRRkhie6Azo1PZe0pGfAe8Ngn1LnUNqoKyWg5dOVw7ClUqxFCni4/W4GG5OakR++ZdzYZbLfW4xmSBzm0eczP68/c7jQ/mHXAHD4fHY7DbKHJ71vMZ0GEOLGMW1dSBIL2RyS/MoKXeDhZ19GzTr7cxscbfIqLy0+iXduS+LTGBAoDNGpsRWohNa7vfN2T6HXTm7lJOAQOj3IIkJbZ3XMwozyCzKZOb6mWQUZuhcS3a3IOGHznqIc5KNvxPMAWa/XEsRPqoJa2ORajPt49szuq0ImSoTHR3NlClTeOKJyrULz8zM5IYbbqBdu3YMHDiQ1atX89tvvzF48GAApk2bxvDhwxk1ahT9+vUjKSmJ+fPnV8eWBUE4zSh1q0+i1hKJDIp09trxKmS2zNdbViauxX7+gx7TDsSUxyWElQet5ili6b3YGFaFhfJ6fCwABQEua0OxKQB+eRD+r6euJ1OJzRWzcjD/oOJ+Kl9bG9CqpWvDrjQMUwry7Xfrg3fsyCanaym0tfJvrGp9KrYZC5lPt36qO/fmfuuV3AtA13RR68rTBlqruKdAa6042SXZPL7scd7e9DZ3L7xbZ5Fxzyy6vuP1vHvhu4bBrf66hdT//nWdiWdMZHjL4X4FONcnqi3YVw2urQzvv/8++/btw2KxkJmZyZ9//ukUMQChoaHMmjWL7OxsCgsLmT9/vs/4GEEQBCP25u7ly+1f6sZ25yi1XWJCYogPVYSH1x49aev151FJFLcd4jHNaWVQs2+O/Ku7vjlM+cu/UOM2Ud09ZO/W1ZfRCouc4qOgWpTCEzxEGUCrmFac3+R8Z3Xa/4L1ZfX3BQU5n6UKGKeQsRoLGYB9ufuczzMSJAD/N0CpbhwYEOiMSflj3x9O64lROrZ7QLC20WOhtZAV6SsAfQXfrg26Muks46wxIxdS85jmADzZ2zjk4fdRv7Ng9IJa34LAX+JD45nadypnJZ1V01s5pVQ6RmbGDH26m8PhID09nU8++YRhw4ZV28YEQRCqi0u+9SzUqWb5dG3Y1ZkZ5LVrsiY4F4CQSJ3FRGV9ZrngUS0yBfpSEfbyL8wCTU8kq8mEDQiE8uwlxaqjFRYlan+mwGAIjqBU01ohKCCIO7vfyc1dbgaUmB+Aw+WupejAMPJsxRwKMhNtV4SF6lJS30tsxjEyACO+HUH3ht35aNhHHkLm3JRzmdB9gi6uKDI4kqKyIp5b9RxBgUFc3uZyw95L7gHBWhFZaC0kOCDYw/L07oXvenUVndf4PJ1YfffCd2kWrcQdjW47mgubXcgTfz/BooOLnHOMCuCpTDl3Ck8tf4pXz3/V6xyhdlBpITNt2jTdeUBAAA0bNmTs2LFMnjy52jYmCIJwKujasKvTteC12WBemseQNl35obMe4qXVL7E9WwmeJbGTkpbtsKONZCkODGRZWChH3eJrLFHJhOen6xpN6oRMufXiYEQ86/f8wBmNzgCUANu/r/lbZ1EY2mIoz618zhnb0bwwh53BQRQHBDjjelQB47TIeImRUdmQtYH80nyP+JQ7ut1B14ZddWNRQVFkohQM/GnPT4qQMUjHdh/T/uwLywqJDI70sJAZ9XpSuaTVJU4hExUc5RE3ExMSw//O+x/nfnGu1zW0XN7mcoa1GHZaFZarq1RayOzdu/dk7EMQBOGUExwQTJg5zOVashgImbw0pa2AG6qQOSvpLC5ocgEvrX7JZbGIaADN+sC+pc7aLQD7HRbuSGrkuVZIOOH5QGmhEvS7/hOKMzY7rxeXu2YuSgiGZY9xY6cbAeWL3d0tEh0czXmNz2PpYSUwOcJhp5m1jG0hrnmq+0f9ki60FvL1zq+9/pwA8ty7dgNt49p6jGnTftXYFqelSoNWyFhtVp3VpshaZOju8ZVynRDmip3x1mcoKjjKr0q/KiJi6gaVjpG56aabyM/3VNeFhYXcdNNN1bIpQRCEU4GaraIKGY9g39zD8FoH13lyNxj1PuCymIQGhhId4qrH4szWSVIsFaVeyutrKSl3lzgs+Vh3/Qnf30VxxkbndXfrhSpSvHVlbhrd1HkcZnfQ3KpP1Va/6NUv6swiV8uFIc2H0K9JPzrEd9Ddsy9vn8dzjNw8WiGzPnM9uZZc3tv8nnOsR6MegD5uJqtY07IB5fO6W8fMAWavrQpAHwTstcs3lWwOKtQJKi1kPvroI4qLPX8RiouL+fjjj6tlU4IgCKcC9YvYa9bSgRX689v+gi5KkTo1RibUHEpUcBQmlC/Z3NLypIfygF+LP0ImJAIH8PiOT+mx/H5ejo+lSBNHs7noMH+FuawMaiq5tq+QlqZRGiHjcNDC6nJwNQqOcXZ7NrI4TD57MrMGzqJNXBvd+J0L7qzwc4BnTZbz5pznPG4X145zUxTXjtYCoxVSANPXTfeIj6mo55G2SaKvFgmDmir9+3om9vS5nlB38Nu1lJeXh8PhwOFwkJ+fT2io65fGZrPx888/06iRp8lUEIT6gdVuJb0gXffXfl0k0BToLHIWVqZ8WXoVMlnbXMfNztNdUl1LYeYwAkwBRAZHkl+aT54lT8kcilCyh/yxyEwLyOOG0BC+z1asMB/HeGbgTNC4pNRAZW/ZNtrS/WF2O4maHks9Y1o5LRvuHZK7NezmdNH4UxHXCF8F6J7s/aSzPs6mrE3Ownc/7P6hwnW1MUkV4d6+QcvT5z7N2clnM7T5UL/XE2o3fguZ2NhYTCYTJpOJtm09/aImk4kpU6ZU6+YEQag9TFo8iYUHFzJr4Cz6NelX09vxG/cKuAmhCWQWKxaA8JxDgMu1pDZAdKKmT3cYARe/prukfrGqbpqY4BhFyKhxMuGqkDHe13UdrmPZ4WXsz9vPYkcBWxo2qPRn8yZktLErjWw2Yu2uIN3kWJelxd0io63F4i07qF1cO7Yf386A1AGG11Vrj+G98e1oHtMcc4CZXTm7SC9MJyUyhbk75nq9x/k5wqvnD+WYkBiuaX9Ntawl1A78FjKLFi3C4XAwYMAAvv76a+LjXV1Kg4ODadasGSkpKSdlk4Ig1DwLDy4E4OP/Pj41QsZWplR99cOi4Ytim94VnhDmEjIR5Vk4anzFseJjrvL4e5fC9p+Vm867z9XV2m1d1aoRHRINBZpaK2GKlceba+nspLNZk+FqJnnU7FkpuCK8xchEBUcxofVo0tbP5vrcfLY27QEocShx0U283q+1pnizyMwcOJPMokyvDRt9VdINDggmJDCEuJA4soqzyLXk6pobGqVc/zbqN77Y9gUdEzp6XVflqnZX8eX2L53CVDg98FvInH/++YCStdS0aVOfQVeCINRfvMVlVCuFx2DWWdB6EFz+zgktVWzVC5n4MNeXXLjdAcU5JEckE2AKoMRWwtHiozQMb+hq9hgcCclneF3XKWTKRYCzYm2UUs/Fm2upZ2JPQszGQsRfAk3exc/tnW+CP5QaKLGxzSCvXMhoqtgGmAJ0WTw6IeNFkEQHRztr1Rjh6/dD/d6ICIogqziLorIiXT2eR3s9ytMrnnae90/tT0pkCg+c+YDXNbU8eNaDxIfGM7DpQL/mC/WDSgf7NmvWjGXLljFmzBjOPfdcDh8+DMAnn3zCsmXLqn2DgiDULioKuqwKHpVq/52v9Cna9KVHD6LKkFaQxrub39WNhQW63Cnhdgcc3UFQYBDJEUpxtIO/Pgjf3OFqSXD15xDg+U+l2kBRtVyoLhWnRSahFYx6H8uwlzzufWfwO8SExBBh9t7jxx98BbUS6nLxxAS7xEtsSKx+miZVWc2+Au+CpKKUZF+1XtzXKLQWOkWUCZOzgJ3Ks32e9bjXFyGBIdzZ/U6v1iKhflJpIfP1118zZMgQwsLCWLduHRaLEj2fm5vL888/X+0bFASh5tH22Knucu5vb3yb3p/3ZsvRLcYTDIrR+cvYX8d6tCbQNggMd9jh4D8ApEalAnBgz++w8XNw2CEqGVqeb7i2+jNR11OtGbp6K12uwNKglce9vVN6A57BtpXFp6gIiYKzb4XUc4jpMc457C5EtWvEBLvEj7bJpra4XEXWeH8sdurPrKisyPlzDA8K17mZejTq4TPeRhBUKi1knnvuOd566y3effddgjRNyfr06cO6deuqdXOCINQOtBVW1Yyf6mLmhpmU2kt5+acbIW2DMlioqStybGeV184o1LcIuLTVpVzd/mrneaMyG6Qp/26pKcsHzRqPe5j3WItCayHgKWSc6dflGPVFUvFWuE3LU0ePeb32Yt8Xfd980csw/jdCkl3VdxtHNdZN0QoZtU8T6JtWVkZwdW/UvcI5qtuq2FrstGyFmcMUl145daXrtFDzVFrIbN++nX79PAP9YmJiyMnJqY49CYJQy9Bm8xiVm68OAopz4N3+yonWCnNsV7Ws36VBF5477zld9ktzq9W5vmqRORikETKhxhaBZYeXOZsaql/KTteSWwVcn0LGD4HQzGqcSvzEOU+QGp1a4f0qHw39iGkXTPNw32j3oK2Oe2bimYASR1OZVOxWsa347KLPuKLtFV7nqOtpXUvh5nDdXrTWGUHwRaVbFCQlJbFr1y6aN2+uG1+2bBktW7asrn0JglCL0FZZNepkXCUcSnyKihmH4s6x2yA/3TXvaPUIGdXaoE0xTi6zwbE94HCQWi5w9mkszUYZUza7jTv+vMN57uFacmusqHXRgNKlWqVljP7fzIZhDRnbaSyvrHnFOdbJUkq43U5KmY254zbQ43Ols3FlLWM9EnsYjmstMtrquD0Se/DBkA9oGtWUUnspSw8v5ep2Vxst4UHXhl1pFt2MjVkb2Xnc06LmzbUESvzQwgMLJUVa8JtKW2RuueUW7rnnHlatWoXJZCItLY3PPvuMSZMmcccdd1S8gCAIJwWrzcqVP1zJjb/e6MqcqSZ0FhmDTsZVYukrlM0623lqVj0JRdlQnKN5eNVdS1pUS0xoYChnm8LpYCmlfWkplOZD4VE6fHMPJoeDrSHB7FTFjIFoW5W+SneuBuyqFhntz/5Q/iGnsDk35Vzu6XEPbwx6w3n92g7X6taKDo7m2vb6sXCHg0UHDjP3WDFBQaE8dNZD9EzsyYiWI6ryY/DAHOD6e1ZrkQGlj1RiRCKpUaksvWopd/e42+91Y0JimH/JfNrHt/e4poqn4yXHnb9b6ljvlN48ds5jtIyVP4wF/6i0ReaRRx7BbrczcOBAioqK6NevHyEhIUyaNIm77rrrZOxREAQ/OFxwmK3ZWwH4eufX3NS5+nqfaWNkTti1dGy3YnlZ+BxpmngUZyJx3iE47KqvwtHqETKqRcZkMvGevQGkbcNpb8k9SEp+Jr0iGrIyLIzNIcG0sVopKy1w/iP59sa32Zi10dnnSMWbRWbRgUXcvcj1xd8ypiU3d7lZd29wYDCTzpzktMBEBEcQFBjET5f9xKPLHuWm1lfA3rGEOxwQqqx/fcfrub7j9dXyMwF9xVxfwbWBAZWvcwNw1xl3MWHBBC5vc7lzTP2Zfbr1U+dYVSsJC0KlhYzJZOKxxx7jwQcfZNeuXRQUFNCxY0ciIyMpLi4mLEy6hQpCTaANzswqyvIxs/JohUxeaZ6raJwXiqxFvLLmFc5NOZdBzQa5LtjK4P9cLo5PYlxunkJ1vTnX6RfLOQDWEgg6sQwfbcyFya22DLlKhd8GNqVAXn5AAP9LiOOnyFLmF2aQFJHEzA0zDddVXSJq6rIqZJ5c/qTX52vRunbUL/Om0U359KJPFTebc9NVExIVcV7j89iQtQFQ4mGqm35N+rFw9EKdtUfr3lPxVUhPEHxRaSGjEhwcTMeOSqVFi8XCa6+9xksvvURGRkYFdwqCcDLQ/mVd3a4lbQ+iMnsZJbYSn6m/z658lh/3/Mi8HfPYPHaz60J5po9zn5r6LHnqcd5ht9UckL0HEiuu7Kq/zYE5wOzsu6MTVO5CpvyZUeVC5pUEV92VOdvmcHu3270+JjJI6fasWmSOFh/l9XWvk2PJ0c3zVmJf+3PUpoYDSmVjlVDvPYxOhPFdxhMYEKhra1DdaLORwNOFBb57NAmCL/yW3xaLhcmTJ3PmmWdy7rnn8u233wIwe/ZsWrRowbRp07jvvvtO1j4FQagArUXGPQXYX1alr2Jz1maPcTVFVmVz1ma+3PalV8vPL3t/MX6Am4Ao1lh1DEv0x5cHxv73rfdNG/Hfd/BSC0JNyt9qP4z8QW8FUGNfwsu/UH99BIAoTU8ilQBTgGcPpnJu6HgDkcGKkNGu/97m9zzmun+Zq6hCCCpwr4ScnC96c4CZm7vcfEr7Z2nTvFVaxLQ4Zc8X6hd+W2SefPJJ3n77bQYNGsTy5csZPXo0N954IytXruS1115j9OjRBAaeHNOnIAgVYylzZcdUxSJztPgoN/+uxHBsuH6DLibCvcz/+N/HA/DWprf4/YrfdUXWbHabLqNG54Yq1VtkLAEuIZMdGEixyUSYtpJvw3aQvRuWvAhNe0Or/v59mLk3kBMQQIFNERceac6W8jifoHDAJVKiDYQMuLpNq0QFR7H8muW6sYigCEyYvNY/8eZa0gocQ/dK455weC2cNd7w/rpIg1BPIdMq1rNwoCD4g99CZt68eXz88cdccsklbNmyha5du1JWVsbGjRul75Ig1AK0zRGrImTSCly1W6744QqSIpJ4Y+AbmEwmD4uMytHioxzOP0zzmOaG64BizXG6TNwEUYnbvx3p5kBaauumhGsK0i3/P/+FDHBnoksg6ArPORxgKbfIFOotSkYWmeySbG75/RbdmFHAc4ApwGcRN2+uJa11wtAiM+ZrpQt3sz5e165rGFlk3FPRBcFf/HYtHTp0iJ49ewLQuXNnQkJCuO+++0TECEIt4UQtMlnFri/1XTm7WHZ4GcctSmyMWrTMCPdYkKMlR3Xnur1Y9YKoxC24NENbVbd5X2cHaeVBB3xt34PNoa6GjLrmjOs+BrtVOe54qe6eqFhP98bXO7+u1HO94a34nbZ2i1Xdl5awOGh+3gl3Aa9NRBu4yXw1ohQEX/gtZGw2G8HBrh4aZrOZyMhIH3cIgnAq0Qb75lhynEGu/uJezh9cVWnVomXaL10V9wJw7pVsdULHzbWkxsgElCdC56gBv837wvXfQki0y8ZRpBdIlSEkUCNkftDUQhn6gm5eVJAr8LZ1bOtKP0ftRt0ypiWXtrq0gtkK2iaL3ixf9Y0AU4BHe4WTkTElnB747VpyOByMGzeOkBDlH4SSkhJuv/12IiL0Ufbz58+v3h0KguAX2mBfBw6OFR8jMcL/Mu8LDizwGFP7CalfsM2im3kEvrpbf9yFzIq0Ffx37D9GtRkFpUVo7QpqjEyj0HgySo65MpcadYRAM98X7Oa11Ma8nplFN5ufwmz9Zx5DXr8kw+MhuTukbwAg2hwJZUcAGNh0IJlFmR5CzRcv9XuJ3/b9xmPnPEZ8aDzf7f7O73tB35yzvnNRy4vYlr2N2f/O1lU7FoTK4reQGTt2rO58zJgx1b4ZQRCqxu6c3bqy9gCZRZl+CZmD+QfJKMxgdcZqj2vqF6v6HhsS6zGnIiEzfd10QLH4zP3vEx6MCGdEobKeGiPTKLwRGSXHyA+LgfwCaKZ0h37s6HIwB/JYwwR+PJQOf8+APhVUl/3tUd/X3Ql2/THWJiSeUMdhSmwltI1rS0pkCnnZLiFzQeoFLD64mA7xHQyXurD5hVzY/ELn+TuD3+H2P2/n3h73+txCTEgMuZZczk05t3J7r+Pc3u12UqNTuaDJBTW9FaEO47eQmT179snchyAIJ8CDfz3oMZZZnOl1/qIDi9iZs5NbutzCRfMv8jqvsKyQMnsZpXZFnBjFNnh0e7YbN0l8e9PbADzaqAGN047QsbTUJWQiUyB7KzMiAvm+c2++bDsEbdhrgWpR+eMJ6Hw5xDTxumdim0JGDmaHgzL3uBJried8TaZQUHAkfwz/g/WZ6+nbpC+f/PeJbupzfZ5j3o55DG853PvzNfRO6c2Ka1b4rLkD8PWIr9l0dBMDUgf4tW59ITwonNFtR9f0NoQ6jjglBaEesD93v8eYr+q+dy+6m/9b/38sT1vudQ7ouxMD3Nb1No85FVlkjBibksiTDeKdQqahJqNnX+Fh1meu937z/hUGGz0G302AA6sgQYltiSjPQJp34WywWZWMqZVveN4brJFM0Y2JDY2lf9P+mAPMupTgq9tdTUxIDDd3ublSganhQeEVJkUkRiQyuNngKrcBEITTmSpX9hUEoeYpshaRV5pHw/CGHC7QV8TNLPJukVHZcXyHz+tF1iJnnIzZZKZxZGO6NOjC5qOuonkeQsaLRcadXyJdLp24kDgfM8Gh1QHHDLphL/ofrP9UebW7iK+iIsgtr2sVXWaFWb2UejRablmkvAdrkhZSuuum3N3jbrKKs+id3NujwaMgCLUDscgIQh3msu8uY/BXg3UZSoObDQYUIVNmL/OwzNg0/Xu0PZQAkiOSdedF1iKnUIkOicZkMjGy9UjdHA/Xkh8WGXcKrPou0/nWfF0WFtrUZa0gsVmVPknZe1xjlnymNHBlV4XnHPIUMSHR0Li855O2tk1yN920+NB4Zg2cxZiOYySrRhBqKfJ/piDUYdIKleJzR4qUTJt7e9zrLDWfUZjBzb/fzMB5A9mevd15j9ZV5C5kmkc3150XlhWSu+ZdAGIDlTiPK9pewayBs3jm3GcAyLPos3qsNoNaKD5oGtWUIc2H6MYKSgt0ReeOO8pIG/iYcpK2QSlqB/Dl9TCtk85K48hP160VZlR/Zvg017HWwhMltUwEoa4hQkYQ6ihay4pKu/h2NApT4k1WZaxi7ZG1OHBwxQ9XsD17O5d/fzmTl012zlcFkEqz6Ga68/zSfHI2fApAbJ5SZybAFEC/Jv2c1XzdC+L561pSGd12NF0bduWbS76hf6pSubegtACLzVXgz4GD1Q2aQmAIHNvpTJdmR3lPp1xXC4HZZfrPFJy9V//AUe9Dlytc5wOfhAAzjHyzUvsWBKF2IDEyglBHcXfHgNKAUNuEUMv0ddPZeXwnO4/vdI4dztfH1bina7+3+T2GRSjBsNEWfTG7mOAYwHuw7xV5+bS2WtkbFMSX0VF4o3FUYwBax7V29iMqsBZ4uKjyHFZo3gd2L4T0TUr9FzcyAwOZFq+PtzFp3U4A7d0yjtoMhscz9Z2mBUGoM4hFRhDqKEZtCMKDwr12WbY7PPsIHS3WV8s1CrpVg3JjbWVQ5hIXaip2fmm+zjqkWmTCHA6uyyugR4kFXzQMc+1X7SD99qa3OVRwSDcvvzQfIsqzm4qPw08PeKxVEGCQHZS+yXXc7yEIMmgVICJGEOosImQEoY5iJGQigiKIDo7Wl+QvxyiLSVsNGCDuyFavz4uz2WHPYue5apFx4NBZh9QYmZDyOBajRoxatMLL2VwSmLBggm7emxvf5EdTeeXbQ6thzfs+1wX46WAaqLE2Z1wPAx6r8B5BEOoWImQEoY7iHpsCSvdkk8lk2GnZqJeSO1F/z/B6bWRBARx3xZsEBQYRH6p0pz5vznksPrgYcLmWgv0QMtHB0TqLTEKYZy8nLZPzNigH2340vK7tph0fFElTk6s/HOG+1xYEoW4iQkYQ6ijuac8jWo4gLlRxDWnFgYpRTI07qvhwp1uJhZbWMvjve/jrZSjve9QrqZdzzne7lL5CqmspqHypaAMhkxieyHcjv+OTiz4hONAlNqrSqNFJXHMs2sJzAUFw4bOu89jUqq8tCEKtRYSMINRR3F1LT/R+wnmstciEm8Pxh3NK7XS2eGYctbeU8mD2ceVk/zJY+Bysfg+A27q5Kv2qBfnUbCMji0xncyy3dr2VTy/6lJYxLWkZ01L3LPfzStFqoM4ig8kEcc1d5/EnsLYgCLUWETKCUEfR1m8Z1WaUrp+PNu4kJTJFd98tXW5h0ZWLdGMjWo7g3ZxSAoG3MjJpE9eGZ859hp/7vc68tAy6uQucfUsBaBXbivmXKB3v1eBcNUYm2OEAUyBRdpeVp2FwFHedcZfXEv/hQeGM7zze5+f2sO/c9heM/hDaXUSpeyuAmKau47gWPtcVBKFuIkJGEOoge3L38MZGpW/QyNYjeeKcJ3TX1TRmgCZR+gaL4UHhHina7eLbgV0RIH2KS5h/yXwuC2tK6tZfjTew/WeY3gW2fE3jSCV9Or80n/zSfGdLg3CHAxI7OYN+AVqHJxsup+XenvfSMaGj87xjQkeig13NKnWZSdFNlGq8nS6DyEZ6iwxAfAuIbqw0koxtiiAI9Q8RMoJQB3nq76ecx61jW3s0G2wQ1sB53Celj+5aaGCoR1bTqDajnHEvABRkwXsD4B+lYzWtBiqCQMVhh5wD8MvDhAeFO91Xx4qPOYOQY202SOyMCYixKenZFyWe7dfniwpy1Z1JDE9k6dVLnXsuGD7dNTFcky4e2QhLgOuftJf6vQSBQTDhH7hzpaRYC0I9RYSMINRBduW4yuprrRUqoZreRGq1XJUSWwkmk4kLmlwAwNlJZxMZHOm0yAC6SrkANGwHqb3woLyWjBpkfNxy3Bm7E2O3Q6P2YApgbloGcw6n0zqhg1+fT60nAxAcGEyAKcBpRcoP1tSBCYt3HYc3cFpk+lqhV3L5fkMiIdiV1i0IQv2iRoXM1KlTOeuss4iKiqJRo0aMHDmS7du36+aUlJQwYcIEEhISiIyMZNSoURw5csTLioJQ/3FvTeDAM9PovMbn0SupF3d0u8MjFVttIvncec9xe7fbebZPeWaPpvEkJTn6BeNaQLBB0HC5QFDTsB9Y/ICz/1Osza4UsItrTkqZjU6lVogwLtbnTqSmI7VqiVHFTZ5Zm1KtETKBZmeMTGRpkV/PEQSh7lOjQmbJkiVMmDCBlStX8scff2C1WrnwwgspLHSVQr/vvvv44YcfmDdvHkuWLCEtLY3LL7+8BnctCDXL1H+m6lKptUXkVEICQ3hvyHvc2f1OTCYTgSaXW2VQs0EAxITEMKH7BFcwsFbIFOmbSdL0HNCkSTspFzxqld+sYlen7Vi7XREaI2ZASg/oPREatvfrM6rF9gBnerYqlvZYc3gjNoaDZjOE6FsfqBaZkAqK8AmCUH+o0V5Lv/6qDyT88MMPadSoEWvXrqVfv37k5uby/vvv8/nnnzNgwAAAZs+eTYcOHVi5ciXnnHNOTWxbEGqM4rJivtz+pfO8b+O+DGw6sML7frjsBzZnbaZVbCslsNcd9/oxaetdxwMeh+SuSl0Wjw3lALAvd5/HpSi7XREucc3g1kUe132huqoAggMUIdMkqgnrMtfx3LppEBfDvKhIFmn2vSd3D6/HxwIQEqXP1BIEof5Sq2JkcnMV33p8vPKX19q1a7FarQwaNMg5p3379jRt2pQVK1YYrmGxWMjLy9O9BKGu43A4KC4rZnu23vX68vkvYw6o+O+R1KhULmp5kbGIAbAW689XzFTeu14N/R5UjgMNhIwlD2xlnJOs/6Oig6WUgJQzFBFTBVTrC7hcS6lR+oJ2R82BStBxOS+tfsl5HNiqYnEnCEL9oNYIGbvdzr333kufPn3o3LkzABkZGQQHBxMbG6ubm5iYSEaGcbn1qVOnEhMT43ylpko1T6Fuc7T4KEO/HsrZn53NN7u+0V3T1o45IUo8+zYBYNXEmmiFTJAmXmbn79zbXd8X6cP0I367kYzQWmSCyp+rpnlrea5oJ45yq0yZxjV2xCp/wAjC6UKtETITJkxgy5YtzJkz54TWmTx5Mrm5uc7XwYMHK75JEGoxj//9uDOAdv7O+bprAaby/4UPrYXfHgNrifvteuw2yDH4f8Li5Ys/29VbiSaa1OmH9kBYudiYcw2xy2fpbgt3OCC66u4drZBRLTIxITEe8760ZrA/bz/gKsQHcCj/kMdcQRDqJzUaI6MyceJEfvzxR/766y+aNHEV70pKSqK0tJScnBydVebIkSMkJRlXBg0JCSEkxLPzryDUVf4+/HfFk95TYsgwh8LAJ7zP++VhWP0uXPMltBuqjG3/Fbb9YDz//Idcx+2GwWVvQ1JXCAqDyCQoLm9dsGwatHArOBdVcfE7b8SHuFxLoYFKurVRmjlAXqkiwtRmlQBXtruyys8WBKFuUaMWGYfDwcSJE/nmm29YuHAhLVroS4j37NmToKAgFixY4Bzbvn07Bw4coHfv3qd6u4JQI2jjRQzRWCI4ssX33NXvKu+/TVbeHQ744ipY/6nn3IiG0PES17nJBN2uhsTyqrtRxn9MOIn2dAX5S3JkMh0TOtIhvgODmw0G8KhGrDLu13E4HA4sdqXH091n3M0Vba+o8rMFQahb1KhFZsKECXz++ed89913REVFOeNeYmJiCAsLIyYmhvHjx3P//fcTHx9PdHQ0d911F71795aMJeG0ITkimeySbI/xJ3s/qRxkbnUN2qyQvgkSWhvXfVHJU1xVlBbqx4MjobQ8tTuhgk7UFQqZqruWzAFm5lw8B5Om5YC2SJ4Wq91KjiXHaZHpmdjT5XITBKHeU6P/t7/55pvk5uZywQUXkJyc7Hx9+aUrvXTatGkMHz6cUaNG0a9fP5KSkpg/f76PVQWhfmH0Bf7Muc8wuu1o5eS4Jo5l9wJ4uy/MG+t70bLyWJqiY/rxZudqTtz6Frnj1oRxxpEs4m023srIVAZOQMgAOhED3oUMQGZRplPIBBvVuxEEod5SoxYZh3vtCgNCQ0OZNWsWs2bNqnCuINRHSspFR3JEMumF6QAMbKZJL843qHS983clUDeuueIS8oa7kBn8rHIveM9kUjnrZkU4HVwFQP+iYi44cNglf8IbeL21KvjK0DpSdASLTXEtiZARhNOLWhHsKwiCd4rLlBovD5/1MEkRSXRq0Ek/ocC4FAEzuivF7NQ6MO7YbXohc9tfSm8klaKjvjcWkQDjf4dju+H/egBuNpyA6jX4ultotPy27zdn1pJ7Q0xBEOo34kgWhFpOiU2xyMSFxnmKGDC2yKgsfA7Ucv3uqdlHtriETMv+kNxNf93dWuON0FjPsZNkFdF29dby/e7vybfmA65KwIIgnB6IkBGEWo5qkdF2tNaRe8D3AoXl/Y82z9WPv90Pdv2pHIcnuMY7l2f89Lrdvw2GutV3GfqiYt05Cfx42Y/8fPnPXNjsQgDOTDzTY464lgTh9EJcS4JQGdZ9AgVHoN+kE14qozCDMHOYYaE3lQ2ZG8gsUoJnDWNE7HZI2+D7QSU5SvPG7+/yvLZ5nvLeqINr7JL/g27XQIu+vtdVCdT+M2KCc/wUQFUgIiiCiKAIppw7hXNTzmVg04E8v+p5ftn3i3OOCBlBOL0Qi4wg+EtpEXw/ERY+C0f+VcaO7Ya/Z1RcUVeDzW7jlt9vYfBXgzlvznm8svoVCq1KGvSh/EOUZvwLxTlYbBau/+V6532GQmb3QqUqb1C4vm2AluLj+hRtI1LOcB0Hh0ObQWCuQqxJWFzFc6qByOBIRrUdRWxoLKPbjdZdkxgZQTi9ECEjCP6Ssdl1rJbu/2w0/PEE/Pm038tszNrIyvSVzvOP/vuItze9zYbMDQybP4wH5l8CHwzheMlx3X2hsy+Gfcv0i61+T3nvMhpuXQzDp8PkQ3D+wy53UXEOFJSnRIfFwZDnPTfVoK3f+/eJ1kV1ijgr6Sxd1d8goy7dgiDUW0TICIK/pK13HR/bqbxn71be17wP6Rthxhnw62Sfyyw7vMxjbHv2dj5f/hwAiyPCIWsbOZYc3ZywY7tgzrX6G4/uUN47j4KG7eDMGyEkCvo/6greLT4OheVCJrk79J4AsW5dqSsqbucvsTXTpDUl0lWzxld2kyAI9Q8RMoLgD/kZ8OvDrvPMbVh3/YlF/c60lSp9jLL3wMo3wFrsdall+xd4jAUXZRN9cI3zvNBk4njeYef5LTm5hDjQ13axWSFHaZhoWIVXdfMUH3dZZCIbKe/j/wCt5SLwBK0Yw16C6CZw0Ssntk4V6dygc408VxCEmkeEjCBUxLHd8OHF+qEtcxn0192c17QJu4PKg121QbeHVhsulWvJZWveHo/xndnbKdVYEiY1asCuI8p6PRt05e7jGgGjplPnHQZ7mdIo0qhBoypkfpsMG79QjiMaKu9RidBqgOEeq0Sv2+D+fyGhVfWtWQkmdJ9AYngiFzS5oEaeLwhCzSFCRhAq4tdH4Ngu3dCm0BCyAwMpCQjgx8gIZbBMY4XJ3osR+/L2AZBYVqYbPxzg4JsoV1PEZeFhvLTtQwBi3Qtgq24i1ToTGmtcfC4y0XWctU15V4UMwMAnlQDhfg9R12kQ1oBfRv3CjAEzanorgiCcYkTICEJFHCuPg7n4VXhUabaYERjovLw6VF/fJTfARJHq8inHZrcBcDD/IABNrHoh44tYa6l+IEdZw9nwMcS4KzSxTT3HtDVfkjrDIwdgwGN+76U2ExQQJPExgnAaIkJGEHzhcLg6RbcaAMERZAcE8Hp8rHNKptklaopNJs5rlsrAQ/OdvcR2HN9Bnzl9eHvj204hk1pWxqtHsmhbBn0a99E98nxzvO48riRfv6ePhivvqpAJjjDeu5GQCXYTPScaGyMIglDDiJARBF8UH3e5jKIbAzAhqSGFGldOVqCZ8qgVDpqVeJkCbBRYCwCcdWJmbpjpLG6XUlbGhUXFfJ1xlP6auI5LUgcys2E/Ls8vcI41yHNrQVBWorQlsJQLHG9doWMMMoi8WW8EQRDqKCJkBMEXuYeU94iGYA6hzF7GlhB9wbUyE+Q0bAeArfvVzvGsI5vBVsbxvIPOMbU2TJxN7X9URGqQqwZKTHhDsOSRqnE9NchVOl4z8EnXQ9PW+WGRSYWLX9OPeZsrCIJQRxEhI5z2ZJdk897m98gqynKOORwOPtzyIRsOL1cGopU6JQfy9X2NwsoziLKGvwRXfkJBjxuc1479/gjTvriQbYWHnGP7cpSMpVg18who+sUY53F0WAL0vJF2pa64mAY2uxLbct790OVKZTBtPZSWW218iZOzxkPTc13nImQEQahniJARTnue/PtJXl/3OhMWTHCOrUhbwatrX+X6f2dhA3ZHNeCl1S+x5OAS55wnjmbTrNxykhkYCB0voaC81QDA3pzdfGBziSOA3XlKNlNcQCiEK52ck8pszusWmwVSutNh7G/OsTi7DRK7gMkEqWcrg4fXuYRMRe6ihJauY29uKEEQhDqKNI0U6jUZhRk0CGuAOcD7r/qSQ4o42Zrt6keUV5rnPB6cmkKWdSf8t9M11mwwV+59n8XhYWwDsooVwaLGxQD8EWHQG6mc2NjmkK1YavQtF5WsmwZJ3RkQ1JBj+Ydoai1TMowAUnoo72nrXWPuAbzuaIvliUVGEIR6hlhkhHrJ/J3z+WzrZwz+ajAP//Wwz7lRQZ5WCovFVYAuy+wpgppFN4MRM2hkV4THlqNbAL2Q2VQeSzOosIipR/V9k+L6Pggm1/9+k3KLaBOZyrUdXC0IXm88jE/TjyhCJ7FctDRoo7wXHXVV661InGiDfiXYVxCEeoZYZIR6x7oj63hq+VPO89/3/+5zfkRwBPlWfYpzQfpGn/c0jWoKbS6jgSkPNr/DvB3zGN9lvLOLNUBReWZTqrWMi/PzSbKWcmNyIlF2iGs1WEnn3vg5AGNvWcNY987RiR1dx6pLKTRascCUFsDRcgtRRRaZ8mwrAILEIiMIQv1ChIxQ7zhafLRSc3M11peC0gIigyMpytG3EYgPiSVb08SxabRSoyUw0PW/0MQFE+nWsJvHM5pGNsZ0PIczSyz8XBIFV35MUGAQDPmf0vuo2zWudgJa2l0E138LgcFKQ0iVqCSl0rBarbciK0uTs5SA34gECJT/5QVBqF+Ia0mod1hsFo+xgtIC3bnVbiW/NJ8R34ygWNNa4Lvd34Eln4LDawFIsZYxzBTNjIEzmT1ktnNes2ile/RlrS9zju3K2cXXO7/2eHbTVhc6j1PPuYfUuPKYlfB4GDwFGrU3/iAmE7TqD831BfOcfZUs5XE8Rg0jtQSa4aZf4KpPfc8TBEGog8ifZ0Kd5mD+QfJL89mYtRGrzcoNnW4gvzTfY15GYQatg11f+ON/G8/6zPUe89ZkrOG649kUOqxAKJd0voEJZz8MAQE4HA4ubHYh5gAzCaEJACRFJPHTZT9x8TcXe6yl0rT1UFjyupIx1GH4iX9o9waRjToazxMEQTgNECEj1Glu/u1m0grTnOdDWwxl6j9TPeZlFGXQutwSUlxWbChiAP488Cc7MnL4MikWgMioFGdDRpPJxKsXvOpxT9Popnw5/Euu+vEqwzUbNT4brv9GcR9VR9ZQtEbIRCbpG0EKgiCcZohrSaiz2B12nYgBuHi+yzJyWevLiA5WquZmFGY4xw/k6YvauTOqXMQAPtO2tTSPbu48HtxssG4PAaYAaHkBJHvGz1QJrUXm7JsVF5QgCMJpilhkhDpLkbXIY6zEVuI8DjWHMqT5EObtmKcXMprqvIGmQEwmE1e3u5ouDbrw8FJ9qnZJWQn+EB4UzjuD38HusNOncR/yS/MJNAUSHhRe2Y9VMVoh03pQ9a8vCIJQhxAhI9RJci25uqJ1sSGx5GiyikDpa9QmTqm7ogqZr3Z8xZQVUwAY1WYUT5/7tO6e+f+8yiqLUp+lfXx7Lm19qd976p3S23kcdTIr6GrTreNbnbznCIIg1AFEyAh1jn25+7js+8sosyvtAeJD4xncbDBfbv9SN69zg87Eh8YDsDFrIza7jf9b/3/O6+ckn+Ox9jOOeL47voMhZ9xBy/MfPYmf4gRIOUN5D4tX6soIgiCcxoiQEeocm45ucooYgOjgaK5pfw0H8g5gMpm4rsN17M/bz5XtrmR/3n4A9uXt4+U1LzutNo0jG9OvST9lgbQNkL4RetxAytE93JGTB016U2uJSID7/pV2A4IgCIiQEeoghwsO687DzGG0im3FOxe+4zG3ZYyrYeJnWz8DIDggmJ8v/1kJwgV45wLAAWUlkF1eCK+hl9outYWYJjW9A0EQhFqBZC0JdY7D+Xohoy1o5445wMwlrS7RjTWOauwSMQA4lLdfHgKHXTmOSqqOrQqCIAgnGREyQp0h15LLwgML2Zu3F4BhzYcRHxrP1e2v1k/c+QccWus8far3Uzrh0jSqqWuuzWr8MElpFgRBqBOIa0moM7y0+iW+3/298/zaDtfyYr8XMWlFR14afHaFctx+OFz1KcGBwfRK6sWK9BUEmAIYkzoYfn4I8g7DniWeDxr6wkn+JIIgCEJ1IUJGqDP8degv3XlKZIpexICrIzTAth8V60zbC3ntgtf4fvf3dGrQiW6/ToHdC4wf0mU0nHNHNe9cEARBOFmIkBFqNT/v+Zms4ixSIlM86sQ0CGvgeUPuQf353iXQ9kIigyO5tsO1UFpkLGJMAXDZO9Dy/OrbvCAIgnDSESEj1FqsdqtHpV0Vc4DZLWAXcDjg9yeU46BwsBbBipmwbxmMmQ/7/4a517vmRybCTb/B1h+g2bnQ5MyT9EkEQRCEk4UE+wq1loN5Bz3GHjn7EXol92LOxXM8bzi2G4qzleNz73aNp2+Anx+A7ybq50/4B+JbQJ+7RcQIgiDUUcQiI9Q6DuQdoGF4Q3bn7taN397tdq7rcB3XdbjO+Masbcp7QBCc/zCU5MKqN5Wxf7/FmWYdHAUXvQxhsSdj+4IgCMIpRISMUKv4fOvnTP1nquG1/qn9vd9oLYFVbynHnS6DgAAlaFcVMqqIiUmF+7ZU34YFQRCEGkVcS8LJJXuvIjL85J1NntV5VZpHN/d+4/IZsG+pcpzUWXmPawZPHtfPi/OxhiAIglDnECEjVJnjJceZv3M+FpvFeMKa2TCjO/x0v1/rZRRmcKzkmG4swBTArIGz+PbSbwkPCvd+8+avXMfdx2gWCICOI13nnUf5tRdBEAShbiCuJcFvsoqyiA+NJzAgEIBZG2bx5fYvWXJwCa8PeF03d+7SZ1i++SOmmkyEbfgMmp8HBUegz71eq+ZOXDDRY+ybS7/R9UsypChbKW4HcNWnSlNFLZe9DcNelLYDgiAI9ZAatcj89ddfjBgxgpQUpbDZt99+q7vucDh48sknSU5OJiwsjEGDBrFz507jxYSTQnFZMZOXTqbLR10YMG8Ajy571Hnty+1fArDw4EKeW/kcf+7/EwBH5nae3TOPBRHhzI8q79D87R3w59NwaA3HS46zLXubx7O2H9/uPL6uw3V8OPRD3yLm8Dp4sw+81AJKC6BBO2h3kee8oFARMYIgCPWUGhUyhYWFdOvWjVmzZhlef+mll5gxYwZvvfUWq1atIiIigiFDhlBS4n/MhXBiLE9bzo97fnSe/7z3Z/bk7sHhcBAZFOkc/3L7l9y3+D525+zmyOFVzvF0s5vR7+Aq7l10L6N/GM3q1bNg39/OS4EmxdLTKaETj5z9CD0Te3rfmMMBX4+HI5rA3RGvQ7m1SBAEQTg9qFHX0rBhwxg2bJjhNYfDwfTp03n88ce59NJLAfj4449JTEzk22+/5eqrrza8T6heDuUf8hjbdmwb4eZwCqwFHteWHFpCk7z9zvO5UZEMKyiiU2kpAIu3zWNdoFLr5dPVr5GcfZzCoVNJ7XY9NocNgNf7v64UsVszG/o+ANt+glYDoEm5sDm8DnL2Q/Ye/cOb9a6OjywIgiDUIWptjMzevXvJyMhg0KBBzrGYmBh69erFihUrvAoZi8WCxeIKPs3Lyzvpe63PpBeme4ztzt1NdEg0AC1jWnJj5xt54m+lou63G95lr80lcIoDApiekMDMjHRMDrirXMQALIwIZ2FEOPw7U3mVkxCWAH+OgUP/wJbyIN71n8Dd6+HoDnhvEJSLHietB1fXRxYEQRDqELVWyGRkZACQmJioG09MTHReM2Lq1KlMmTLlpO7tdOJwwWGPsW92foPZpPzqtIptxcjWIwkOCObhpQ/rRMxdEW35v8IdrAwN4szmTWlrM+Gs5+IDsylQETFacvbDf9/BodV6EXP9t2Avg9Szq/LxBEEQhDpOvUu/njx5Mrm5uc7XwYOeZe6FinE4FMGhupbOTjqbp3o/BUBWcRZvbHwDgG4NuwHQOq617v4bc/K4JbQZPRr1cI7tCHSJmCCHsaCZc/EcyNxqvKmvboSVb+jHmp0LbQZDaIyfn0wQBEGoT9RaIZOUpGSZHDlyRDd+5MgR5zUjQkJCiI6O1r0E/9ictZnrfrqOPl/0occnPfh6x9dOi8yTvZ9kZOuRuvkNwhpwVburAGgT3YreVmX89uO53J+Tj6n7dcwaOIvUqFTdfSaHg7/2H2JcZFu+GPoJfYpdwdsd3+gHb/oR69LsPLj4NTCHVP0DC4IgCHWeWutaatGiBUlJSSxYsIDu3bsDSrzLqlWruOOOO2p2c/WUa3++Vnf+9IqnAaUoXUpECuYAM9P7T+feRfcCMLL1SELNoQCYcvfz2uGDrA8Noc9tayA6BQICiQR+uuwntmZv5aofFdET4XAQ6XDwQOpQSOzO3Y5Y7MXHuSs7F5O9zLWB+FZKJd5WAyA8Ab693XVt7A9KsTtBEAThtKZGhUxBQQG7du1ynu/du5cNGzYQHx9P06ZNuffee3nuuedo06YNLVq04IknniAlJYWRI0fW3KbrKUXWIq/XksKTCAoMAmBg04E8cc4TZBZlcmvXW5UJpYUw4wwigb7meIh1s8CYTHSI7+A8t6oHHS4BoGOHUbyz2KC/0ojp0KKfcuxwKBWCrUUQ20xEjCAIggDUsJBZs2YN/fu7GgHef79Syn7s2LF8+OGHPPTQQxQWFnLrrbeSk5PDeeedx6+//kpoaGhNbbneklmU6fVaw/CGuvMr212pn/DLw67j1gMN1zBpqvk2i0iBG9+FqPJA7l63Q+5ByEuHyEbQtDd0vxbKxVP5AnDrEljyAgx43L8PJQiCINR7TA6Hl6jLekJeXh4xMTHk5uZKvIwPftv3G5OWTHKeT0saxH0ZSqXeMxqdwcfDPja+0VoCLzQFmwXaXAjDp0NMY8Opb254k9n/zuaTYZ/QLr5ddX8EQRAEoR7h7/d3rY2REU4tqojpUVLCh+mZmPZ+AC2aAmCi3JqSewiiUmDHL7DhczjzRjCHKiImohFcO9drHyWAO7rfwW3dbiPAJG4hQRAEoXoQISNwtPio8/ic4hJVtjAqv4CvoyKZeMZE2PoDfDkGOl6q1HMBOPgPdC13M7U836eIURERIwiCIFQnImROcyw2C0O+GgJA69jW3LF3ofPaE0ezub0YkhqeAe81UQZVEQNQmAkryivydhhxqrYsCIIgCE7kz+PTnIUHFlJqV/ogDWrcT3ctEEgqzIaDq6Cs2PsikUnQ1rhnliAIgiCcTETInOb8k+FqBXDd2q9dFzpdDp0uU47Xfqi/KcAMYXGu8ys/BnPwydukIAiCIHhBhMxpzC97f+GrHUpTxv+ZEolN3wymQKV/0ejZkNxdmbh5rv7G4dNhxOtKobq71kHTXqdy24IgCILgRGJkTmM+3fopAJ2DYhm2YzWYAuC6edCqvLZPUhf9DY17Qter4IwxSmBvx0tP8Y4FQRAEQY8ImdOUVemr2JS1CYDn9v5HEMA5d+oL2jXrA/EtIXuPcj7qPeVcEARBEGoJImROU2asm+E8bmot72/Ub5J+UlAo3PQb7P2rvN9R/CncoSAIgiBUjMTInKYctxwH4JygeMUac+7d+gBelchG0OUKETGCIAhCrUSEzGmI3WEnqygLgMfTDyuDyd1qcEeCIAiCUDXEtXQaYbVb+WDzB2zM2kiJrQSzKYCU/Cyl7YAUtBMEQTjtmb/uEFGhQQzumFjTW/EbETKnEXO3z2XmhpnO8wERzQhin5I+bQ6puY0JgiAINc6h40XcP3cjAA8MbsuE/q0JCKi49UxNI66l04jlact157ekH1AOGnWsgd0IgiAINYmlzMaOI/k4HA4AluzIcl579Y8d/LApraa2VinEInOaYHfY2Zi5AYDRefl0s5TSvqBQudhqoPcbBUEQhHrFZ6v289g3W5znV57ZhKdGdNKNASzfdYxLuzc+1durNCJkThP25e4jtzSPULudyceOK5lKoNSKadyjJrcmCIIgnCJW7D7mIVjmrjnEsp1HPeYu2ZFFmc2OOTAAu91BVoGFxOjQU7VVvxHX0mnC2sy1AHS2lLpEDMCgKUqVXkEQBKHe8+nK/YbjabklAFzUJYnFky7AZIKMvBLe/kspiPr2X3vo9fwCft6czqxFu2j+yE88++N/TrdUTSIWmdMAh8PBb/t+A+CckhIY9T4smw7hcdDkzJrdnCAIglBlsgtL+WTFflo2jGBEtxSfc3/ZnM6v/2YAcEbTWNYfyPGYc+cFrWneIIKrz2rKF/8c4OXftrNwWyZr9yu1x+78bJ1z7vvL9tKndQID2tdshpMImdOA5WnLWZW+CoDBhUVKM8g7ltXspgRBEIQT4r2le3jup60ABJjggnYNiQoNMpzrcDh4/Nst2OwORvVowoujuvDEd//SokE4oLicrjqrKZ0bxwBw2RmN+eIfJSFEFTFGfLJivwgZ4eTz37H/ALigsIiWIQmQ0KqGdyQIgiD4g83u4KPl+1i55xjpuSX0a9uAuwe2YdnOozz/81bnPLtDiXW5qU9zTAbhAgezizlWWEpwYABTL++Cufxd5dZ++u+Fs1vEc0XPJny19pDhvu4Z2IbXF+xk0fYsfvs3gyGdkqrpE1ceETKnAemF6QC0L7VCy4ESEyMIglAHsNsd3PbJGv7cmukc23w4l7lrDpGVbwEUy0nT+HBeX7CTZ3/8j02HcpjQvzULtmZiKbNxz8A25BRZWXdAsaq0T44i2OxfeOzUy7vQNjGSrHwLF7RrxFnN43nsm810aRLDDb2bc7yolI9X7Of5n7cysH0jzIE1E3YrQuY0IH3vQgCSy8rgoldqeDeCIAiCL44WWAg0mbh7znqWGmQTqSIG4OkRnQgLDuT1BTsB+G5DGt9tcNV/CQoM4P8W7qTEagegU0q03/sICgzwsNS8PNrVzuaRYe35cVM6+SVl7DlaSNvEKL/Xrk5EyNRzHA4He4uOQJCZpDIbhPr/SywIgiCcWv787wh3fr6O0jK7c6xDcjS9WsSzYNsRDmYXO8dH9WjC/7d35/FRlffixz+zZ5nJvmMI+74YQZBNECiLiqI/xasUAa1LRW3LtaKlBby27lotl8qtyHJ7KagIapW1kRRZBFkVA4GEAAGyAiGZLLOd5/fHJIMhCZKQhYnf9+uV18x5zjLfeXJmznee85zzhAZ5+8SM7BbDl4fza2zv9Q3p1aZ7JoQ2WqxBZiOLp91Il1grQeaWSyfk8utWbs/p7Zw2GQnUNHoPfa6lwxFCCFGHC2UunlhePYl57OYOrPvVMObd0ZPPZgylTVggQztF8fEvB/Onu3r5lnvj3r78dmxXjJVDCjx6c4daX6N/u/BGjfn6xLAWTWJAWmRaL6VgxwLSvl8KZhji8GAb8uuWjkoIIUQd9p48j9OjVSu7sV2E73l4sJmts26ptTNvRLCZGbd0YurgdgSbDeh0OlbsPEmJww1Av6Rwbu8TT7e41tcqL4lMa5XyAmz9M8ciI8BspYMlQjr5CiHENeyb4+cAmNA3gW5xNixGPbd0i6m2TG1JzA9ZLRcP68+O78ar6w7z+j19GN87vvEDvkZIItMaFRyBrX/GA+wO9I5q3aHnpJaNSQghRJ0+3X+av6ZmAjC6e0yjjHE05aYkfj6w7Y8mP/5O+si0RivuA+CbhO6cMJmwmYIZ2nd6CwclhBA/XQ63h7c2pvPtqaIa804XlfOrlfsB6BEfwu19Ln+H3vpo7UkMSCLT+pTkwTnv2Bhp8d0BGJQwhFBL4/VUF0IIUT9/33GCv3yZwR3/vY2F//a2vHg0hdOtsWz7cd9yf77vegz61p98NCY5tdTaHN3ge3rYFgHnoHtk9xYMSAghWjdNU7y+MZ09x88z944evkucq0aOXrPvlG8oAYBX1h3mlXWHAe/QAlrluIuLHuxP17iWuReLP5NEphXxlF/gH6m/Y0N8LEmB0aw7sRGAbhHdWjgyIYRovVbtPcW7lf1bbvtL/caxq0piEkIDanTsFVdGEpmr4XbAh1Oh/TAYNKP2ZZSCzBTIPwzthoLHCWczIDTRO92I5y8/2fIHXov03iPggCr2lUsic3XcHo3N6QWkHMoj0GwgLiSAuNAAYkMCiK98DDAZWjpMIa5pJRUu1h3MZXyvuDoHNvQ3Sine35pVrbWlLgPaR/D+1P4EmY188V0Oe46fw+nRcHkUR/PtPD++m5xSaiBJZK7GoX/CkXXev7oSmc9/DXuW1j7vltkw/NmL0wXpkPc95ByA+L7Q6+7a1zv5NdjzIHsXnN4D5UWodkP5W95mqOV4GhUYVZ939ZPh8mjklzjIvVCBUooAk4GkyCDfl2z2uTJWfnOSj3afIv8HtwSvTViQibgQb1ITFxJAbGgAMTYLASYDZqOeAKOe68KD6BAdLEmP8BslFS7O2p1YA4xYLUYsRv0VdR7NL67g9Q3pWAOM9LkulOTEcFbvO81fUo6y6KtjrPvVzegAvZ8fuFfsyvYlMWajnjv6JpBXXFFjWAGrxciSaTcSXHlp9B19E7ijb+N16P2pk0SmgTaf3MzBU5sYFGChf4UDys5B0MUbF1FRDB9N87bG1CX1FTBbofg07F8O5ReHSlc6PRnm7nTs1A09GmSkwP7/g7OZkHewxqY+Lz/BmWhvwvLW0Jc47y5n0XeLmHXjrMZ6y9eM4goXe46fJ7e4grziCvJLHOQXV1BQ4sDlUeh0oNfp0Om8PfZ1eM9DV5U53Rq5lctXNetWMRv0DO0cRfuoYN7fmuUrjww2c8f1CZiNevIuVJBbXEFu5WOFS6OozEVRmYvDuSWXjV2ng8TwIDrFWOkcY6VjjJVOMVb6Xhcmv8YagVIKpfz/ANnSNE0x/8sM3v13hm+MHgCDXkew2YAtwESwxUCwxZvgBJuNWAOMJEUE8dDQ9izdfpyP6hg1+UienY6/W+ub7tUmhAl9vJ8tTUFIgJG7ktu02ACEVyqvuILXNnj7uYzqFsNz47vRuZaxhnZlnSPIbPAlMaLx6ZRS6scX81/FxcWEhoZy4cIFQkIa746Gv9/6ez7N/BSbR2PbyVPoHv4XJN4IeL9MP/n4PrJPf81jRRewdPwZ/HyVt6VlyW3QbgjkfAslZ+rcvkMHm4KCGFLmIFx5al2mwhRKjj6eAPdRxrS9eM+BTfdsIi645YZUbyo5F8pZvDWLFbuysVferfJqmQw6YmwBGA06Sh1uCu3OavN7xIfw5MhOjO4eW+uIsUopisvd3sSmuIK8CxXkVCY4hXYHDreG0+2h3KVx4mwpRWWuWuO4uUs0y6bf2OiXStodbv6dXsCurLP0SAghNiSAUocHt6ahKYVH8x60PErh0VRlmffPoNcxtmccCWGBjRpTY1t/MIfnVn9HqcONy+ON+/nx3fjFsNpv0S4u8miKk+fKyMi3czS/hIx8Oxn5dg7nlvhuk2/Q6/BcmvFfhsmgw+W5uHyvNiEcybXXuGPt5fRqE8Kdfdvg9Gi4PQqXRyM2xEJy23C6xdmuiSTn3dRMXl1/mI7RwXzx9DBpaW0CV3r8lhSxgQbGD+TTzE8pMeg5p9cTefBjXyKzIfNz5pQegrBQ3gsLZf3NM/lwz59ZfHAxPfsMJSkkiWlxXem+5R0A/tn1Zg4YdWQFBLH7/CEsOiNxFaVkmU2MLC3jnfzqzZSnVBR3OV6goMLbHya4w5voKQDgBtOzpJ82ENVRuyY+7FVcHg2HW8PjUbg0DY+mcGuq+rTHewC9dNrucLPx+1w+O3AGd+UXalJkEJ2ircSEWIixeU/pRNssmI16369yhULTQAFaVZlSGA16Xz+XyGBztV/vR/NKWPtdLusO5lBU5mL+A8l0jLbW+b50Oh2hQSZCg0w/erWBUoqzpU6O5tnJKLCTmW/nSF4J2zPPsuVIAYu3HWdUtxjCgkyEBZkbXNeFdgf/SstjY1oeWzMKq43bUl8vrztMx2grc27vwaCOkQ3eTlM5VmDn8f/bW63Moyle35DOmaIK+iaG0jnGRo96jPjbGjndGsfPlnI0r3rCcqywtM79I9BkYPZt3Zk8sC1KQanTTanDg93hxu5wU/qDx1KHmwvlLt5NzaTUefGH18pHb+KmDpHYHW5ueSMVp1vj1f/Xm51Z5yhzeOgca+UvKUcpc3oY0TWa788Uk3OhgoOnizl4urjOuPpcF0rfxDA6RgfTIdpKh6hgIoLNNX4IuD0aWzMKOVfqZOL1bdDrdRzNKyHaZmnwZ0wpxR+/OORrsf2PG9tKEtPCpEXmKoz6e3/yNQcLcvO5qbwCs96EuvOvTDy0gGOOc5dd12a28UX/uRzSazy29dnLLnt9hYMTATYirG2ZEzmKnLDbWXOgkO2ZhTgDviGwzYcAKGXAfvhPAASZDcSHeg/WcSGBxIVafH04Iq1mci84OHmujHKXhyCzgSCzgQCTgUCTgWMFpdgdLpIigwk0GTAadJgMekwGvfe5Xo/JoMNo0GM26HFrGoV2J4V2B4UlDu+j3UmB3eErK65onBaUge0jeHx4R0Z0jW41N3r67y+P8sbGI9XKoqxmXr67Dz/rEXtF2zhxtpSN3+exMS2X3SfO88NPdVJkEAadDo9SvtMARoMOg16HXvfDR3zP9Tode06c53TRxZF220UGEWm1EGU1Vz56n0dZLUQGm7EGGL37iF7n21eMeu8+YjToMBp0XCh3sSvL+9kw6vWYjd5lvPvXxfXMBj2BZgNRVkudB4nN6flMX/KNb3rt08OIspl5cvk+dh2v+fnrHh9CgEnPew/2J9hspNTpptzpoczp8T23O9x8e6qILrG2RrmzanOqah08ea6MjAJvslKVNJ84W1Znq4rFqKdjtJXOsVY6VT3GWEmKDMZUzx9DB09f4NP9p/l472kAtjx7i++W+YV2Bx5NERsSUG2dc6VOdHjHEXJ7NF7fmM7p8+W+fcJYuU9lFZayP7uIkjq+S0IDTbSLCkYpxYVyF8XlLoor3LW+77AgEx89NqjWU0F1mZ9ylH3ZRUQEm1lVedosItjMF08PJT702m619FdXevyWROYqTP7fAXyrvF/0Pyst4638QpaG2Hiz8sqhfq427DYUotPX3lG0c1g3LhSHka99XWOeu7QjIdYyylROtfLkmGTmDZpHhzBvs/nUdVPZm+/9RVp67Gluuq43h3KKOV/HKYxrhUHvPYCaKh+NBr33Ua/zHQCrpk0GPb3ahPAfN7alb2JYS4fe6NwejadX7mPtd7k15sXYLIQHmQkwGzBV1s0PkwWTQU9mgb1G35zebUIZ2zOWMT3j6BxjbVDS59EUmw/n84v/3d3g99YYrBYjkT9ImKJsFoJMBv53xwnf6Yr+SeGs+uVgwBv3liMFLN95ku2ZhZQ5az81+2PmTujB9CHtKSpzsjPrHEmRQaTnlpCcGE7byKBGe3/1pZQir9hBWs4F0s4Uk5ZTTEa+ndPny6u1hlzKajHSqbJPVmffo4024YGN3j+rwuXBralq4/40Bk1TZBbY2XvyPIdySsgssHOsoJQzF8qp75Es2Gzgtj7xTBvcvkaLncujcTTPzvdnLvD9mWL2njzPt6cu1NjGtudG0uYaP/XqzySRqdRkiYymMetvPVgbePEywk0nTzMlIZZcoxGjUpQefhG3+TyBbRehN5XgLOpPZ/1DBNpOcJg3QHfxS8d59maUZkZzROOpuI6YwAQWTunDv/P/wbYz29Ch49C5i5f43dPlHubcNIchK4ZQ4iph1YRVdI3oCng/hCfPlVXrr5FX7H2eV1zBWbuT2BALSZHBBFsMlDs1yl3eX6TlLg8mg57rwgMpKHHi8mi4NQ2X23vKx/WDc9Yuj8Lt0dDpdL5f5lFWC1G2i88jrWairRYirRaCzAaMlYlLa2lNaUxFZU4cbo392UU89vc99VrXoNcxsH0EY3vG8bMesY3ar8Xp1jh1voyzpU5vi1vl49lSB4UlTu+j3Ynd4cZdtX9o3kd3Lb+Gu8XZCAk0eZetvLOpW1OX7FsapQ7Pj/ar+FmPWEZ0jWZopyiSIoNrzFdK8ebGI2xMy8Vk0HMkr6Ra/41Ak7c1MtBsINhsJNBsYH92kW9+11gb6XnVk8Qgs4GJyW0Y2D6C0EATFS4Nh9tDhctDhUu7+PiDMofL+9mqmtbr4bbeCXSPr94iUNuXsVJw6nyZL2lJO1PM2VJnLUt6RQSb6RRtpdMlLSxxIQGt9nNX4fJw/GwpxwvLMOorT/cGev8ig82s3neaF/+ZRtvIIH5/Ww9+seybaknf9CHt6BAVzPdnivn+TDHpuSV17nuBJgPxYQF8/tRQgszSO6MpSSJTqckSmXNZ/GPJUF6Oiqh19k2Zd7HJOZC+iWE8f2snZn22iRO5NlDeHd8QeIzAtu+j03tQmpHoc6/SNjyMickJ3Nm3je+Kmx/afno7//Pt/7C/YD+a0jDqjLiVG6POyK7JuzAZWse9GYRXzoVyztqdFJW5qHB5vAd4TfmSBW9HSI3wYDPDu0RfVb+apqKUN5mpSm4AQq7wHiJKKUoc7sqkqWYS1TYiiIeGtK9XX7Cq0w4mg55Ak6HWq5s0TfHMqgOsrjw9UiXIbGhw605jM+h1dIwOpkd8CD0SQugSayMxIoiE0EACzdJf48d8fews7/zrKDuOna1zGZvFSI+EEHomhNIzIYRebULpEmv1XRlZ39Nuov4kkanUZIlM9i6OLxvPxOvi8VyScISTTMrPl1LqcPsOLk63RmaBnQPZRSzamkVGvh29sYThA/cypc84RiaNvOKXXnF4BS/tfMk3fU+Xe5g7aG7jvC8hBBUuD0u3H+dYgZ2eCaH8/KYkAMpdHnZknmXnsbPsPnEeh1sjwORNigJMBgJMegKMBixVzyv7nVU9987TszPrHFuPFqL94Ov30sYS740DvKJtFnomhFRLXKSD6dXzaIq/pBzlnZSj2CxGHripLX2vC6NnQgiJ4UFyGX8La1WJzIIFC3j99dfJzc2lb9++zJ8/nwEDBlzRuk2VyLjTPsf44WS+MHTgSeevMQSeJCB+NQYVxIKRixnarmujvVZt1hxdw+KDi2lja8OfR/yZQKOcpxVCiIYotDswG/VX3Foomkerufz6gw8+YObMmSxcuJCBAwfy9ttvM3bsWNLT04mJablxKXLPZHMdEOAMRbmicLui0DkHsGnmcGIu6ZXfFO7qfBd3db6ryV9HCCFauyirpaVDEFfhmj/J99Zbb/HII48wffp0evTowcKFCwkKCmLx4sUtGldRofdqIp01ihcn9mLa4HZ8MmNIsyQxQgghhPC6pltknE4ne/bs4fnnn/eV6fV6Ro8ezY4dO2pdx+Fw4HBcvNy5uLj2mypdrbKiPABMthimVJ4/F0IIIUTzuqZbZAoLC/F4PMTGVr8pWGxsLLm5Ne+5AfDyyy8TGhrq+0tMTGyS2JTbiVMZCQq/shuWCSGEEKLxXdMtMg3x/PPPM3PmTN90cXFxkyQzA59cgtvtoaence5YK4QQQoj6u6YTmaioKAwGA3l5edXK8/LyiIurfVBEi8WCxdI8HbeMRgNGo1wCKYQQQrSUa/rUktlspl+/fqSkpPjKNE0jJSWFQYMGtWBkQgghhLgWXNMtMgAzZ85k6tSp9O/fnwEDBvD2229TWlrK9OnTWzo0IYQQQrSwaz6Rue+++ygoKGDOnDnk5uZy/fXXs379+hodgIUQQgjx0+MXd/a9Gk05+rUQQgghmsaVHr+v6T4yQgghhBCXI4mMEEIIIfyWJDJCCCGE8FuSyAghhBDCb0kiI4QQQgi/JYmMEEIIIfyWJDJCCCGE8FuSyAghhBDCb0kiI4QQQgi/dc0PUXC1qm5cXFxc3MKRCCGEEOJKVR23f2wAglafyJSUlACQmJjYwpEIIYQQor5KSkoIDQ2tc36rH2tJ0zTOnDmDzWZDp9M12naLi4tJTEwkOztbxnBqQlLPzUfqunlIPTcPqefm0ZT1rJSipKSEhIQE9Pq6e8K0+hYZvV7Pdddd12TbDwkJkQ9JM5B6bj5S181D6rl5SD03j6aq58u1xFSRzr5CCCGE8FuSyAghhBDCb0ki00AWi4W5c+disVhaOpRWTeq5+UhdNw+p5+Yh9dw8roV6bvWdfYUQQgjRekmLjBBCCCH8liQyQgghhPBbksgIIYQQwm9JIiOEEEIIvyWJzGUsWLCAdu3aERAQwMCBA9m1a9dll//oo4/o1q0bAQEB9O7dm7Vr1zZTpP6tPvX83nvvMWzYMMLDwwkPD2f06NE/+n8RXvXdn6usXLkSnU7HxIkTmzbAVqS+dV1UVMSMGTOIj4/HYrHQpUsX+f64AvWt57fffpuuXbsSGBhIYmIiv/nNb6ioqGimaP3Tli1bmDBhAgkJCeh0Oj755JMfXSc1NZUbbrgBi8VCp06dWLp0adMGqUStVq5cqcxms1q8eLH6/vvv1SOPPKLCwsJUXl5erctv27ZNGQwG9dprr6m0tDT1+9//XplMJvXdd981c+T+pb71/MADD6gFCxaoffv2qUOHDqlp06ap0NBQderUqWaO3L/Ut56rZGVlqTZt2qhhw4apO++8s3mC9XP1rWuHw6H69++vbr31VrV161aVlZWlUlNT1f79+5s5cv9S33pevny5slgsavny5SorK0tt2LBBxcfHq9/85jfNHLl/Wbt2rZo9e7ZavXq1AtSaNWsuu/yxY8dUUFCQmjlzpkpLS1Pz589XBoNBrV+/vslilESmDgMGDFAzZszwTXs8HpWQkKBefvnlWpefNGmSuu2226qVDRw4UD322GNNGqe/q289X8rtdiubzaaWLVvWVCG2Cg2pZ7fbrQYPHqwWLVqkpk6dKonMFapvXb/77ruqQ4cOyul0NleIrUJ963nGjBlq5MiR1cpmzpyphgwZ0qRxtiZXksg8++yzqmfPntXK7rvvPjV27Ngmi0tOLdXC6XSyZ88eRo8e7SvT6/WMHj2aHTt21LrOjh07qi0PMHbs2DqXFw2r50uVlZXhcrmIiIhoqjD9XkPr+b/+67+IiYnh4Ycfbo4wW4WG1PVnn33GoEGDmDFjBrGxsfTq1YuXXnoJj8fTXGH7nYbU8+DBg9mzZ4/v9NOxY8dYu3Ytt956a7PE/FPREsfCVj9oZEMUFhbi8XiIjY2tVh4bG8vhw4drXSc3N7fW5XNzc5ssTn/XkHq+1KxZs0hISKjxwREXNaSet27dyvvvv8/+/fubIcLWoyF1fezYMb788ksmT57M2rVrycjI4IknnsDlcjF37tzmCNvvNKSeH3jgAQoLCxk6dChKKdxuN48//ji/+93vmiPkn4y6joXFxcWUl5cTGBjY6K8pLTLCb73yyiusXLmSNWvWEBAQ0NLhtBolJSVMmTKF9957j6ioqJYOp9XTNI2YmBj+9re/0a9fP+677z5mz57NwoULWzq0ViU1NZWXXnqJv/71r+zdu5fVq1fzxRdf8OKLL7Z0aOIqSYtMLaKiojAYDOTl5VUrz8vLIy4urtZ14uLi6rW8aFg9V3njjTd45ZVX+Ne//kWfPn2aMky/V996zszM5Pjx40yYMMFXpmkaAEajkfT0dDp27Ni0QfuphuzT8fHxmEwmDAaDr6x79+7k5ubidDoxm81NGrM/akg9/+EPf2DKlCn84he/AKB3796Ulpby6KOPMnv2bPR6+V3fGOo6FoaEhDRJawxIi0ytzGYz/fr1IyUlxVemaRopKSkMGjSo1nUGDRpUbXmATZs21bm8aFg9A7z22mu8+OKLrF+/nv79+zdHqH6tvvXcrVs3vvvuO/bv3+/7u+OOO7jlllvYv38/iYmJzRm+X2nIPj1kyBAyMjJ8ySLAkSNHiI+PlySmDg2p57KyshrJSlXyqGTIwUbTIsfCJutG7OdWrlypLBaLWrp0qUpLS1OPPvqoCgsLU7m5uUoppaZMmaKee+453/Lbtm1TRqNRvfHGG+rQoUNq7ty5cvn1FahvPb/yyivKbDarVatWqZycHN9fSUlJS70Fv1Dfer6UXLV05epb1ydPnlQ2m009+eSTKj09XX3++ecqJiZG/fGPf2ypt+AX6lvPc+fOVTabTa1YsUIdO3ZMbdy4UXXs2FFNmjSppd6CXygpKVH79u1T+/btU4B666231L59+9SJEyeUUko999xzasqUKb7lqy6//u1vf6sOHTqkFixYIJdft6T58+ertm3bKrPZrAYMGKC+/vpr37zhw4erqVOnVlv+ww8/VF26dFFms1n17NlTffHFF80csX+qTz0nJSUpoMbf3Llzmz9wP1Pf/fmHJJGpn/rW9fbt29XAgQOVxWJRHTp0UH/605+U2+1u5qj9T33q2eVyqXnz5qmOHTuqgIAAlZiYqJ544gl1/vz55g/cj2zevLnW79yqup06daoaPnx4jXWuv/56ZTabVYcOHdSSJUuaNEadUtKmJoQQQgj/JH1khBBCCOG3JJERQgghhN+SREYIIYQQfksSGSGEEEL4LUlkhBBCCOG3JJERQgghhN+SREYIIYQQfksSGSFEk5s2bRoTJ05s6TCEEI1oy5YtTJgwgYSEBHQ6HZ988km91p83bx46na7GX3BwcL22I4NGCiGuik6nu+z8uXPn8s4777T4eDbTpk2jqKio3l+2QojalZaW0rdvXx566CHuvvvueq//zDPP8Pjjj1crGzVqFDfeeGO9tiOJjBDiquTk5Pief/DBB8yZM4f09HRfmdVqxWq1tkRoQogmNH78eMaPH1/nfIfDwezZs1mxYgVFRUX06tWLV199lREjRgA1vxsOHDhAWloaCxcurFcccmpJCHFV4uLifH+hoaHodLpqZVartcappREjRvDUU0/x61//mvDwcGJjY3nvvfcoLS1l+vTp2Gw2OnXqxLp166q91sGDBxk/fjxWq5XY2FimTJlCYWGhb/6qVavo3bs3gYGBREZGMnr0aEpLS5k3bx7Lli3j008/9TVfp6amApCdnc2kSZMICwsjIiKCO++8k+PHj/u2WRX7Cy+8QHR0NCEhITz++OM4nc6mrFYh/N6TTz7Jjh07WLlyJd9++y333nsv48aN4+jRo7Uuv2jRIrp06cKwYcPq9TqSyAghWsSyZcuIiopi165dPPXUU/zyl7/k3nvvZfDgwezdu5cxY8YwZcoUysrKACgqKmLkyJEkJyeze/du1q9fT15eHpMmTQK8LUP3338/Dz30EIcOHSI1NZW7774bpRTPPPMMkyZNYty4ceTk5JCTk8PgwYNxuVyMHTsWm83GV199xbZt27BarYwbN65aopKSkuLb5ooVK1i9ejUvvPBCi9SbEP7g5MmTLFmyhI8++ohhw4bRsWNHnnnmGYYOHcqSJUtqLF9RUcHy5ct5+OGH6/9iTTokpRDiJ2XJkiUqNDS0Rvmlo2cPHz5cDR061DftdrtVcHCwmjJliq8sJydHAWrHjh1KKaVefPFFNWbMmGrbzc7OVoBKT09Xe/bsUYA6fvx4rbHVNoL33//+d9W1a1elaZqvzOFwqMDAQLVhwwbfehEREaq0tNS3zLvvvqusVqvyeDyXrxAhfiIAtWbNGt/0559/rgAVHBxc7c9oNKpJkybVWP8f//iHMhqNKjc3t96vLX1khBAtok+fPr7nBoOByMhIevfu7SuLjY0FID8/H/CeP9+8eXOt/W0yMzMZM2YMo0aNonfv3owdO5YxY8Zwzz33EB4eXmcMBw4cICMjA5vNVq28oqKCzMxM33Tfvn0JCgryTQ8aNAi73U52djZJSUn1fOdCtH52ux2DwcCePXswGAzV5tX2GV60aBG3336773NfH5LICCFahMlkqjat0+mqlVVdDaVpGuD9YpwwYQKvvvpqjW3Fx8djMBjYtGkT27dvZ+PGjcyfP5/Zs2ezc+dO2rdvX2sMdrudfv36sXz58hrzoqOjG/zehPipS05OxuPxkJ+f/6N9XrKysti8eTOfffZZg15LEhkhhF+44YYb+Pjjj2nXrh1GY+1fXTqdjiFDhjBkyBDmzJlDUlISa9asYebMmZjNZjweT41tfvDBB8TExBASElLnax84cIDy8nICAwMB+Prrr7FarSQmJjbeGxTCz9jtdjIyMnzTWVlZ7N+/n4iICLp06cLkyZN58MEHefPNN0lOTqagoICUlBT69OnDbbfd5ltv8eLFxMfHX/YKqMuRzr5CCL8wY8YMzp07x/33388333xDZmYmGzZsYPr06Xg8Hnbu3MlLL73E7t27OXnyJKtXr6agoIDu3bsD0K5dO7799lvS09MpLCzE5XIxefJkoqKiuPPOO/nqq6/IysoiNTWVp59+mlOnTvle2+l08vDDD5OWlsbatWuZO3cuTz75JHq9fIWKn67du3eTnJxMcnIyADNnziQ5OZk5c+YAsGTJEh588EH+8z//k65duzJx4kS++eYb2rZt69uGpmksXbqUadOm1TgFdaWkRUYI4RcSEhLYtm0bs2bNYsyYMTgcDpKSkhg3bhx6vZ6QkBC2bNnC22+/TXFxMUlJSbz55pu+X3mPPPIIqamp9O/fH7vdzubNmxkxYgRbtmxh1qxZ3H333ZSUlNCmTRtGjRpVrYVm1KhRdO7cmZtvvhmHw8H999/PvHnzWqgmhLg2jBgx4rI3ujSZTLzwwguXvcJPr9eTnZ19VXHo1OWiEEKInzi5I7AQ1zZpFxVCCCGE35JERgghhBB+S04tCSGEEMJvSYuMEEIIIfyWJDJCCCGE8FuSyAghhBDCb0kiI4QQQgi/JYmMEEIIIfyWJDJCCCGE8FuSyAghhBDCb0kiI4QQQgi/JYmMEEIIIfzW/wfvcASM2e+0dQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data\n",
    "plt.plot(df_simple_PPO[\"global_step\"], df_simple_PPO[\"return_value_smoothed\"], label='Simple PPO')\n",
    "plt.plot(df_rnd[\"global_step\"], df_rnd[\"return_value_smoothed\"], label='RND')\n",
    "plt.plot(df_icm[\"global_step\"], df_icm[\"return_value_smoothed\"], label='ICM')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Return Value')\n",
    "plt.title('Training curve of 1 run')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this scenario, it's evident that the returns obtained from using simple PPO show only marginal improvement over the course of 10,000,000 training timesteps on the Breakout-v5 environment.\n",
    "- Conversely, RND and ICM exhibit significant improvements, with PPO emerging as the most effective method. This observation contrasts with the learning curve observed on the simpler CartPole-v1 environment. We attribute this difference to the increased difficulty of Breakout-v5, which necessitates thorough exploration of observations.\n",
    "- While the substantial gap between PPO and the other two methods isn't always consistent (here we present the results of only one run), PPO often encounters failures. This underscores the necessity for exploration enhancement methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
