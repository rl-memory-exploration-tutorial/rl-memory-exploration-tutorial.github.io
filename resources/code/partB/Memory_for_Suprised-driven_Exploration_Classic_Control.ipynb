{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<center>Notebook: Memory for Surprise-driven Exploration (Classic Control Ver.)</center>**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:**\n",
    "1. Burda, Y., Edwards, H., Storkey, A., & Klimov, O. (2018). Exploration by random network distillation. arXiv preprint arXiv:1810.12894.\n",
    "2. Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017, July). Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning (pp. 2778-2787). PMLR.\n",
    "3. Huang, S., Dossa, R. F. J., Ye, C., Braga, J., Chakraborty, D., Mehta, K., & Ara√É≈°jo, J. G. (2022). Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. Journal of Machine Learning Research, 23(274), 1-18.\n",
    "4. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). Openai gym. arXiv preprint arXiv:1606.01540.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we present the implementation of method in the paper **\"Exploration by random network distillation\"**, which we refer to as RND intrinsic reward, and in the paper **\"Curiosity-driven exploration by self-supervised prediction\"**, which we refer to as ICM intrinsic reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Setting up the libraries** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these commands from the terminal to install related libraries and set up the working environment\n",
    "# pip install gym # Install the gym library with RL environments\n",
    "# pip install envpool\n",
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "# import gym\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import envpool\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gym.wrappers.normalize import RunningMeanStd\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. CartPole Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this notebook we will use the simple CartPole-v1 environment (https://www.gymlibrary.dev/environments/classic_control/cart_pole/) for demonstration. üéÆ \n",
    "- The goal in this environment is to balance the pole on top of a cart by applying left or right action.\n",
    "- The observation space is (4,), including cart position, cart velocity, pole angle and pole angular velocity. And, action space is Discrete(2) including action to move left or right. üîÑ You will get a +1 reward for each step the Pole is standing.\n",
    "- This notebook can also be used for other classic control environments as well. üòä All you need to do is change the env_id and the state and action space. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.gymlibrary.dev/_images/cart_pole.gif\" width=\"300\" height=\"200\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Simple PPO** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you only want to run the surprise motivation, please run until section 3.2.2 to initialize the parameters and PPO.**\n",
    "- In this section, we will test a simple PPO algorithm on the CartPole-v1 environment. üïπÔ∏è\n",
    "- The implementation of PPO in this notebook is inspired by the implementation of PPO from CleanRL package. üìä\n",
    "- The result of this algorithm will be presented at the end of the notebook. üìù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'env_id': 'CartPole-v1',                              # env_id can be changed here\n",
    "          'exp_name': \"RND\",\n",
    "          'torch_deterministic': True,\n",
    "          'cuda': True,\n",
    "          'seed': 1,\n",
    "          'num_envs': 8,                                        # number of multi-environments\n",
    "          'num_steps': 128,                                     # number of steps running in each environments per rollout\n",
    "          'num_minibatches': 4,                                 # number of minibatches\n",
    "          'total_timesteps': 100000,                            # total training timesteps\n",
    "          'learning_rate': 2.5e-4,                              # learning_rate\n",
    "          'anneal_lr': True,                                    # reducing learning rate during learning\n",
    "          'num_iterations_obs_norm_init': 50,\n",
    "          'gamma': 0.99,\n",
    "          'int_gamma': 0.99,\n",
    "          'gae_lambda': 0.95,\n",
    "          'int_coef': 1.0,\n",
    "          'ext_coef': 2.0,\n",
    "          'update_epochs': 4,\n",
    "          'update_proportion': 0.25,\n",
    "          'clip_coef': 0.2,\n",
    "          'norm_adv': True,\n",
    "          'clip_vloss': True,\n",
    "          'ent_coef': 0.00,\n",
    "          'vf_coef': 0.5,\n",
    "          'max_grad_norm': 0.5,\n",
    "          'target_kl': None}\n",
    "\n",
    "\n",
    "state_space = 4                                                    # state space of env\n",
    "action_space = 2                                                   # action space of env\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and params[\"cuda\"] else \"cpu\")\n",
    "\n",
    "# Set seed.\n",
    "random.seed(params[\"seed\"])                                                 \n",
    "np.random.seed(params[\"seed\"])\n",
    "torch.manual_seed(params[\"seed\"])\n",
    "torch.backends.cudnn.deterministic = params[\"torch_deterministic\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.1 Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordEpisodeStatistics(gym.Wrapper):\n",
    "    def __init__(self, env, deque_size=100):\n",
    "        super().__init__(env)\n",
    "        self.num_envs = getattr(env, \"num_envs\", 1)\n",
    "        self.episode_returns = None\n",
    "        self.episode_lengths = None\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observations = super().reset(**kwargs)\n",
    "        self.episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n",
    "        self.episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        self.lives = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        self.returned_episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n",
    "        self.returned_episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        return observations\n",
    "\n",
    "    def step(self, action):\n",
    "        observations, rewards, dones, _, infos = super().step(action)\n",
    "        self.episode_returns += infos[\"reward\"]\n",
    "        self.episode_lengths += 1\n",
    "        self.returned_episode_returns[:] = self.episode_returns\n",
    "        self.returned_episode_lengths[:] = self.episode_lengths\n",
    "        self.episode_returns *= 1 - infos[\"terminated\"]\n",
    "        self.episode_lengths *= 1 - infos[\"terminated\"]\n",
    "        infos[\"r\"] = self.returned_episode_returns\n",
    "        infos[\"l\"] = self.returned_episode_lengths\n",
    "        return (observations, rewards, dones, infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)                # Initialize layer weights according to orthogonal method.\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)             # Set the bias of the layer.\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env_id, agent, use_int_rews):\n",
    "    env = gym.make(env_id)\n",
    "    sum_result = 0\n",
    "    for _ in range(50):\n",
    "        ob, _ = env.reset()\n",
    "        while True:\n",
    "            if use_int_rews:\n",
    "                action, _, _, _, _ = agent.get_action_and_value(torch.Tensor(ob).to(device))\n",
    "            else:\n",
    "                action, _, _, _ = agent.get_action_and_value(torch.Tensor(ob).to(device))\n",
    "            next_ob, reward, terminated, truncated, info = env.step(action.cpu().numpy())\n",
    "            sum_result += reward\n",
    "            done = np.logical_or(terminated, truncated)\n",
    "            if done:\n",
    "                break\n",
    "            ob = next_ob\n",
    "    return sum_result/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.2 PPO Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, envs, use_int_rews=False):\n",
    "        super().__init__()\n",
    "        self.use_int_rews = use_int_rews\n",
    "        # print(envs.single_observation_space.shape)\n",
    "\n",
    "        self.critic_ext = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_space, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.critic_int = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_space, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_space, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, action_space), std=0.01),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        if self.use_int_rews:                                                                                       # If intrinsic reward is used\n",
    "            return (action, probs.log_prob(action), probs.entropy(), self.critic_ext(x), self.critic_int(x),)\n",
    "        else:                                                                                                       # If intrinsic reward is not used\n",
    "            return (action, probs.log_prob(action), probs.entropy(), self.critic_ext(x),)\n",
    "\n",
    "    def get_value(self, x):\n",
    "        if self.use_int_rews:                                                                                       # If intrinsic reward is used\n",
    "            return self.critic_ext(x), self.critic_int(x)\n",
    "        else:                                                                                                       # If intrinsic reward is not used\n",
    "            return self.critic_ext(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.3 Main Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = params[\"env_id\"]\n",
    "exp_name = params[\"exp_name\"]\n",
    "seed = params[\"seed\"]\n",
    "run_name = f\"{env_id}__{exp_name}__{seed}__{int(time.time())}\"\n",
    "\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id) for i in range(params[\"num_envs\"])],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up agent and model\n",
    "Agent = PPOAgent(envs, use_int_rews=False).to(device)\n",
    "optimizer = optim.Adam(\n",
    "    Agent.parameters(),\n",
    "    lr=params[\"learning_rate\"],\n",
    "    eps=1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((params[\"num_steps\"], params[\"num_envs\"]) + envs.single_observation_space.shape).to(device)  \n",
    "actions = torch.zeros((params[\"num_steps\"], params[\"num_envs\"]) + envs.single_action_space.shape).to(device)   \n",
    "logprobs = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "rewards = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "dones = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "values = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "avg_returns = deque(maxlen=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(params[\"num_envs\"] * params[\"num_steps\"])                \n",
    "minibatch_size = int(batch_size // params[\"num_minibatches\"])\n",
    "num_iterations = params[\"total_timesteps\"] // batch_size     \n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "\n",
    "next_obs = torch.Tensor(envs.reset()[0]).to(device)\n",
    "next_done = torch.zeros(params[\"num_envs\"]).to(device)\n",
    "results_simple_PPO = {\"global_step\":[],\n",
    "                      \"return_value\":[]}\n",
    "\n",
    "tracking_global_step = 0\n",
    "\n",
    "for iteration in range(1, num_iterations+1):\n",
    "    if params[\"anneal_lr\"]:\n",
    "        updated_lr = (1.0 - (iteration - 1.0) / num_iterations) * params[\"learning_rate\"]\n",
    "        optimizer.param_groups[0][\"lr\"] = updated_lr\n",
    "    \n",
    "    for step in range(0, params[\"num_steps\"]):\n",
    "        global_step += 1 * params[\"num_envs\"]\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done \n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = Agent.get_action_and_value(next_obs)\n",
    "        values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "        \n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "        done = np.logical_or(terminated, truncated)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "        if \"final_info\" in info:\n",
    "            for info in info[\"final_info\"]:\n",
    "                if info and \"episode\" in info:\n",
    "                    print(\n",
    "                    f\"global_step={global_step}, episodic_return={info['episode']['r'][0]}\"\n",
    "                        )\n",
    "                    \n",
    "        if global_step - tracking_global_step > 2000:\n",
    "            return_eval = evaluate(params[\"env_id\"], Agent, use_int_rews=False)\n",
    "            results_simple_PPO[\"global_step\"].append(global_step)\n",
    "            results_simple_PPO[\"return_value\"].append(return_eval)\n",
    "            tracking_global_step = global_step\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value = Agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(params[\"num_steps\"])):\n",
    "            if t == params[\"num_steps\"] - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            delta = rewards[t] + params[\"gamma\"] * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + params[\"gamma\"] * params[\"gae_lambda\"] * nextnonterminal * lastgaelam\n",
    "        returns = advantages + values\n",
    "\n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(params[\"update_epochs\"]):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = Agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > params[\"clip_coef\"]).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if params[\"norm_adv\"]:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - params[\"clip_coef\"], 1 + params[\"clip_coef\"])\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if params[\"clip_vloss\"]:\n",
    "                value_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                    newvalue - b_values[mb_inds],\n",
    "                    -params[\"clip_coef\"],\n",
    "                    params[\"clip_coef\"],\n",
    "                )\n",
    "                value_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                value_loss_max = torch.max(value_loss_unclipped, value_loss_clipped)\n",
    "                value_loss = 0.5 * value_loss_max.mean()\n",
    "            else:\n",
    "                value_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - params[\"ent_coef\"] * entropy_loss + value_loss * params[\"vf_coef\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(Agent.parameters(), params[\"max_grad_norm\"])\n",
    "            optimizer.step()\n",
    "\n",
    "        if params[\"target_kl\"] is not None and approx_kl > params[\"target_kl\"]:\n",
    "            break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(Agent, \"pretrained_models/simple_ppo_for_suprised.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved PPO agent\n",
    "# agent = torch.load(\"pretrained_models/simple_ppo_for_suprised.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from results_simple_PPO\n",
    "ppo_global_step = results_simple_PPO[\"global_step\"]\n",
    "ppo_return_value = results_simple_PPO[\"return_value\"]\n",
    "\n",
    "\n",
    "df_ppo = pd.DataFrame({'global_step': ppo_global_step, 'return_value': ppo_return_value})\n",
    "\n",
    "\n",
    "df_ppo.to_csv('./data/results_simple_ppo.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Random Network Distillation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we will test the Random Network Distillation algorithm on the CartPole-v1 environment. üïπÔ∏è\n",
    "- The result of this algorithm will be presented at the end of the notebook. üìù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 Key Points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prediction problem is randomly generated. This involves 2 NNs, fixed target network sets the prediction problem (find an embedding $f(O)$ for an observation) and predictor network trained on data collected (with the task to predict $\\hat{f}(O)$) from the agent, minimizing MSE Loss $\\text{MSE} = \\| \\hat{f}(x; \\theta) - f(x) \\|^{2}_2$.\n",
    "- Prediction error is expected to be higher in novel state (suprise state) that the agent is not familiar with. \n",
    "- $R=R_E+R_I$, thus, $V=V_E+V_I$.\n",
    "- Reward and Observation Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.1 RND Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNDModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Prediction network\n",
    "        self.predictor = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_space, 256)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(256, 256)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(256, 256), std=1.0),\n",
    "        )\n",
    "\n",
    "        # Target network\n",
    "        self.target = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_space, 256)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(256, 256)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(256, 256), std=1.0),)\n",
    "\n",
    "        # fixed the target network params\n",
    "        for param in self.target.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, next_obs):\n",
    "        target_feature = self.target(next_obs)\n",
    "        predict_feature = self.predictor(next_obs)\n",
    "\n",
    "        return predict_feature, target_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingSumOfReward:\n",
    "    def __init__(self, gamma):\n",
    "        self.moving_sum_of_reward = None\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, rews):\n",
    "        if self.moving_sum_of_reward is None:\n",
    "            self.moving_sum_of_reward = rews\n",
    "        else:\n",
    "            self.moving_sum_of_reward = self.moving_sum_of_reward * self.gamma + rews\n",
    "        return self.moving_sum_of_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.2 Main Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = params[\"env_id\"]\n",
    "exp_name = params[\"exp_name\"]\n",
    "seed = params[\"seed\"]\n",
    "run_name = f\"{env_id}__{exp_name}__{seed}__{int(time.time())}\"\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id) for i in range(params[\"num_envs\"])],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up agent and model\n",
    "Agent = PPOAgent(envs, use_int_rews=True).to(device)\n",
    "optimizer = optim.Adam(\n",
    "    Agent.parameters(),\n",
    "    lr=params[\"learning_rate\"],\n",
    "    eps=1e-5,\n",
    ")\n",
    "rnd_model = RNDModel().to(device)\n",
    "combined_parameters = list(Agent.parameters()) + list(rnd_model.predictor.parameters())\n",
    "optimizer = optim.Adam(\n",
    "    combined_parameters,\n",
    "    lr=params[\"learning_rate\"],\n",
    "    eps=1e-5,\n",
    ")\n",
    "\n",
    "rew_runnning_mean_std = RunningMeanStd()\n",
    "discounted_reward = MovingSumOfReward(params[\"int_gamma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((params[\"num_steps\"], params[\"num_envs\"]) + envs.single_observation_space.shape).to(device)  # (128, 4,, 4, 84, 84)\n",
    "actions = torch.zeros((params[\"num_steps\"], params[\"num_envs\"]) + envs.single_action_space.shape).to(device)   # (128, 4) \n",
    "logprobs = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "rewards = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "surprise_rewards = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "dones = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "ext_values = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "int_values = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "avg_returns = deque(maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(params[\"num_envs\"] * params[\"num_steps\"])                      # 4 * 128\n",
    "minibatch_size = int(batch_size // params[\"num_minibatches\"])\n",
    "num_iterations = params[\"total_timesteps\"] // batch_size                        # 20000000/(4*128) -> num iterations\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "\n",
    "next_obs = torch.Tensor(envs.reset()[0]).to(device)\n",
    "next_done = torch.zeros(params[\"num_envs\"]).to(device)\n",
    "\n",
    "results_RND = {\"global_step\":[],\n",
    "                \"return_value\":[],\n",
    "                \"intrinsic_reward\":[]}\n",
    "\n",
    "tracking_global_step = 0\n",
    "\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    if params[\"anneal_lr\"]:\n",
    "        updated_lr = (1.0 - (iteration - 1.0) / num_iterations) * params[\"learning_rate\"]\n",
    "        optimizer.param_groups[0][\"lr\"] = updated_lr\n",
    "\n",
    "    # n-step rollouts\n",
    "    for step in range(0, params[\"num_steps\"]):\n",
    "        global_step += 1 * params[\"num_envs\"]\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value_ext, value_int = Agent.get_action_and_value(next_obs)\n",
    "        \n",
    "        ext_values[step], int_values[step] = (\n",
    "                value_ext.flatten(),\n",
    "                value_int.flatten(),\n",
    "            )\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob.flatten()\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "        done = np.logical_or(terminated, truncated)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "        # Normalize obs for rnd\n",
    "        rnd_next_obs = next_obs\n",
    "        \n",
    "        # Get the target F(O) and predict \\hat(F)(O) value from rnd model\n",
    "        target_next_feature, predict_next_feature = rnd_model.target(rnd_next_obs), rnd_model.predictor(rnd_next_obs)\n",
    "\n",
    "        # Calculate the surprise reward\n",
    "        surprise_rewards[step] = ((target_next_feature - predict_next_feature).pow(2).sum(1) / 2).data\n",
    "\n",
    "        if \"final_info\" in info:\n",
    "            for info in info[\"final_info\"]:\n",
    "                if info and \"episode\" in info:\n",
    "                    print(\n",
    "                    f\"global_step={global_step}, episodic_return={info['episode']['r'][0]}, surprise_reward={np.mean(surprise_rewards[step].cpu().numpy())}\"\n",
    "                        )\n",
    "                    \n",
    "        if global_step - tracking_global_step > 2000:\n",
    "                return_eval = evaluate(params[\"env_id\"], Agent, use_int_rews=True)\n",
    "                results_RND[\"global_step\"].append(global_step)\n",
    "                results_RND[\"return_value\"].append(return_eval)\n",
    "                tracking_global_step = global_step\n",
    "\n",
    "\n",
    "    # Calculate the discounted reward \n",
    "    surprise_reward_per_env = np.array(\n",
    "        [discounted_reward.update(reward_per_step) for reward_per_step in surprise_rewards.cpu().data.numpy().T]\n",
    "    )\n",
    "\n",
    "    mean, std, count = (\n",
    "        np.mean(surprise_reward_per_env),\n",
    "        np.std(surprise_reward_per_env),\n",
    "        len(surprise_reward_per_env),\n",
    "    )\n",
    "    \n",
    "    rew_runnning_mean_std.update_from_moments(mean, std**2, count)\n",
    "\n",
    "    # Normalize the curiousity_rewards based on the running_mean_std\n",
    "    surprise_rewards /= np.sqrt(rew_runnning_mean_std.var)\n",
    "\n",
    "    # Calculate value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value_ext, next_value_int = Agent.get_value(next_obs)\n",
    "        next_value_ext, next_value_int = next_value_ext.reshape(1, -1), next_value_int.reshape(1, -1)   # -> get next state values external & internal\n",
    "        ext_advantages = torch.zeros_like(rewards, device=device)\n",
    "        int_advantages = torch.zeros_like(surprise_rewards, device=device)\n",
    "        ext_lastgaelam = 0\n",
    "        int_lastgaelam = 0\n",
    "        for t in reversed(range(params[\"num_steps\"])):\n",
    "            if t == params[\"num_steps\"] - 1:\n",
    "                ext_nextnonterminal = 1.0 - next_done\n",
    "                int_nextnonterminal = 1.0\n",
    "                ext_nextvalues = next_value_ext\n",
    "                int_nextvalues = next_value_int\n",
    "            else:\n",
    "                ext_nextnonterminal = 1.0 - dones[t + 1]\n",
    "                int_nextnonterminal = 1.0\n",
    "                ext_nextvalues = ext_values[t + 1]\n",
    "                int_nextvalues = int_values[t + 1]\n",
    "            ext_delta = rewards[t] + params[\"gamma\"] * ext_nextvalues * ext_nextnonterminal - ext_values[t]\n",
    "            int_delta = surprise_rewards[t] + params[\"int_gamma\"] * int_nextvalues * int_nextnonterminal - int_values[t]\n",
    "            ext_advantages[t] = ext_lastgaelam = (\n",
    "                ext_delta + params[\"gamma\"] * params[\"gae_lambda\"] * ext_nextnonterminal * ext_lastgaelam\n",
    "            )\n",
    "            int_advantages[t] = int_lastgaelam = (\n",
    "                int_delta + params[\"int_gamma\"] * params[\"gae_lambda\"] * int_nextnonterminal * int_lastgaelam\n",
    "            )\n",
    "        ext_returns = ext_advantages + ext_values\n",
    "        int_returns = int_advantages + int_values\n",
    "\n",
    "    # Collect batch data for optimization\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape(-1)\n",
    "    b_ext_advantages = ext_advantages.reshape(-1)\n",
    "    b_int_advantages = int_advantages.reshape(-1)\n",
    "    b_ext_returns = ext_returns.reshape(-1)\n",
    "    b_int_returns = int_returns.reshape(-1)\n",
    "    b_ext_values = ext_values.reshape(-1)\n",
    "\n",
    "    b_advantages = b_int_advantages * params[\"int_coef\"] + b_ext_advantages * params[\"ext_coef\"]\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(batch_size)\n",
    "\n",
    "    rnd_next_obs = b_obs\n",
    "\n",
    "    clipfracs = []\n",
    "    for epoch in range(params[\"update_epochs\"]):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            # Forward MSE loss of the RND Model\n",
    "            predict_next_state_feature, target_next_state_feature = rnd_model(rnd_next_obs[mb_inds])\n",
    "            forward_loss = F.mse_loss(\n",
    "                predict_next_state_feature, target_next_state_feature.detach(), reduction=\"none\"\n",
    "            ).mean(-1)\n",
    "\n",
    "            mask = torch.rand(len(forward_loss), device=device)\n",
    "            mask = (mask < params[\"update_proportion\"]).type(torch.FloatTensor).to(device)\n",
    "            forward_loss = (forward_loss * mask).sum() / torch.max(\n",
    "                mask.sum(), torch.tensor([1], device=device, dtype=torch.float32)\n",
    "            )\n",
    "            _, newlogprob, entropy, new_ext_values, new_int_values = Agent.get_action_and_value(\n",
    "                b_obs[mb_inds], b_actions.long()[mb_inds]\n",
    "            )\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > params[\"clip_coef\"]).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if params[\"norm_adv\"]:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - params[\"clip_coef\"], 1 + params[\"clip_coef\"])\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            new_ext_values, new_int_values = new_ext_values.view(-1), new_int_values.view(-1)\n",
    "            if params[\"clip_vloss\"]:\n",
    "                ext_value_loss_unclipped = (new_ext_values - b_ext_returns[mb_inds]) ** 2\n",
    "                ext_v_clipped = b_ext_values[mb_inds] + torch.clamp(\n",
    "                    new_ext_values - b_ext_values[mb_inds],\n",
    "                    -params[\"clip_coef\"],\n",
    "                params[\"clip_coef\"],\n",
    "                )\n",
    "                ext_value_loss_clipped = (ext_v_clipped - b_ext_returns[mb_inds]) ** 2\n",
    "                ext_value_loss_max = torch.max(ext_value_loss_unclipped, ext_value_loss_clipped)\n",
    "                ext_value_loss = 0.5 * ext_value_loss_max.mean()\n",
    "            else:\n",
    "                ext_value_loss = 0.5 * ((new_ext_values - b_ext_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            int_value_loss = 0.5 * ((new_int_values - b_int_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            value_loss = ext_value_loss + int_value_loss\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - params[\"ent_coef\"] * entropy_loss + value_loss * params[\"vf_coef\"] + forward_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if params[\"max_grad_norm\"]:\n",
    "                nn.utils.clip_grad_norm_(\n",
    "                    combined_parameters,\n",
    "                    params[\"max_grad_norm\"],\n",
    "                )\n",
    "            optimizer.step()\n",
    "\n",
    "        if params[\"target_kl\"] is not None:\n",
    "            if approx_kl > params[\"target_kl\"]:\n",
    "                break\n",
    "\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(Agent, \"pretrained_models/ppo_for_RND.pth\")\n",
    "# torch.save(rnd_model, \"pretrained_models/rnd.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the saved PPO agent\n",
    "# agent = torch.load(\"pretrained_models/ppo_for_RND.pth\")\n",
    "# # Load the saved ICM model\n",
    "# icm = torch.load(\"pretrained_models/rnd.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from results_RND\n",
    "rnd_global_step = results_RND[\"global_step\"]\n",
    "rnd_return_value = results_RND[\"return_value\"]\n",
    "\n",
    "df_rnd = pd.DataFrame({'global_step': rnd_global_step, 'return_value': rnd_return_value})\n",
    "\n",
    "# Save DataFrames to CSV files\n",
    "df_rnd.to_csv('data/results_rnd.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Curiosity-driven Exploration by Self-supervised Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we will test the Intrinsic Curiousity Motivation (ICM) algorithm on the BreakOut-v5 environment. üïπÔ∏è\n",
    "- The result of this algorithm will be presented at the end of the notebook. üìù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1 Key Points**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The architecture is a network with two tasks, which we refer to as inverse prediction task and forward prediction task. The network, firstly, encodes the state $s_{t}$ and state $s_{t+1}$ into feature vectors $\\phi(s_{t})$ and $\\phi(s_{t+1})$. The network used the two encoded vectors as input to predict action $a_t$ in the inverse prediction task. It then used the result feature vectors $\\phi(s_{t})$ and action $a_t$ as input to predict $\\phi(s_{t+1})$. \n",
    "- The loss for the inverse prediction task is a cross entropy loss between the action chosen by the architecture and the real action that the agent has taken. The loss for the forward prediction task is an MSE loss.\n",
    "- Prediction error is expected to be higher in novel state (suprise state) that the agent is not familiar with. \n",
    "- Reward and Observation Normalization.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./image/Curiousity-driven exploration.png\" alt=\"AutoEncoder forr Count Based Exploration\" width=\"420\" height=\"350\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2 Models**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.2.1 ICM Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICMModel(nn.Module):\n",
    "    def __init__(self, use_cuda=True):\n",
    "        super(ICMModel, self).__init__()\n",
    "\n",
    "        self.eta = 1.\n",
    "        self.device = device\n",
    "\n",
    "        self.feature = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_space, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 64), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.inverse_net = nn.Sequential(\n",
    "            nn.Linear(64 * 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_space)\n",
    "        )\n",
    "\n",
    "        self.residual = [nn.Sequential(\n",
    "            nn.Linear(action_space + 512, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        ).to(self.device)] * 4\n",
    "\n",
    "        self.forward_net_1 = nn.Sequential(\n",
    "            nn.Linear(action_space + 64, 512),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.forward_net_2 = nn.Sequential(\n",
    "            nn.Linear(action_space + 512, 64),\n",
    "        )\n",
    "\n",
    "        for p in self.modules():\n",
    "            if isinstance(p, nn.Linear):\n",
    "                init.kaiming_uniform_(p.weight, a=1.0)\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        state, next_state, action = inputs\n",
    "\n",
    "        encode_state = self.feature(state)\n",
    "        encode_next_state = self.feature(next_state)\n",
    "        # get pred action\n",
    "        pred_action = torch.cat((encode_state, encode_next_state), 1)\n",
    "        pred_action = self.inverse_net(pred_action)\n",
    "        # ---------------------\n",
    "\n",
    "        # get pred next state\n",
    "        pred_next_state_feature_orig = torch.cat((encode_state, action), 1)\n",
    "        pred_next_state_feature_orig = self.forward_net_1(pred_next_state_feature_orig)\n",
    "\n",
    "        # residual\n",
    "        for i in range(2):\n",
    "            pred_next_state_feature = self.residual[i * 2](torch.cat((pred_next_state_feature_orig, action), 1))\n",
    "            pred_next_state_feature_orig = self.residual[i * 2 + 1](\n",
    "                torch.cat((pred_next_state_feature, action), 1)) + pred_next_state_feature_orig\n",
    "\n",
    "        pred_next_state_feature = self.forward_net_2(torch.cat((pred_next_state_feature_orig, action), 1))\n",
    "\n",
    "        real_next_state_feature = encode_next_state\n",
    "        return real_next_state_feature, pred_next_state_feature, pred_action\n",
    "    \n",
    "    def compute_intrinsic_reward(self, state, next_state, action):\n",
    "        # Create 0 vector for the onehot encoded action\n",
    "        action_onehot = torch.zeros(len(action), action_space, device=self.device)\n",
    "        # Scatter the value as according to the value in action\n",
    "        action_onehot.scatter_(1, action.view(len(action), -1), 1)\n",
    "\n",
    "        real_next_state_feature, pred_next_state_feature, pred_action = self.forward([state, next_state, action_onehot])\n",
    "        intrinsic_reward = self.eta * F.mse_loss(real_next_state_feature, pred_next_state_feature, reduction='none').mean(-1)\n",
    "        return intrinsic_reward\n",
    "    \n",
    "    def inference(self, states, next_states, actions):\n",
    "        action_onehot = torch.zeros(len(actions), action_space, device=self.device)\n",
    "        action_onehot.scatter_(1, actions.view(-1, 1).long(), 1)\n",
    "\n",
    "        real_next_state_feature, pred_next_state_feature, pred_action = self.forward([states, next_states, action_onehot])\n",
    "        return real_next_state_feature, pred_next_state_feature, pred_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingSumOfReward:\n",
    "    def __init__(self, gamma):\n",
    "        self.moving_sum_of_reward = None\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, rews):\n",
    "        if self.moving_sum_of_reward is None:\n",
    "            self.moving_sum_of_reward = rews\n",
    "        else:\n",
    "            self.moving_sum_of_reward = self.moving_sum_of_reward * self.gamma + rews\n",
    "        return self.moving_sum_of_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.2.2 Main Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = params[\"env_id\"]\n",
    "exp_name = params[\"exp_name\"]\n",
    "seed = params[\"seed\"]\n",
    "run_name = f\"{env_id}__{exp_name}__{seed}__{int(time.time())}\"\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id) for i in range(params[\"num_envs\"])],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up agent and model\n",
    "icm = ICMModel().to(device)\n",
    "Agent = PPOAgent(envs, use_int_rews=True).to(device)\n",
    "\n",
    "combined_parameters = list(Agent.parameters() ) + list(icm.parameters())\n",
    "optimizer = optim.Adam(\n",
    "    combined_parameters,\n",
    "    lr=params[\"learning_rate\"],\n",
    "    eps=1e-5,\n",
    ")\n",
    "\n",
    "rew_runnning_mean_std = RunningMeanStd()\n",
    "discounted_reward = MovingSumOfReward(params[\"int_gamma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((params[\"num_steps\"]+1, params[\"num_envs\"]) + envs.single_observation_space.shape).to(device)  # (128, 4,, 4, 84, 84)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(params[\"num_envs\"] * params[\"num_steps\"])                      # 4 * 128\n",
    "minibatch_size = int(batch_size // params[\"num_minibatches\"])\n",
    "num_iterations = params[\"total_timesteps\"] // batch_size                        # 20000000/(4*128) -> num iterations\n",
    "global_step = 0\n",
    "tracking_global_step = 0\n",
    "start_time = time.time()\n",
    "\n",
    "next_obs = torch.Tensor(envs.reset()[0]).to(device)\n",
    "next_done = torch.zeros(params[\"num_envs\"]).to(device)\n",
    "\n",
    "results_ICM = {\"global_step\":[],\n",
    "                \"return_value\":[],\n",
    "                \"intrinsic_reward\":[]}\n",
    "\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    actions = torch.zeros((params[\"num_steps\"], params[\"num_envs\"]) + envs.single_action_space.shape).to(device)   \n",
    "    logprobs = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    rewards = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    surprise_rewards = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    dones = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    ext_values = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    int_values = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "\n",
    "    # Calculate the new learning rate as according to the annealing rate if needed.\n",
    "    if params[\"anneal_lr\"]:\n",
    "        updated_lr = (1.0 - (iteration - 1.0) / num_iterations) * params[\"learning_rate\"]\n",
    "        optimizer.param_groups[0][\"lr\"] = updated_lr\n",
    "\n",
    "    for step in range(0, params[\"num_steps\"]):\n",
    "        global_step += 1 * params[\"num_envs\"]\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value_ext, value_int = Agent.get_action_and_value(obs[step])\n",
    "        \n",
    "        ext_values[step], int_values[step] = (\n",
    "                value_ext.flatten(),\n",
    "                value_int.flatten(),\n",
    "            )       \n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob.flatten()        \n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "        done = np.logical_or(terminated, truncated)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "        # next_obs_b[step] = next_obs\n",
    "\n",
    "        icm_obs = obs[step]\n",
    "        icm_next_obs = next_obs\n",
    "        \n",
    "        surprise_rewards[step] = icm.compute_intrinsic_reward(icm_obs.to(device), icm_next_obs.to(device), actions[step].long())\n",
    "        \n",
    "        if \"final_info\" in info:\n",
    "            for info in info[\"final_info\"]:\n",
    "                if info and \"episode\" in info:\n",
    "                    print(\n",
    "                    f\"global_step={global_step}, episodic_return={info['episode']['r'][0]}, surprise_reward={np.mean(surprise_rewards[step].data.cpu().numpy())}\"\n",
    "                        )\n",
    "                    \n",
    "        if global_step - tracking_global_step > 2000:\n",
    "                return_eval = evaluate(params[\"env_id\"], Agent, use_int_rews=True)\n",
    "                results_ICM[\"global_step\"].append(global_step)\n",
    "                results_ICM[\"return_value\"].append(return_eval)\n",
    "                tracking_global_step = global_step\n",
    "            \n",
    "    obs[-1] = next_obs\n",
    "\n",
    "    # Calculate the discounted reward \n",
    "    surprise_reward_per_env = np.array(\n",
    "        [discounted_reward.update(reward_per_step) for reward_per_step in surprise_rewards.cpu().data.numpy().T]\n",
    "    )\n",
    "\n",
    "    mean, std, count = (\n",
    "        np.mean(surprise_reward_per_env),\n",
    "        np.std(surprise_reward_per_env),\n",
    "        len(surprise_reward_per_env),\n",
    "    )\n",
    "    \n",
    "    rew_runnning_mean_std.update_from_moments(mean, std**2, count)\n",
    "\n",
    "    # Normalize the curiousity_rewards based on the running_mean_std\n",
    "    surprise_rewards /= np.sqrt(rew_runnning_mean_std.var)\n",
    "\n",
    "    # Calculate value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value_ext, next_value_int = Agent.get_value(next_obs)\n",
    "        next_value_ext, next_value_int = next_value_ext.reshape(1, -1), next_value_int.reshape(1, -1)   \n",
    "        ext_advantages = torch.zeros_like(rewards, device=device)\n",
    "        int_advantages = torch.zeros_like(surprise_rewards, device=device)\n",
    "        ext_lastgaelam = 0\n",
    "        int_lastgaelam = 0\n",
    "        for t in reversed(range(params[\"num_steps\"])):\n",
    "            if t == params[\"num_steps\"] - 1:\n",
    "                ext_nextnonterminal = 1.0 - next_done\n",
    "                int_nextnonterminal = 1.0\n",
    "                ext_nextvalues = next_value_ext\n",
    "                int_nextvalues = next_value_int\n",
    "            else:\n",
    "                ext_nextnonterminal = 1.0 - dones[t + 1]\n",
    "                int_nextnonterminal = 1.0\n",
    "                ext_nextvalues = ext_values[t + 1]\n",
    "                int_nextvalues = int_values[t + 1]\n",
    "            ext_delta = rewards[t] + params[\"gamma\"] * ext_nextvalues * ext_nextnonterminal - ext_values[t]\n",
    "            int_delta = surprise_rewards[t] + params[\"int_gamma\"] * int_nextvalues * int_nextnonterminal - int_values[t]\n",
    "            ext_advantages[t] = ext_lastgaelam = (\n",
    "                ext_delta + params[\"gamma\"] * params[\"gae_lambda\"] * ext_nextnonterminal * ext_lastgaelam\n",
    "            )\n",
    "            int_advantages[t] = int_lastgaelam = (\n",
    "                int_delta + params[\"int_gamma\"] * params[\"gae_lambda\"] * int_nextnonterminal * int_lastgaelam\n",
    "            )\n",
    "        ext_returns = ext_advantages + ext_values\n",
    "        int_returns = int_advantages + int_values\n",
    "    \n",
    "    # Collect batch data for optimization\n",
    "    b_obs = obs[:-1].reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_next_obs = obs[1:].reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape(-1)\n",
    "    b_ext_advantages = ext_advantages.reshape(-1)\n",
    "    b_int_advantages = int_advantages.reshape(-1)\n",
    "    b_ext_returns = ext_returns.reshape(-1)\n",
    "    b_int_returns = int_returns.reshape(-1)\n",
    "    b_ext_values = ext_values.reshape(-1)\n",
    "\n",
    "    b_advantages = b_int_advantages * params[\"int_coef\"] + b_ext_advantages * params[\"ext_coef\"]\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(batch_size)\n",
    "    icm_obs = b_obs\n",
    "    icm_next_obs = b_next_obs\n",
    "\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    forward_mse = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(params[\"update_epochs\"]):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            real_next_state_feature, pred_next_state_feature, pred_action = icm.inference(icm_obs[mb_inds].to(device), icm_next_obs[mb_inds].to(device), b_actions[mb_inds])\n",
    "\n",
    "            # Inverse cross entropy loss of the action prediction\n",
    "            inverse_loss = ce(\n",
    "                    pred_action, b_actions[mb_inds].long())\n",
    "            # Forward MSE loss of the next state prediction\n",
    "            forward_loss = forward_mse(\n",
    "                    pred_next_state_feature, real_next_state_feature.detach())\n",
    "\n",
    "            _, newlogprob, entropy, new_ext_values, new_int_values = Agent.get_action_and_value(\n",
    "                b_obs[mb_inds], b_actions.long()[mb_inds]\n",
    "            )\n",
    "            \n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if params[\"norm_adv\"]:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - params[\"clip_coef\"], 1 + params[\"clip_coef\"])\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            new_ext_values, new_int_values = new_ext_values.view(-1), new_int_values.view(-1)\n",
    "            if params[\"clip_vloss\"]:\n",
    "                ext_value_loss_unclipped = (new_ext_values - b_ext_returns[mb_inds]) ** 2\n",
    "                ext_v_clipped = b_ext_values[mb_inds] + torch.clamp(\n",
    "                    new_ext_values - b_ext_values[mb_inds],\n",
    "                    -params[\"clip_coef\"],\n",
    "                params[\"clip_coef\"],\n",
    "                )\n",
    "                ext_value_loss_clipped = (ext_v_clipped - b_ext_returns[mb_inds]) ** 2\n",
    "                ext_value_loss_max = torch.max(ext_value_loss_unclipped, ext_value_loss_clipped)\n",
    "                ext_value_loss = 0.5 * ext_value_loss_max.mean()\n",
    "            else:\n",
    "                ext_value_loss = 0.5 * ((new_ext_values - b_ext_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            int_value_loss = 0.5 * ((new_int_values - b_int_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            value_loss = ext_value_loss + int_value_loss\n",
    "            entropy_loss = entropy.mean()\n",
    "\n",
    "            loss = pg_loss - params[\"ent_coef\"] * entropy_loss + value_loss * params[\"vf_coef\"] + forward_loss + inverse_loss\n",
    "        \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if params[\"max_grad_norm\"]:\n",
    "                nn.utils.clip_grad_norm_(\n",
    "                    combined_parameters,\n",
    "                    params[\"max_grad_norm\"],\n",
    "                )\n",
    "            optimizer.step()\n",
    "\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(Agent, \"pretrained_models/ppo_for_ICM.pth\")\n",
    "# torch.save(icm, \"pretrained_models/icm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the saved PPO agent\n",
    "# agent = torch.load(\"pretrained_models/ppo_for_ICM.pth\")\n",
    "\n",
    "# # Load the saved ICM model\n",
    "# icm = torch.load(\"pretrained_models/icm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from results_RND\n",
    "icm_global_step = results_ICM[\"global_step\"]\n",
    "icm_return_value = results_ICM[\"return_value\"]\n",
    "icm_intrinsic_reward = results_ICM[\"intrinsic_reward\"]\n",
    "\n",
    "df_icm = pd.DataFrame({'global_step': icm_global_step, 'return_value': icm_return_value})\n",
    "\n",
    "# Save DataFrames to CSV files\n",
    "df_icm.to_csv('data/results_icm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Results Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simple_PPO = pd.read_csv('data/results_simple_ppo.csv')\n",
    "df_rnd = pd.read_csv('data/results_rnd.csv')\n",
    "df_icm = pd.read_csv('data/results_icm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_simple_PPO, df_rnd, df_icm]\n",
    "\n",
    "for df in dfs:\n",
    "    df[\"return_value_smoothed\"] = df[\"return_value\"].ewm(alpha=1-0.9).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data\n",
    "plt.plot(df_simple_PPO[\"global_step\"], df_simple_PPO[\"return_value_smoothed\"], label='Simple PPO')\n",
    "plt.plot(df_rnd[\"global_step\"], df_rnd[\"return_value_smoothed\"], label='RND')\n",
    "plt.plot(df_icm[\"global_step\"], df_icm[\"return_value_smoothed\"], label='ICM')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Return Value')\n",
    "plt.title('Evaluate value of 1 run  (evaluate over 50 episodes)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When experimenting with the CartPole environment, we didn't observe significant differences between using simple PPO and incorporating exploration methods.\n",
    "- We attribute this to the environment's simplicity, which renders exploration techniques unnecessary.\n",
    "- However, if you explore the notebook \"Memory_for_Surprise-driven_Exploration_Atari.ipynb\", you will find the considerably more challenging environment where exploration techniques demonstrate significant impact."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
